{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "\n",
    "from rpy2.robjects.packages import importr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from chromax import Simulator\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "import matplotlib\n",
    "from jax import device_get\n",
    "from scipy import stats\n",
    "\n",
    "from typing import Dict, Any, List\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Concatenate, Lambda\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_genetic_map(n_markers, n_chromosomes):\n",
    "  df = pd.DataFrame(generate_marker_effects(n_markers=n_markers), columns=['Yield'])\n",
    "  df['cM'] = np.random.uniform(0, 200, len(df))\n",
    "  df['CHR.PHYS'] = '1A'\n",
    "  df = df.sort_values(by='cM')\n",
    "  df = df[['CHR.PHYS', 'cM', 'Yield']]\n",
    "  # save df as csv under filename\n",
    "  return df\n",
    "\n",
    "def simplify_geneticmap(lst, qtl):\n",
    "    # Select 'qtl' random indexes\n",
    "    indexes_positive = random.sample(range(len(lst)), qtl)\n",
    "    \n",
    "    # Select 'qtl' random indexes not already selected\n",
    "    remaining_indexes = set(range(len(lst))) - set(indexes_positive)\n",
    "    indexes_negative = random.sample(remaining_indexes, qtl)\n",
    "\n",
    "    # Modify the list\n",
    "    for i in range(len(lst)):\n",
    "        if i in indexes_positive:\n",
    "            lst[i] = random.uniform(0.5, 1.0)  # Assign random float between 0.5 and 1.0\n",
    "        elif i in indexes_negative:\n",
    "            lst[i] = random.uniform(-0.5, -1.0)  # Assign random float between -0.5 and -1.0\n",
    "        else:\n",
    "            lst[i] = 0  # Assign 0\n",
    "\n",
    "    return lst\n",
    "\n",
    "def generate_population(n_pop=100, n_markers=500):\n",
    "    \"\"\"\n",
    "    Generate a numpy array of randoms of length 500 with randomized 0, 1, or 2 at each position.\n",
    "    It will generate 100 individuals based on n_pop.\n",
    "\n",
    "    Returns: numpy array of size (n_pop, n_markers)\n",
    "    \"\"\"\n",
    "    shape=(n_pop, n_markers, 2)\n",
    "    # Define the elements to choose from and their associated probabilities\n",
    "    elements = [0, 1, 2]\n",
    "    probabilities = [1/3, 1/3, 1/3]  # equal probabilities for 0, 1, and 2\n",
    "\n",
    "    # Generate the population\n",
    "    population = np.random.choice(elements, size=(n_pop, n_markers), p=probabilities)\n",
    "\n",
    "    return np.random.choice([True, False], size=shape)\n",
    "\n",
    "\n",
    "def generate_marker_effects(n_markers=500, mu=0, sigma=0.1):\n",
    "    \"\"\"\n",
    "    Generate a numpy array of marker effects with a normal distribution.\n",
    "\n",
    "    Parameters:\n",
    "    n_markers (int): Number of markers.\n",
    "    mu (float): Mean of the distribution.\n",
    "    sigma (float): Standard deviation of the distribution.\n",
    "\n",
    "    Returns:\n",
    "    numpy array of marker effects\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the marker effects\n",
    "    marker_effects = np.random.normal(mu, sigma, n_markers)\n",
    "\n",
    "    return marker_effects\n",
    "\n",
    "def parse_markerEffects(genetic_map, nChr):\n",
    "    # Get the length of the genetic map\n",
    "    length = len(genetic_map)\n",
    "\n",
    "    # Create a new array for storing the chromosome number for each marker\n",
    "    chr = [0] * length\n",
    "\n",
    "    # Calculate the number of markers per chromosome\n",
    "    markers_per_chr = length // nChr\n",
    "\n",
    "    # Iterate over the range of the genetic map length\n",
    "    for i in range(length):\n",
    "        # Calculate the chromosome number and store it in the chr array\n",
    "        chr[i] = i // markers_per_chr + 1\n",
    "\n",
    "    return chr\n",
    "\n",
    "\n",
    "def reshape_pop(maizeHaplo):\n",
    "    reshapeHaplo = maizeHaplo.reshape(int((maizeHaplo.shape[0])/2),2,maizeHaplo.shape[1])\n",
    "    reshapeHaplo = reshapeHaplo.transpose((0,2,1))\n",
    "    return reshapeHaplo\n",
    "\n",
    "def return_genetic_map_df(markerEffects, nChr, geneticMap):\n",
    "    chr = parse_markerEffects(markerEffects, nChr)\n",
    "    chr = [int(x[0]) for x in chr]\n",
    "    trait = markerEffects\n",
    "    pos = geneticMap\n",
    "    # Assuming chr, trait, pos are your arrays\n",
    "    df = pd.DataFrame({'CHR.PHYS': chr, 'Yield': trait, 'cM': pos})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "x <- seq(0, 2*pi, length.out=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%R -o x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %R install.packages(\"AlphaSimR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(\"AlphaSimR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "nInd = 10\n",
    "nChr = 2\n",
    "segSites = 5\n",
    "\n",
    "founderGenomes = runMacs(nInd = nInd,\n",
    "                         nChr = nChr,\n",
    "                         segSites = segSites,\n",
    "                         species = \"MAIZE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "SP = SimParam$new(founderGenomes)\n",
    "SP$addTraitA(segSites)\n",
    "# SP$setVarE(h2=.02)\n",
    "pop = newPop(founderGenomes, simParam=SP)\n",
    "ans = fastRRBLUP(pop, simParam=SP, useQtl=TRUE, use='gv')\n",
    "ans@gv[[1]]@addEff\n",
    "markerEffects = slot(slot(ans, \"gv\")[[1]], \"addEff\")\n",
    "maizeHaplo = pullSegSiteHaplo(pop)\n",
    "maizeGeno = pullSegSiteGeno(pop)\n",
    "#cm positions of each marker\n",
    "genMap = SP$genMap\n",
    "geneticMap = unlist(genMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%R -o maizeHaplo\n",
    "%R -o maizeGeno\n",
    "%R -o markerEffects\n",
    "%R -o geneticMap\n",
    "%R -o nInd\n",
    "%R -o nChr\n",
    "%R -o segSites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nMarkers = segSites * nChr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def split_and_average(lst, set_size):\n",
    "    # lst is from get_baseline\n",
    "    #average list\n",
    "    lst = [x.mean() for x in lst]\n",
    "    # Split the list into sublists\n",
    "    split_lst = [lst[i:i + set_size] for i in range(0, len(lst), set_size)]\n",
    "\n",
    "    # Calculate the average of the x-th element in each sublist\n",
    "    averages = []\n",
    "    for x in range(set_size):\n",
    "        x_elements = [sublist[x] for sublist in split_lst if len(sublist) > x]\n",
    "        averages.append(mean(x_elements))\n",
    "\n",
    "    return averages\n",
    "\n",
    "def sample_two_offspring_per_cross(dataset):\n",
    "    \"\"\"\n",
    "    Samples two offspring per cross from the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (np.ndarray): The input dataset with shape (n_cross, n_offspring, markers, diploid).\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A new array with sampled offspring, shape (n_cross, 2, markers, diploid).\n",
    "    \"\"\"\n",
    "    n_cross = dataset.shape[0]\n",
    "    n_offspring_per_cross = 2\n",
    "    # Create an array to store the indices of the offspring to sample\n",
    "    indices = np.random.choice(dataset.shape[1], size=(n_cross, n_offspring_per_cross), replace=False)\n",
    "    # Use advanced indexing to select two offspring per cross\n",
    "    sampled_offspring = dataset[np.arange(n_cross)[:, None], indices, :, :]\n",
    "    return sampled_offspring.reshape(-1, sampled_offspring.shape[2], sampled_offspring.shape[3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def collapse_first_two_axes(dataset):\n",
    "    \"\"\"\n",
    "    Collapses the first two axes of the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (np.ndarray): The input dataset with shape (n_cross, n_offspring, markers, diploid).\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A new array with the first two axes collapsed, shape (n_cross * n_offspring, markers, diploid).\n",
    "    \"\"\"\n",
    "    # Calculate the new shape\n",
    "    new_shape = (dataset.shape[0] * dataset.shape[1],) + dataset.shape[2:]\n",
    "    # Reshape the dataset\n",
    "    collapsed_dataset = dataset.reshape(new_shape)\n",
    "    return collapsed_dataset\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# number_individuals = 50  # Replace with your actual number of individuals\n",
    "# number_markers = 100     # Replace with your actual number of markers\n",
    "# ploidy = 2              # Replace with your actual ploidy level\n",
    "# batch_size = 5\n",
    "# batch_id=1 \n",
    "\n",
    "# initial_population = create_fake_population(batch_size, number_individuals, number_markers, ploidy)\n",
    "# genetic_map_df = create_fake_geneticmap(number_markers)\n",
    "# marker_strength = np.array(genetic_map_df['Yield'])\n",
    "\n",
    "# dummy_score = np.random.rand(batch_size, number_individuals)\n",
    "\n",
    "# actor_model = create_actor_model(number_individuals, number_markers, ploidy)\n",
    "# critic_model = create_critic_model(number_individuals, number_markers, ploidy, actor_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy = actor_model([initial_population,dummy_score])\n",
    "# value_estimate = critic_model([initial_population, dummy_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #split the population in half based on the policy metric\n",
    "# top_policy_parents = select_parents(policy[batch_id,:].numpy(), total_parents=int(initial_population.shape[1] /2))[1].numpy()\n",
    "\n",
    "# bottom_policy_parents = np.arange(initial_population.shape[1])\n",
    "# bottom_policy_parents = np.setdiff1d(bottom_policy_parents, top_policy_parents)\n",
    "\n",
    "# top_policy_parents = initial_population[batch_id][top_policy_parents]\n",
    "# bottom_policy_parents = initial_population[batch_id][bottom_policy_parents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_scores = calculate_scores(initial_population[batch_id], marker_strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_scores(panmixia(bottom_policy_parents, 500), marker_strength).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_pop = panmixia(top_policy_parents, 50)\n",
    "# new_scores = calculate_scores(new_pop, marker_strength)\n",
    "# print(new_scores.mean())\n",
    "# new_parents = scores2parents(new_scores,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_pop = panmixia(new_pop[new_parents], 50)\n",
    "# new_scores = calculate_scores(new_pop, marker_strength)\n",
    "# print(new_scores.mean())\n",
    "# new_parents = scores2parents(new_scores,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\n",
    "#     f'''\n",
    "#     Batch Size : {batch_size}\n",
    "      \n",
    "#     Population Size : {number_individuals}\n",
    "#     Marker Count : {number_markers}\n",
    "\n",
    "#     Population Input Shape : {initial_population.shape}\n",
    "#     Scores Input Shape : {dummy_score.shape}\n",
    "\n",
    "#     Policy Output Shape : {policy.shape}\n",
    "#     Values Output Shape : {value_estimate.shape}\n",
    "  \n",
    "#     '''\n",
    "#       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS FO SIMULATING THE ENVIRONMENT AND ACTIONS\n",
    "\n",
    "\n",
    "def ppo_step(agent, parent_index,parent_score, cycle, total_cycles):\n",
    "    \"\"\"\n",
    "    Takes an action in the environment and returns the next state, reward, and a done flag.\n",
    "\n",
    "    Inputs:\n",
    "    - agent: An instance of the BreederAgent class.\n",
    "    - parent_index (batch_size, selected_parent_count): The indices of selected parents for breeding. FROM POLICY\n",
    "    - cycle: The current cycle number within the episode.\n",
    "    - total_cycles: The total number of cycles in an episode.\n",
    "\n",
    "    Outputs:\n",
    "    - step_data: A list of dictionaries, each containing the new state, reward, and done flag for a batch.\n",
    "    \"\"\"\n",
    "    step_data = []\n",
    "    #parents_index is calculated by the actor\n",
    "    for c, x in enumerate(parent_index):  # for each batch\n",
    "        parents_subpop = agent.current_population[c][x]\n",
    "        mass_pop = panmixia(parents_subpop, total_offspring=agent.population_size * 5)\n",
    "        mass_score = calculate_scores(mass_pop, agent.marker_strength)\n",
    "        prev_score = agent.current_score[c].mean()\n",
    "        reward = mass_score.mean() - prev_score\n",
    "\n",
    "        # Sample a subset of the new population to form the next state\n",
    "        new_pop_indices = np.random.choice(mass_pop.shape[0], size=agent.population_size, replace=False)\n",
    "        new_pop = mass_pop[new_pop_indices]\n",
    "        new_score = calculate_scores(new_pop, agent.marker_strength)\n",
    "\n",
    "        done = (cycle == total_cycles - 1)\n",
    "        batch_data = {\n",
    "            'new_population': new_pop,\n",
    "            'new_score':new_score,  # Store the actual next state\n",
    "            'reward': reward,\n",
    "            'parent_scores': parent_score,\n",
    "            'previous_score': prev_score,\n",
    "            'current_score': new_score.mean(),\n",
    "            'done': done\n",
    "        }\n",
    "        step_data.append(batch_data)\n",
    "\n",
    "    return step_data\n",
    "\n",
    "\"\"\"\n",
    "In summary, the advantage you are calculating is effectively measuring the relative benefit \n",
    "of the breeding decisions made by the agent compared to the expected outcome of the current\n",
    "policy. It is a crucial component in training the agent, as it guides the optimization \n",
    "process towards decisions that yield better-than-average improvements in the plant population.\n",
    "\n",
    "The advantage of a breeding decision (action) is the difference between the reward from that \n",
    "decision and the critic's estimated value of the population before breeding. If the advantage\n",
    "is positive, it means that the breeding decision led to a better-than-expected improvement\n",
    "in the population's mean score.\n",
    "\"\"\"\n",
    "\n",
    "def calculate_advantages(agent, episode_data):\n",
    "    # Calculate Advantages\n",
    "\n",
    "    advantages = []\n",
    "    agent.gamma = .95\n",
    "    # Assuming agent.gamma is the discount factor\n",
    "    gamma = agent.gamma\n",
    "\n",
    "    for cycle, step in enumerate(reversed(episode_data)):\n",
    "        # The reversed list index starts from the last to the first\n",
    "        reversed_cycle = (len(episode_data) - 1) - cycle\n",
    "\n",
    "        # Get the reward and new values\n",
    "        reward = np.array([x['reward'] for x in episode_data[reversed_cycle]])\n",
    "        new_score = np.array([x['new_score'] for x in episode_data[reversed_cycle]])\n",
    "        new_pop = np.array([x['new_population'] for x in episode_data[reversed_cycle]])\n",
    "        new_value = agent.critic([new_pop, new_score])\n",
    "        if reversed_cycle > 0:\n",
    "            # If not the first cycle, get the previous values\n",
    "            old_score = np.array([x['new_score'] for x in episode_data[reversed_cycle - 1]])\n",
    "            old_pop = np.array([x['new_population'] for x in episode_data[reversed_cycle - 1]])\n",
    "        else:\n",
    "            # For the first cycle, use the initial population and score\n",
    "            old_score = agent.initial_score\n",
    "            old_pop = agent.initial_population\n",
    "\n",
    "        # Calculate the value of the old population\n",
    "        old_value = agent.critic([old_pop, old_score])\n",
    "\n",
    "        # Calculate advantage\n",
    "        delta = reward + gamma * new_value - old_value\n",
    "        advantages.append(delta)\n",
    "\n",
    "\n",
    "    # meant to evaluate how good the decision to select certain individuals for breeding \n",
    "    # (action) was, compared to an average or baseline selection strategy (state value)\n",
    "\n",
    "    # Convert list of advantages to a numpy array\n",
    "    advantages = np.array(advantages)\n",
    "\n",
    "    # Remember to reverse the advantages array so it corresponds to the original order\n",
    "    advantages = np.flip(advantages, axis=0)\n",
    "\n",
    "    return advantages\n",
    "\n",
    "def get_r(values):\n",
    "\n",
    "    # Create an array of indices, same length as your list\n",
    "    indices = np.array(range(len(values)))\n",
    "\n",
    "    # Perform linear regression\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(indices, values)\n",
    "    return slope\n",
    "\n",
    "def add_batchdim(arr):\n",
    "    return np.expand_dims(arr, axis=0)\n",
    "\n",
    "def create_fake_geneticmap(number_markers):\n",
    "    # 'chr' will always be '1A' for every marker\n",
    "    chr_array = ['1A'] * number_markers\n",
    "    \n",
    "    # 'yield': Create a marker_strength array with 1 float between -0.5 and +0.5 randomly\n",
    "    # yield_array = np.random.poisson(np.random.randint(1,10), size=number_markers)\n",
    "    poisson_values = np.random.poisson(np.random.randint(1, 10), size=number_markers)\n",
    "\n",
    "    # Scale the Poisson values to the range [0, 1]\n",
    "    scaled_poisson_values = poisson_values / np.max(poisson_values)\n",
    "\n",
    "    # Stretch and shift the values to the range [-1, 1]\n",
    "    yield_array = (scaled_poisson_values * 2) - 1\n",
    "    \n",
    "    # 'cM': create an array for number_markers length evenly sampled between 0 and 100\n",
    "    cM_array = np.linspace(0, 100, num=number_markers)\n",
    "    \n",
    "    # Create the DataFrame with the auto-generated data\n",
    "    df = pd.DataFrame({'CHR.PHYS': chr_array, 'Yield': yield_array, 'cM': cM_array*.01})\n",
    "    \n",
    "    return df\n",
    "\n",
    "import numpy as np\n",
    "from random import choice, shuffle\n",
    "\n",
    "def create_fake_population(total_pops, number_individuals, number_markers, ploidy):\n",
    "    # List of generation methods\n",
    "    generation_methods = [\n",
    "        lambda: np.random.randint(2, size=(number_individuals, number_markers, ploidy)),\n",
    "        lambda: np.random.rand(number_individuals, number_markers, ploidy),\n",
    "        lambda: np.random.standard_normal((number_individuals, number_markers, ploidy)),\n",
    "        lambda: np.random.choice([0, 1], size=(number_individuals, number_markers, ploidy))\n",
    "    ]\n",
    "    \n",
    "    populations = []\n",
    "    for _ in range(total_pops):\n",
    "        # Randomly select a generation method and generate the population\n",
    "        gen_method = choice(generation_methods)\n",
    "        population = gen_method()\n",
    "        populations.append(population)\n",
    "        \n",
    "    # Combine all populations into a single numpy array\n",
    "    combined_population = np.array(populations)\n",
    "    return combined_population\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_scores(population, marker_strength):\n",
    "    \"\"\"\n",
    "    Calculates the additive score by matrix multipling the population (n,m) with the marker strengths (m,)\n",
    "    \"\"\"\n",
    "    # Perform a dot product between the dosages and marker strength arrays\n",
    "    dosages = np.sum(population,axis=2)\n",
    "    scores = np.dot(dosages, marker_strength)\n",
    "    return scores\n",
    "\n",
    "def panmixia(selected_parents, total_offspring = 2):\n",
    "    \"\"\" Handles the random crossing for us ; heuristic!\n",
    "\n",
    "    selected_parents ( n , m , d )\n",
    "    \n",
    "    \"\"\"\n",
    "    n, m, d = selected_parents.shape\n",
    "    offspring_target = total_offspring\n",
    "    offspring_list = []\n",
    "\n",
    "    while len(offspring_list) < offspring_target:\n",
    "        # Randomly pick two parents without replacement\n",
    "        parents_indices = np.random.choice(n, size=2, replace=False)\n",
    "        parent1, parent2 = selected_parents[parents_indices]\n",
    "\n",
    "        # Simulate random recombination for each marker\n",
    "        offspring = np.zeros((m, d), dtype=parent1.dtype)\n",
    "        for i in range(m):\n",
    "            # Randomly choose one allele from each parent for each marker\n",
    "            for j in range(d):\n",
    "                parent_allele = np.random.choice([parent1[i, j], parent2[i, j]])\n",
    "                offspring[i, j] = parent_allele\n",
    "\n",
    "        # Add the new offspring to the offspring list\n",
    "        offspring_list.append(offspring)\n",
    "\n",
    "    # Convert offspring list to a numpy array with shape (offspring_target, m, d)\n",
    "    offspring_array = np.array(offspring_list)\n",
    "    return offspring_array\n",
    "\n",
    "\n",
    "def scores2parents(scores,K):\n",
    "    \"\"\"\n",
    "    hint: use output from calculate_scores\n",
    "    \"\"\"\n",
    "    # Specify the number of top values you want (K)\n",
    "    K = 5\n",
    "\n",
    "    # Get the indices that would sort the array\n",
    "    sorted_indices = np.argsort(scores)\n",
    "\n",
    "    # Take the last K indices of the sorted indices array\n",
    "    top_k_indices = sorted_indices[-K:]\n",
    "\n",
    "    # Since argsort returns indices in ascending order, reverse to get the top values\n",
    "    top_k_indices = top_k_indices[::-1]\n",
    "\n",
    "    print(\"Indices of top K values:\", top_k_indices)\n",
    "    print(\"Top K values:\", scores[top_k_indices])\n",
    "\n",
    "    return top_k_indices\n",
    "\n",
    "\n",
    "def select_parents(policy, total_parents):\n",
    "    \"\"\"\n",
    "        input: Policy from actor ( metric value for each individual )\n",
    "        returns : index of parents to be included in random_crosses for next step of breeding program\n",
    "    \"\"\"\n",
    "    values, indices = tf.math.top_k(policy, total_parents)\n",
    "    return values,indices\n",
    "\n",
    "def create_actor_model(num_individuals, num_markers, ploidy):\n",
    "    # Define the input layers\n",
    "    population_input = Input(shape=(num_individuals, num_markers, ploidy), name='population_input')\n",
    "    scores_input = Input(shape=(num_individuals,), name='scores_input')\n",
    "\n",
    "    # Flatten the population input to process it with Dense layers\n",
    "    flat_population = Flatten()(population_input)\n",
    "\n",
    "    # Combine the flattened population and scores inputs\n",
    "    combined_input = Concatenate()([flat_population, scores_input])\n",
    "\n",
    "    # Define the hidden layers\n",
    "    hidden1 = Dense(128, activation='relu')(combined_input)\n",
    "    hidden2 = Dense(64, activation='relu')(hidden1)\n",
    "\n",
    "    # Output layer with one scalar value per individual\n",
    "    policy_output = Dense(num_individuals, activation='sigmoid', name='policy_output')(hidden2)\n",
    "\n",
    "    # Create the actor model\n",
    "    actor_model = Model(inputs=[population_input, scores_input], outputs=policy_output)\n",
    "\n",
    "    return actor_model\n",
    "\n",
    "def create_critic_model(num_individuals, num_markers, ploidy, actor_model):\n",
    "    # Define the input layers\n",
    "    population_input = Input(shape=(num_individuals, num_markers, ploidy), name='population_input')\n",
    "    scores_input = Input(shape=(num_individuals,), name='scores_input')\n",
    "\n",
    "    # Call the actor model to get the policy output for the given state\n",
    "    policy_output = actor_model([population_input, scores_input])\n",
    "\n",
    "    # Use Lambda layer to apply tf.stop_gradient to prevent backpropagation\n",
    "    policy_output_no_gradient = Lambda(lambda x: tf.stop_gradient(x))(policy_output)\n",
    "\n",
    "    # Flatten the population input to process it with Dense layers\n",
    "    flat_population = Flatten()(population_input)\n",
    "\n",
    "    # Combine the flattened population, scores, and policy outputs without gradients\n",
    "    combined_input = Concatenate()([flat_population, scores_input, Flatten()(policy_output_no_gradient)])\n",
    "\n",
    "    # Define the hidden layers\n",
    "    hidden1 = Dense(128, activation='relu')(combined_input)\n",
    "    hidden2 = Dense(64, activation='relu')(hidden1)\n",
    "\n",
    "    # Output layer with a single value estimate for the state\n",
    "    value_output = Dense(1, activation='linear', name='value_output')(hidden2)\n",
    "\n",
    "    # Create the critic model\n",
    "    critic_model = Model(inputs=[population_input, scores_input], outputs=value_output)\n",
    "\n",
    "    return critic_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BreederAgent:\n",
    "    \"\"\"\n",
    "    A PPO agent that learns to select parents for breeding programs using Actor-Critic method.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initializes the breeding program with a configuration dictionary.\n",
    "        \n",
    "        Parameters:\n",
    "        config (dict): Configuration dictionary containing all necessary parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the basic attributes from the config\n",
    "        self.compute_baselines = config.get('compute_baselines', False)\n",
    "        self.population_size = config['population_size']\n",
    "        self.marker_count = config['marker_count']\n",
    "        self.initial_population = config['initial_population']\n",
    "        self.genetic_map = config['genetic_map']\n",
    "        self.marker_strength = np.array(self.genetic_map['Yield'])\n",
    "        self.initial_score = np.array([calculate_scores(x,self.marker_strength) for x in self.initial_population])\n",
    "\n",
    "        self.ploidy = config['ploidy']\n",
    "\n",
    "\n",
    "        # Initialize the Actor and Critic models\n",
    "        self.actor = create_actor_model(num_individuals=self.population_size, num_markers=self.marker_count, ploidy=self.ploidy)\n",
    "        self.critic = create_critic_model(num_individuals=self.population_size, num_markers=self.marker_count, ploidy=self.ploidy, actor_model=self.actor)\n",
    "\n",
    "\n",
    "        #truncation and random baselines\n",
    "        # self.truncation_baseline = [calculate_scores(x, self.marker_strength) for x in self.initial_population]\n",
    "        # truncation_baseline = self.breeding_simulator.select(self.initial_population, k = int(self.initial_population.shape[0]/2))\n",
    "        # self.truncation_baseline = self.breeding_simulator.GEBV(collapse_first_two_axes(self.breeding_simulator.random_crosses(truncation_baseline, n_crosses = truncation_baseline.shape[0], n_offspring=50))).mean()[0]\n",
    "\n",
    "        # self.random_baseline = self.breeding_simulator.GEBV(collapse_first_two_axes(self.breeding_simulator.random_crosses(self.initial_population, n_crosses = self.initial_population.shape[0], n_offspring=50))).mean()[0]\n",
    "\n",
    "\n",
    "    def reset_environment(self):\n",
    "        self.current_population = self.initial_population\n",
    "        self.current_scores = self.initial_score\n",
    "\n",
    "# Example usage:\n",
    "number_individuals = 50  # Replace with your actual number of individuals\n",
    "number_markers = 100     # Replace with your actual number of markers\n",
    "ploidy = 2              # Replace with your actual ploidy level\n",
    "batch_size = 3\n",
    "initial_population = create_fake_population(batch_size, number_individuals, number_markers, ploidy)\n",
    "genetic_map_df = create_fake_geneticmap(number_markers)\n",
    "marker_strength = np.array(genetic_map_df['Yield'])\n",
    "fraction_selection = 10\n",
    "\n",
    "config = {\n",
    "    'compute_baselines': False,\n",
    "    'initial_population': initial_population,\n",
    "    'genetic_map': genetic_map_df,\n",
    "    'population_size': number_individuals,\n",
    "    'marker_count': number_markers,\n",
    "    'heritability': .99,\n",
    "    'episodes': 10,\n",
    "    'cycles': 2,\n",
    "    'learning_rate': .001,\n",
    "    'replicates': 1,\n",
    "    'ploidy':2,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Truncation Selection program\n",
    "\n",
    "agent = BreederAgent(config)\n",
    "\n",
    "# policy = actor_model([initial_population,dummy_score])\n",
    "# value_estimate = critic_model([initial_population, dummy_score])\n",
    "\n",
    "\n",
    "print([x.mean() for x in agent.initial_score])\n",
    "# example using the raw scores (no actor/critic)\n",
    "rep_scores = []\n",
    "for e in range(2):\n",
    "    scores = []\n",
    "    agent.current_population = agent.initial_population\n",
    "    agent.current_score = agent.initial_score\n",
    "    for c in range(3):#action\n",
    "        if c == 0:\n",
    "            scores.append(agent.current_score.mean())\n",
    "\n",
    "        selected_parents = [select_parents(x, total_parents = int(agent.population_size/fraction_selection))[1].numpy() for x in agent.current_score]\n",
    "        subset_parents = [s[p] for s,p in zip(agent.current_population,selected_parents)]\n",
    "\n",
    "\n",
    "        #view new environment\n",
    "        new_pops = [panmixia(x, total_offspring=100) for x in subset_parents]\n",
    "        sample_index = np.random.choice(new_pops[0].shape[0], size=agent.population_size, replace=False)\n",
    "        new_pops = [x[sample_index] for x in new_pops]\n",
    "        scored_pops = [calculate_scores(x,agent.marker_strength) for x in new_pops]\n",
    "        new_pops = np.array(new_pops)\n",
    "        scored_pops = np.array(scored_pops)\n",
    "\n",
    "        # Add assertions here\n",
    "        assert np.array(agent.current_population).shape == np.array(new_pops).shape, \"Dimensions of current_population and new_pops do not match\"\n",
    "        assert np.array(agent.current_score).shape == np.array(scored_pops).shape, \"Dimensions of current_score and scored_pops do not match\"\n",
    "\n",
    "        agent.current_population = new_pops\n",
    "        agent.current_score = scored_pops\n",
    "        scores.append(scored_pops.mean())\n",
    "    rep_scores.append(scores)\n",
    "    \n",
    "[plt.plot(x,label=f'batch {c}') for c,x in enumerate(rep_scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[plt.hist(x,alpha=.5) for x in agent.initial_score]\n",
    "plt.title('individual scores')\n",
    "plt.show()\n",
    "plt.hist(agent.marker_strength)\n",
    "plt.title('marker strengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PPO Training Loop\n",
    "\n",
    "#Initialize the environment, actor, and critic models, and set up any necessary tracking variables for rewards, scores, and baselines.\n",
    "\n",
    "number_individuals = 50  # Replace with your actual number of individuals\n",
    "number_markers = 100     # Replace with your actual number of markers\n",
    "ploidy = 2              # Replace with your actual ploidy level\n",
    "batch_size = 3\n",
    "\n",
    "initial_population = create_fake_population(batch_size, number_individuals, number_markers, ploidy)\n",
    "genetic_map_df = create_fake_geneticmap(number_markers)\n",
    "marker_strength = np.array(genetic_map_df['Yield'])\n",
    "selection_fraction = 3 #TOADD EVERYWEHRE( top 1/3)\n",
    "config = {\n",
    "    'compute_baselines': False,\n",
    "    'initial_population': initial_population,\n",
    "    'genetic_map': genetic_map_df,\n",
    "    'population_size': number_individuals,\n",
    "    'marker_count': number_markers,\n",
    "    'heritability': .99,\n",
    "    'episodes': 10,\n",
    "    'learning_rate': .001,\n",
    "    'replicates': 1,\n",
    "    'ploidy':2,\n",
    "}\n",
    "\n",
    "agent = BreederAgent(config)\n",
    "\n",
    "#Policy Based Step\n",
    "agent.current_population = agent.initial_population\n",
    "agent.current_score = agent.initial_score\n",
    "\n",
    "# policy = agent.actor([agent.current_population, agent.current_score])\n",
    "# value = agent.critic([agent.current_population, agent.current_score], agent.actor)\n",
    "\n",
    "#Collect Single Episode of 5 Cycles With Current Policy\n",
    "agent.episodes = 1\n",
    "agent.cycles=10\n",
    "\n",
    "#EPISODE EXPERIENCE COLLECTION\n",
    "episode_data = []\n",
    "values = []\n",
    "old_policy_log_probs = []\n",
    "for c in range(agent.cycles):\n",
    "    #a score for each individual in the current population\n",
    "    policy_logits = agent.actor([agent.current_population, agent.current_score])\n",
    "    policy_probs = tf.nn.softmax(policy_logits)\n",
    "    log_probs = tf.math.log(policy_probs + 1e-8)\n",
    "    old_policy_log_probs.append(log_probs)\n",
    "    #the estimated value of the current state (population and scores)\n",
    "    value = agent.critic([agent.current_population, agent.current_score], agent.actor)\n",
    "    parent_data = [select_parents(x, selection_fraction) for x in policy_logits]\n",
    "    parent_score = [x[0].numpy() for x in parent_data] # contains policys metric of selected parents\n",
    "    parent_index = [x[1].numpy() for x in parent_data] # contains index of selected parents\n",
    "    breeding_step = ppo_step(agent,parent_index,parent_score, c,agent.cycles)\n",
    "    new_pops = np.array([x['new_population'] for x in breeding_step])\n",
    "    new_scores = np.array([x['new_score'] for x in breeding_step])\n",
    "    agent.current_population = new_pops\n",
    "    agent.current_score = new_scores\n",
    "    for i in breeding_step:\n",
    "        i['old_policy_log_probs'] = np.array([tf.nn.softmax(x).numpy() for x in policy_logits.numpy()])    \n",
    "    episode_data.append(breeding_step)\n",
    "    values.append([x.numpy()[0] for x in value])\n",
    "\n",
    "\n",
    "advantages = calculate_advantages(agent,episode_data)\n",
    "old_policy_log_probs = tf.stack(old_policy_log_probs, axis=0)\n",
    "old_policy_log_probs = tf.transpose(old_policy_log_probs, perm=[1, 0, 2])\n",
    "\n",
    "# Define the optimizer for both actor and critic networks\n",
    "optimizer_actor = tf.keras.optimizers.Adam(learning_rate=config['learning_rate'])\n",
    "optimizer_critic = tf.keras.optimizers.Adam(learning_rate=config['learning_rate'])\n",
    "\n",
    "# Define the number of optimization epochs and the clipping parameter\n",
    "optimization_epochs = 10\n",
    "epsilon_clip = 0.2\n",
    "\n",
    "# Convert advantages to a tensor\n",
    "advantages = tf.convert_to_tensor(advantages, dtype=tf.float32)\n",
    "\n",
    "# Flatten the advantages to match the policy's output shape\n",
    "advantages = tf.reshape(advantages, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization loop\n",
    "for epoch in range(1):\n",
    "    with tf.GradientTape() as tape_actor, tf.GradientTape() as tape_critic:\n",
    "        # Initialize lists to store the ratios and value losses for each cycle\n",
    "        policy_loss_components = []\n",
    "        value_loss_components = []\n",
    "        \n",
    "        # Loop over each cycle in the recorded episode\n",
    "        for cycle in range(agent.cycles):\n",
    "            # Use the state and score at the specific cycle\n",
    "            cycle_data = episode_data[cycle]\n",
    "\n",
    "            recorded_pop = np.array([x['new_population'] for x in cycle_data])\n",
    "            recorded_score = np.array([x['new_score'] for x in cycle_data])\n",
    "            recorded__log_probs = np.array([x['old_policy_log_probs'] for x in cycle_data])\n",
    "            # Recompute policy and values for the current weights\n",
    "            current_policy_logits = agent.actor([recorded_pop, recorded_score], training=True)\n",
    "            current_values = agent.critic([recorded_pop, recorded_score], training=True)\n",
    "            current_values = tf.reshape(current_values, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified optimization loop for a single step\n",
    "with tf.GradientTape() as tape_actor, tf.GradientTape() as tape_critic:\n",
    "    # Get the recorded information for the single step\n",
    "    recorded_pop = np.array([x['new_population'] for x in episode_data[0]])\n",
    "    recorded_score = np.array([x['new_score'] for x in episode_data[0]])\n",
    "    recorded_log_probs = np.array([x['old_policy_log_probs'] for x in episode_data[0]])\n",
    "\n",
    "    # Recompute policy logits for the current weights\n",
    "    current_policy_logits = agent.actor([recorded_pop, recorded_score], training=True)\n",
    "    \n",
    "    # Compute new log probabilities using the current policy\n",
    "    new_log_probs = tf.math.log(tf.nn.softmax(current_policy_logits))\n",
    "    \n",
    "    # Calculate the ratio between new and old policy probabilities\n",
    "    ratio = tf.exp(new_log_probs - recorded_log_probs)\n",
    "    \n",
    "    # Assume advantages have been calculated already and are stored in a variable named `advantages`\n",
    "    # Compute policy loss as the negative mean of the product of ratios and advantages\n",
    "    policy_loss = -tf.reduce_mean(ratio * advantages[0])\n",
    "    \n",
    "    # Compute value loss using the critic model\n",
    "    current_values = agent.critic([recorded_pop, recorded_score], training=True)\n",
    "    value_loss = tf.reduce_mean((current_values - .01) ** 2)  # `returns` need to be calculated as per your setup\n",
    "\n",
    "    # Calculate gradients and perform backpropagation\n",
    "    policy_gradients = tape_actor.gradient(policy_loss, agent.actor.trainable_variables)\n",
    "    value_gradients = tape_critic.gradient(value_loss, agent.critic.trainable_variables)\n",
    "\n",
    "    # Apply gradients to the actor and critic models\n",
    "    # optimizer_actor.apply_gradients(zip(policy_gradients, agent.actor.trainable_variables))\n",
    "    optimizer_critic.apply_gradients(zip(value_gradients, agent.critic.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
