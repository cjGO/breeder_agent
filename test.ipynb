{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "\n",
    "from rpy2.robjects.packages import importr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from chromax import Simulator\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "import matplotlib\n",
    "from jax import device_get\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_genetic_map(n_markers, n_chromosomes):\n",
    "  df = pd.DataFrame(generate_marker_effects(n_markers=n_markers), columns=['Yield'])\n",
    "  df['cM'] = np.random.uniform(0, 200, len(df))\n",
    "  df['CHR.PHYS'] = '1A'\n",
    "  df = df.sort_values(by='cM')\n",
    "  df = df[['CHR.PHYS', 'cM', 'Yield']]\n",
    "  # save df as csv under filename\n",
    "  return df\n",
    "\n",
    "def generate_population(n_pop=100, n_markers=500):\n",
    "    \"\"\"\n",
    "    Generate a numpy array of randoms of length 500 with randomized 0, 1, or 2 at each position.\n",
    "    It will generate 100 individuals based on n_pop.\n",
    "\n",
    "    Returns: numpy array of size (n_pop, n_markers)\n",
    "    \"\"\"\n",
    "    shape=(n_pop, n_markers, 2)\n",
    "    # Define the elements to choose from and their associated probabilities\n",
    "    elements = [0, 1, 2]\n",
    "    probabilities = [1/3, 1/3, 1/3]  # equal probabilities for 0, 1, and 2\n",
    "\n",
    "    # Generate the population\n",
    "    population = np.random.choice(elements, size=(n_pop, n_markers), p=probabilities)\n",
    "\n",
    "    return np.random.choice([True, False], size=shape)\n",
    "\n",
    "\n",
    "def generate_marker_effects(n_markers=500, mu=0, sigma=0.1):\n",
    "    \"\"\"\n",
    "    Generate a numpy array of marker effects with a normal distribution.\n",
    "\n",
    "    Parameters:\n",
    "    n_markers (int): Number of markers.\n",
    "    mu (float): Mean of the distribution.\n",
    "    sigma (float): Standard deviation of the distribution.\n",
    "\n",
    "    Returns:\n",
    "    numpy array of marker effects\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the marker effects\n",
    "    marker_effects = np.random.normal(mu, sigma, n_markers)\n",
    "\n",
    "    return marker_effects\n",
    "\n",
    "def parse_markerEffects(genetic_map, nChr):\n",
    "    # Get the length of the genetic map\n",
    "    length = len(genetic_map)\n",
    "\n",
    "    # Create a new array for storing the chromosome number for each marker\n",
    "    chr = [0] * length\n",
    "\n",
    "    # Calculate the number of markers per chromosome\n",
    "    markers_per_chr = length // nChr\n",
    "\n",
    "    # Iterate over the range of the genetic map length\n",
    "    for i in range(length):\n",
    "        # Calculate the chromosome number and store it in the chr array\n",
    "        chr[i] = i // markers_per_chr + 1\n",
    "\n",
    "    return chr\n",
    "\n",
    "\n",
    "def reshape_pop(maizeHaplo):\n",
    "    reshapeHaplo = maizeHaplo.reshape(int((maizeHaplo.shape[0])/2),2,maizeHaplo.shape[1])\n",
    "    reshapeHaplo = reshapeHaplo.transpose((0,2,1))\n",
    "    return reshapeHaplo\n",
    "\n",
    "def return_genetic_map_df(markerEffects, nChr, geneticMap):\n",
    "    chr = parse_markerEffects(markerEffects, nChr)\n",
    "    chr = [int(x[0]) for x in chr]\n",
    "    trait = markerEffects\n",
    "    pos = geneticMap\n",
    "    # Assuming chr, trait, pos are your arrays\n",
    "    df = pd.DataFrame({'CHR.PHYS': chr, 'Yield': trait, 'cM': pos})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In addition: Warning message:\n",
       "In (function (package, help, pos = 2, lib.loc = NULL, character.only = FALSE,  :\n",
       "  libraries ‘/usr/local/lib/R/site-library’, ‘/usr/lib/R/site-library’ contain no packages\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "x <- seq(0, 2*pi, length.out=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%R -o x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.12822827, 0.25645654, 0.38468481, 0.51291309,\n",
       "       0.64114136, 0.76936963, 0.8975979 , 1.02582617, 1.15405444,\n",
       "       1.28228272, 1.41051099, 1.53873926, 1.66696753, 1.7951958 ,\n",
       "       1.92342407, 2.05165235, 2.17988062, 2.30810889, 2.43633716,\n",
       "       2.56456543, 2.6927937 , 2.82102197, 2.94925025, 3.07747852,\n",
       "       3.20570679, 3.33393506, 3.46216333, 3.5903916 , 3.71861988,\n",
       "       3.84684815, 3.97507642, 4.10330469, 4.23153296, 4.35976123,\n",
       "       4.48798951, 4.61621778, 4.74444605, 4.87267432, 5.00090259,\n",
       "       5.12913086, 5.25735913, 5.38558741, 5.51381568, 5.64204395,\n",
       "       5.77027222, 5.89850049, 6.02672876, 6.15495704, 6.28318531])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %R install.packages(\"AlphaSimR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading required package: R6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "library(\"AlphaSimR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "nInd = 10\n",
    "nChr = 2\n",
    "segSites = 5\n",
    "\n",
    "founderGenomes = runMacs(nInd = nInd,\n",
    "                         nChr = nChr,\n",
    "                         segSites = segSites,\n",
    "                         species = \"MAIZE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "SP = SimParam$new(founderGenomes)\n",
    "SP$addTraitA(segSites)\n",
    "# SP$setVarE(h2=.02)\n",
    "pop = newPop(founderGenomes, simParam=SP)\n",
    "ans = fastRRBLUP(pop, simParam=SP, useQtl=TRUE, use='gv')\n",
    "ans@gv[[1]]@addEff\n",
    "markerEffects = slot(slot(ans, \"gv\")[[1]], \"addEff\")\n",
    "maizeHaplo = pullSegSiteHaplo(pop)\n",
    "maizeGeno = pullSegSiteGeno(pop)\n",
    "#cm positions of each marker\n",
    "genMap = SP$genMap\n",
    "geneticMap = unlist(genMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%R -o maizeHaplo\n",
    "%R -o maizeGeno\n",
    "%R -o markerEffects\n",
    "%R -o geneticMap\n",
    "%R -o nInd\n",
    "%R -o nChr\n",
    "%R -o segSites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nMarkers = segSites * nChr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 23:58:54.523415: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Reshape\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import random\n",
    "\n",
    "def simplify_geneticmap(lst, qtl):\n",
    "    # Select 'qtl' random indexes\n",
    "    indexes_positive = random.sample(range(len(lst)), qtl)\n",
    "    \n",
    "    # Select 'qtl' random indexes not already selected\n",
    "    remaining_indexes = set(range(len(lst))) - set(indexes_positive)\n",
    "    indexes_negative = random.sample(remaining_indexes, qtl)\n",
    "\n",
    "    # Modify the list\n",
    "    for i in range(len(lst)):\n",
    "        if i in indexes_positive:\n",
    "            lst[i] = random.uniform(0.5, 1.0)  # Assign random float between 0.5 and 1.0\n",
    "        elif i in indexes_negative:\n",
    "            lst[i] = random.uniform(-0.5, -1.0)  # Assign random float between -0.5 and -1.0\n",
    "        else:\n",
    "            lst[i] = 0  # Assign 0\n",
    "\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_population_heatmap(ax, population_data, marker_strength):\n",
    "    summed_data = np.sum(population_data, axis=2)\n",
    "    cmap = sns.color_palette([\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n",
    "    strength_cmap = matplotlib.colormaps.get_cmap('RdYlGn')  # Updated line\n",
    "    sns.heatmap(summed_data, cmap=cmap, cbar_kws={'ticks': [0, 1, 2]}, ax=ax)\n",
    "\n",
    "    ax.set_xticks(np.arange(population_data.shape[1]) + 0.5)\n",
    "    marker_labels = [f'{i+1}' for i in range(population_data.shape[1])]\n",
    "    ax.set_xticklabels(marker_labels, rotation=0, ha='center')\n",
    "\n",
    "    for tick_label, strength in zip(ax.get_xticklabels(), marker_strength):\n",
    "        tick_label.set_backgroundcolor(strength_cmap((strength + 1) / 2))  # Normalize and map the strength value\n",
    "        tick_label.set_color('white')\n",
    "        tick_label.set_fontweight('bold')\n",
    "        tick_label.set_bbox(dict(facecolor=strength_cmap((strength + 1) / 2), edgecolor='none', boxstyle='round,pad=0.3'))\n",
    "\n",
    "    ax.set_title(\"Heatmap of Population Genotype Dosages\")\n",
    "    ax.set_xlabel(\"Markers\")\n",
    "    ax.set_ylabel(\"Individuals\")\n",
    "\n",
    "def plot_allele_frequencies(ax, data, marker_strength, sort_by_allele_0=False, bar_width=0.8):\n",
    "    allele_counts = np.apply_along_axis(lambda x: np.bincount(x, minlength=3), axis=2, arr=data)\n",
    "    total_allele_counts = allele_counts.sum(axis=0)\n",
    "    allele_frequencies = total_allele_counts / total_allele_counts.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    allele_0_proportions = allele_frequencies[:, 0]\n",
    "    allele_1_proportions = allele_frequencies[:, 1]\n",
    "    x_positions = np.arange(1, len(allele_0_proportions) + 1)\n",
    "    normalized_strength = (np.array(marker_strength) + 1) / 2\n",
    "\n",
    "    cmap = plt.cm.RdYlGn\n",
    "\n",
    "    if sort_by_allele_0:\n",
    "        sorted_indices = np.argsort(-allele_0_proportions)\n",
    "        allele_0_proportions = allele_0_proportions[sorted_indices]\n",
    "        allele_1_proportions = allele_1_proportions[sorted_indices]\n",
    "        normalized_strength = normalized_strength[sorted_indices]\n",
    "\n",
    "    for xpos, a0, a1, strength in zip(x_positions, allele_0_proportions, allele_1_proportions, normalized_strength):\n",
    "        ax.bar(xpos, a0, color='red', edgecolor='black', width=bar_width, label='Allele 0' if xpos == 1 else \"\")\n",
    "        ax.bar(xpos, a1, bottom=a0, color='black', edgecolor='black', width=bar_width, label='Allele 1' if xpos == 1 else \"\")\n",
    "        ax.text(xpos, -0.05, f'{xpos}', horizontalalignment='center', verticalalignment='center', \n",
    "                 color='white', fontsize=8, fontweight='bold', \n",
    "                 bbox=dict(facecolor=cmap(strength), edgecolor='none', boxstyle='round,pad=0.2'))\n",
    "\n",
    "    ax.set_ylim(-0.15, 1)\n",
    "    ax.set_ylabel('Proportion')\n",
    "    ax.set_xlabel('Marker Position')\n",
    "    ax.set_title('Proportion of Alleles 0 and 1 at Each Marker' + (' (Sorted by Allele 0)' if sort_by_allele_0 else ''))\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def create_frame(farm, episode):\n",
    "    temp_dir = farm.temp_dir\n",
    "    fig = plt.figure(figsize=(10, 15))  # Adjust the figure size\n",
    "\n",
    "    gs = gridspec.GridSpec(3, 1, figure=fig)\n",
    "\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    farm.view_policy(ax1, episode)\n",
    "\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    plot_population_heatmap(ax2, self.current_population, self.marker_strength)\n",
    "\n",
    "    ax3 = fig.add_subplot(gs[2])\n",
    "    plot_allele_frequencies(ax3, self.current_population, self.marker_strength)\n",
    "\n",
    "    # Save the figure as a png in the temporary directory\n",
    "    filename = os.path.join(temp_dir.name, f'frame_{episode}.png')\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)  # Close the plot\n",
    "\n",
    "    return filename  # Return the filename for future use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 638.4M\n",
      "Memory usage: 638.4M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1638342/1764213802.py:159: DeprecationWarning: Please use `gaussian_filter1d` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  from scipy.ndimage.filters import gaussian_filter1d\n"
     ]
    }
   ],
   "source": [
    "#VISUALIZATIONS\n",
    "\n",
    "\n",
    "\n",
    "#average fitness for each episode\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_initial_population_policy_tracker_updated(dataset, indices, true_values, ax=None):\n",
    "    def top_50_percent_indices(array):\n",
    "        \"\"\"Helper function to find the top 50% indices of an array.\"\"\"\n",
    "        threshold = np.percentile(array, 50)\n",
    "        return np.where(array > threshold)\n",
    "\n",
    "    # Plotting\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))  # Create a new figure and axes if none is provided\n",
    "\n",
    "    # Preparing the data for imshow - adding the 'true_values' as the first column\n",
    "    data_for_plotting = np.column_stack([true_values.squeeze()] + [d.squeeze() for d in dataset])\n",
    "\n",
    "    # Displaying the arrays as columns with appropriate coloring\n",
    "    ax.imshow(data_for_plotting, aspect='auto', cmap=plt.cm.viridis)\n",
    "\n",
    "    # Adding red rectangles for top 50% values in each array, including 'true_values'\n",
    "    all_data = [true_values] + dataset\n",
    "    for i, array in enumerate(all_data):\n",
    "        top_indices = top_50_percent_indices(array.squeeze())\n",
    "        for index in top_indices[0]:\n",
    "            rect = plt.Rectangle((i-0.5, index-0.5), 1, 1, edgecolor='red', facecolor='none', lw=3)\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    # Setting the axis labels and title\n",
    "    ax.set_xlabel(\"Episode Number\")\n",
    "    ax.set_ylabel(\"Individual ID\")\n",
    "\n",
    "    # Setting x-axis tick labels to include 'Truth' for the first column and sampled indices for the rest\n",
    "    tick_labels = ['Truncation'] + [f\"{idx+1}\" for idx in indices]\n",
    "    plt.xticks(range(len(all_data)), tick_labels)\n",
    "\n",
    "    plt.yticks(range(len(true_values.squeeze())), range(len(true_values.squeeze())))\n",
    "\n",
    "    # Adding grid lines\n",
    "    ax.grid(True, which='both', axis='both', linestyle='-', color='lightgrey', linewidth=0.5)\n",
    "\n",
    "    ax.set_title(\"Initial Population Policy Tracker\")\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sample_dataset_by_percentile(dataset, percentile):\n",
    "    \"\"\"\n",
    "    Samples a dataset based on a given percentile and returns the sampled data along with their original indices.\n",
    "\n",
    "    :param dataset: List or array-like structure containing the dataset.\n",
    "    :param percentile: The percentile step to use for sampling (e.g., 10 for 10th percentile steps).\n",
    "\n",
    "    :return: A tuple containing two lists - the sampled dataset and their corresponding indices.\n",
    "    \"\"\"\n",
    "    if not 0 < percentile <= 100:\n",
    "        raise ValueError(\"Percentile must be between 0 and 100\")\n",
    "\n",
    "    num_elements = len(dataset)\n",
    "    step = int(num_elements * (percentile / 100))\n",
    "    \n",
    "    # Handle the case where step size is 0 due to a very small dataset\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "\n",
    "    # Sampling indices\n",
    "    sampled_indices = range(0, num_elements, step)\n",
    "\n",
    "    # Sampling the dataset\n",
    "    sampled_dataset = [dataset[i] for i in sampled_indices]\n",
    "\n",
    "    return sampled_dataset, list(sampled_indices)\n",
    "\n",
    "def baseline_plot(ax=None):\n",
    "    total_cycles = farm.total_cycles\n",
    "    trunk_farm = farm\n",
    "    for i in range(total_cycles):\n",
    "        truncate_cycle(trunk_farm)\n",
    "\n",
    "    data = [x.to_numpy().flatten() for x in trunk_farm.history]\n",
    "    df = pd.DataFrame(data).T\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))  # Create a new figure and axes if none is provided\n",
    "\n",
    "    sns.boxplot(data=df, ax=ax)\n",
    "\n",
    "    ax.set_title('Baseline Heuristic')  # Add title\n",
    "    ax.set_xlabel('Episode Number')  # Add x-axis label\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# sampled_data, sampled_indices  = sample_dataset_by_percentile(farm.fitness_history, 10)\n",
    "# plot_initial_population_policy_tracker_updated(sampled_data, sampled_indices, farm.simulator.GEBV(farm.initial_population) )\n",
    "\n",
    "# baseline_plot()\n",
    "\n",
    "# fitness_plot()\n",
    "\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Function to convert large number of bytes into a human readable format\n",
    "def bytes_to_human(n):\n",
    "    symbols = ('K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n",
    "    prefix = {}\n",
    "    for i, s in enumerate(symbols):\n",
    "        prefix[s] = 1 << (i + 1) * 10\n",
    "    for s in reversed(symbols):\n",
    "        if n >= prefix[s]:\n",
    "            value = float(n) / prefix[s]\n",
    "            return '%.1f%s' % (value, s)\n",
    "    return \"%sB\" % n\n",
    "\n",
    "# Function to print out memory usage\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(f\"Memory usage: {bytes_to_human(process.memory_info().rss)}\")\n",
    "    return float(bytes_to_human(process.memory_info().rss)[:-1])\n",
    "\n",
    "# Print out memory usage at the start\n",
    "print_memory_usage()\n",
    "\n",
    "# Your code here...\n",
    "# ...\n",
    "\n",
    "# Print out memory usage at the end\n",
    "print_memory_usage()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# policy = actor_output\n",
    "# current_population = population_dummy\n",
    "# simulator = simulator\n",
    "# newpop, reward  = policy2offspring_reward(policy, current_population, simulator)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "\n",
    "import pandas as pd                                                \n",
    "\n",
    "def fitness_plot(farm_history, farm_cycles, ax=None):\n",
    "    farm_history = np.array(farm_history)\n",
    "    chunk_size = farm_cycles\n",
    "    data_list = farm_history\n",
    "    chunked_list = [data_list[i:i + chunk_size] for i in range(0, len(data_list), chunk_size)]\n",
    "    palette = sns.color_palette(\"husl\", chunk_size)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5)) \n",
    "\n",
    "    for i in range(data_list.shape[1]):  # iterate over columns\n",
    "        cycle = data_list[:, i]  # get the i-th cycle\n",
    "        cycle_df = pd.DataFrame(cycle)\n",
    "        window_size = int(len(cycle_df) * 0.1)  # 10% of the array length\n",
    "        cycle_smooth = cycle_df.rolling(window_size).mean()  # Apply rolling average\n",
    "        ax.plot(cycle_smooth, color=palette[i], label=f'cycle {i+1}')  \n",
    "\n",
    "    for i, baseline in enumerate(farm.baselines):\n",
    "        ax.axhline(y=baseline, color='g', linestyle='--', label=f'Heuristic Baseline {i}')  \n",
    "        ax.annotate(f'Heuristic Baseline {i}', (0, baseline), textcoords=\"offset points\", xytext=(10,10), ha='center')  \n",
    "\n",
    "    ax.axhline(y=farm.random_baseline, color='r', linestyle='--', label='Random Action Baseline')  \n",
    "    ax.annotate(f'Random Action Baseline', (0, farm.random_baseline), textcoords=\"offset points\", xytext=(10,10), ha='center')  \n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Reshape\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "#dummy functions to  generate fake data to develop the training pipeline\n",
    "def pop_gen(b, n, m, d):\n",
    "    return np.random.randint(2, size=(b, n, m, d))\n",
    "def reward_gen():\n",
    "    return np.random.rand\n",
    "def scores_gen(n):\n",
    "    return np.random.rand(1, n)\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "def get_r(values):\n",
    "\n",
    "    # Create an array of indices, same length as your list\n",
    "    indices = np.array(range(len(values)))\n",
    "\n",
    "    # Perform linear regression\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(indices, values)\n",
    "    return slope\n",
    "\n",
    "# genetic_map = return_genetic_map_df(markerEffects, nChr, geneticMap)\n",
    "# genetic_map['Yield'] = simplify_geneticmap(list(genetic_map['Yield']),4)\n",
    "\n",
    "# n = int(nInd[0])\n",
    "# m = int(nMarkers)\n",
    "# d = 2\n",
    "# total_parents = n*2\n",
    "\n",
    "# population_dummy = pop_gen(1, n, m, d)  # Extra dimension for batch size\n",
    "# scores_dummy =  scores_gen(n) # Extra dimension for batch size\n",
    "\n",
    "# actor_model = create_actor(n,m,d,total_parents, population_dummy, scores_dummy)\n",
    "# actor_output = actor_model([population_dummy, scores_dummy])\n",
    "# critic_model = create_critic(n,m,d, population_dummy, scores_dummy,actor_output)\n",
    "\n",
    "# simulator = Simulator(genetic_map=genetic_map, h2=.5)\n",
    "# simulator.load_population('mypop.npy')\n",
    "# initial_scores = simulator.GEBV(population_dummy[0])\n",
    "\n",
    "# TOTAL_EPISODES=3\n",
    "# before = print_memory_usage()\n",
    "# for episodes in range(TOTAL_EPISODES):\n",
    "#     inputs = tf.random.normal([1, n, m, d]) # replace with actual inputs\n",
    "#     scores_dummy = tf.random.normal([1, n]) # replace with actual scores\n",
    "#     #policy, current_population, simulator\n",
    "#     new_pop,reward = policy2offspring_reward(actor_output, population_dummy, simulator)\n",
    "#     train_step(actor_model, critic_model, inputs, scores_dummy, reward)\n",
    "#     tf.keras.backend.clear_session()\n",
    "# after = print_memory_usage()\n",
    "\n",
    "# print(f'cost per episode: {( after - before ) / TOTAL_EPISODES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def split_and_average(lst, set_size):\n",
    "    # lst is from get_baseline\n",
    "    #average list\n",
    "    lst = [x.mean() for x in lst]\n",
    "    # Split the list into sublists\n",
    "    split_lst = [lst[i:i + set_size] for i in range(0, len(lst), set_size)]\n",
    "\n",
    "    # Calculate the average of the x-th element in each sublist\n",
    "    averages = []\n",
    "    for x in range(set_size):\n",
    "        x_elements = [sublist[x] for sublist in split_lst if len(sublist) > x]\n",
    "        averages.append(mean(x_elements))\n",
    "\n",
    "    return averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def select_parents(policy):\n",
    "#     \"\"\"\n",
    "#         input: Policy from actor ( metric value for each individual )\n",
    "#         returns : index of parents to be included in random_crosses for next step of breeding program\n",
    "#     \"\"\"\n",
    "#     # Calculate the number of top elements to select.\n",
    "#     k = policy.shape[1] // 2\n",
    "#     # If the tensor has an odd number of elements, add one to 'k' to get the upper half.\n",
    "#     if policy.shape[1] % 2 != 0:\n",
    "#         k += 1\n",
    "#     values, indices = tf.math.top_k(policy, k)\n",
    "#     return values,indices\n",
    "\n",
    "# def create_actor(n, m, d, layer_size,learning_rate):\n",
    "#     \"\"\"\n",
    "#         input:\n",
    "#         n : population size\n",
    "#         m : total markers\n",
    "#         d : ploidy\n",
    "        \n",
    "#         output:\n",
    "#         selection metrics for breeder agent to act on\n",
    "        \n",
    "#     \"\"\"\n",
    "#     score_input = tf.keras.layers.Input(shape=(n,))\n",
    "#     actor_input = tf.keras.layers.Input(shape=(n, m, d))\n",
    "\n",
    "#     x1 = Flatten()(actor_input)\n",
    "#     x2 = Dense(layer_size, activation='relu')(score_input)  # Dense layer for the scores\n",
    "#     x = tf.keras.layers.Concatenate()([x1, x2])  # Concatenate the flattened actor input and the score input\n",
    "#     x = Dense(layer_size, activation='relu')(x)\n",
    "#     x = Dense(layer_size, activation='relu')(x)\n",
    "#     x = Dense(n, activation='linear')(x)  # Output layer with n linear units\n",
    "#     optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "#     actor_model = tf.keras.models.Model([actor_input, score_input], x)\n",
    "#     actor_model.compile(optimizer=optimizer, loss='mean_squared_error')  # Use MSE loss for value prediction\n",
    "\n",
    "#     return actor_model\n",
    "\n",
    "# def create_critic(n, m, d, layer_size,learning_rate):\n",
    "#     \"\"\"\n",
    "#         input:\n",
    "#         n : population size\n",
    "#         m : total markers\n",
    "#         d : ploidy\n",
    "\n",
    "#         output:\n",
    "#         scalar predicted reward (given)\n",
    "        \n",
    "#     \"\"\"\n",
    "#     score_input = tf.keras.layers.Input(shape=(n,))\n",
    "#     actor_output_input = tf.keras.layers.Input(shape=(n,))\n",
    "#     critic_input = tf.keras.layers.Input(shape=(n, m, d))\n",
    "\n",
    "#     x1 = Flatten()(critic_input)\n",
    "#     x2 = Dense(layer_size, activation='relu')(score_input)  # Dense layer for the scores\n",
    "#     x3 = Flatten()(actor_output_input)  # Flatten the actor's output\n",
    "#     x = tf.keras.layers.Concatenate()([x1, x2, x3])  # Concatenate the flattened critic input, score input, and actor's output\n",
    "#     x = Dense(layer_size, activation='relu')(x)\n",
    "#     x = Dense(layer_size, activation='relu')(x)\n",
    "#     x = Dense(1, activation='linear')(x)  # Output layer with single linear unit\n",
    "#     optimizer = Adam(learning_rate=learning_rate)\n",
    "#     critic_model = tf.keras.models.Model([critic_input, score_input, actor_output_input], x)\n",
    "#     critic_model.compile(optimizer=optimizer, loss='mean_squared_error')  # Use MSE loss for value prediction\n",
    "\n",
    "#     return critic_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class BreedingProgram:\n",
    "#     \"\"\"\n",
    "#     Represents a breeding program with a PPO agent. Actor-Critic\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, cook_base, initial_population, genetic_map, population_size, marker_count, chromosome_number, max_generation, heritability, learning_rate =.001, layer_size=8, episodes = 50, cycles=2):\n",
    "#         \"\"\"\n",
    "#         Initializes the breeding program.\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Initialize the basic attributes\n",
    "#         self.cook_base = cook_base\n",
    "#         self.population_size = population_size\n",
    "#         self.marker_count = marker_count\n",
    "#         self.initial_population = initial_population\n",
    "#         self.genetic_map = genetic_map\n",
    "#         self.max_generation = max_generation\n",
    "#         self.marker_strength = np.array(self.genetic_map['Yield'])\n",
    "#         self.episodes = episodes\n",
    "#         self.cycles = cycles\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.layer_size = layer_size\n",
    "\n",
    "#         # Initialize the simulator\n",
    "#         self.simulator = Simulator(genetic_map=self.genetic_map, h2=heritability)\n",
    "#         self.simulator.load_population('mypop.npy')\n",
    "#         self.initial_scores = self.simulator.GEBV(self.initial_population)\n",
    "\n",
    "#         #tempdir - future for capturing gifs\n",
    "#         self.temp_dir = tempfile.TemporaryDirectory()\n",
    "\n",
    "#         # Initialize the current generation and history\n",
    "#         self.current_generation = 0\n",
    "#         self.history = []\n",
    "\n",
    "#         # Initialize the Actor and Critic models\n",
    "#         self.actor = create_actor(n=self.population_size,\n",
    "#                                   m=self.marker_count,\n",
    "#                                   d=2,\n",
    "#                                   layer_size=self.layer_size,\n",
    "#                                   learning_rate = self.learning_rate,\n",
    "#                                   )\n",
    "#         population_dummy = np.random.rand(1, self.population_size,self.marker_count,2)  # Extra dimension for batch size\n",
    "#         scores_dummy = np.random.rand(1, self.population_size)  # Extra dimension for batch size\n",
    "#         output_dummy = self.actor([population_dummy, scores_dummy])\n",
    "\n",
    "#         self.critic = create_critic(n=self.population_size,\n",
    "#                                   m=self.marker_count,\n",
    "#                                   d=2,\n",
    "#                                                                     layer_size=self.layer_size,\n",
    "#                                   learning_rate = self.learning_rate,\n",
    "\n",
    "#         )\n",
    "        \n",
    "\n",
    "\n",
    "#         self.critic_history = []\n",
    "#         self.actor_history = []\n",
    "#         self.reward_history = []\n",
    "#         self.fitness_history = []\n",
    "#         self.policy_history = []\n",
    "\n",
    "#         self.true_history = [] # stores the mean reward for a given mass selection round\n",
    "\n",
    "#                 # Start the breeding program\n",
    "#         self._start_breeding_program()\n",
    "        \n",
    "#     def _start_breeding_program(self):\n",
    "#         \"\"\"\n",
    "#         Starts the breeding program.\n",
    "#         \"\"\"\n",
    "#         self.current_population = self.initial_population\n",
    "#         self.current_scores = self.simulator.GEBV(self.initial_population)\n",
    "#         self.history.append(self.current_scores)\n",
    "#         self.agent_history = []\n",
    "#         if self.cook_base:\n",
    "#             self.get_baselines()\n",
    "#             self.snapshot_history = []\n",
    "#             self.snapshot_history.append(self.actor([self.initial_population.reshape(1, *self.initial_population.shape), self.initial_scores.to_numpy().transpose()], training=True).numpy())\n",
    "#             self.random_baseline = self.get_random_baseline()\n",
    "#     # USE AGENT\n",
    "        \n",
    "\n",
    "    \n",
    "#     def agent_select(self,population):\n",
    "#         \"\"\"\n",
    "#         used to test the existing agent given a population (n,m,d)\n",
    "#         \"\"\"\n",
    "#         # Prepare inputs for actor model\n",
    "#         current_pop = device_get(population.reshape(1, *population.shape))\n",
    "#         current_scores = self.simulator.phenotype(current_pop[0])\n",
    "#         current_scores = current_scores.to_numpy().transpose()\n",
    "\n",
    "#         # Get policy and select parents\n",
    "#         policy = self.actor([current_pop, current_scores])\n",
    "\n",
    "#         return policy\n",
    "\n",
    "#     def truncate_cycle(self):\n",
    "#         #TODO ADD N_CROSSES to sample multiple times from same parent combos -> get average reward IMPORTANT?!\n",
    "#         selected_parents = self.simulator.select(self.current_population, k = (self.current_population.shape[0] / 2))\n",
    "#         new_pop = self.simulator.random_crosses(selected_parents, n_crosses=self.current_population.shape[0])\n",
    "#         new_score = self.simulator.GEBV(new_pop)\n",
    "#         return new_pop, new_score\n",
    "    \n",
    "#     def get_baseline(self, ax=None):\n",
    "#         data = []\n",
    "#         for _ in range(5): #5 samples for baseline target score\n",
    "#             self.reset()\n",
    "#             for i in range(self.cycles):\n",
    "#                 new_pop, new_score = self.truncate_cycle()\n",
    "#                 data.append(new_score)\n",
    "#                 self.current_population = new_pop\n",
    "#                 self.current_scores = new_score\n",
    "#         data = [x.to_numpy().flatten() for x in data]\n",
    "#         df = pd.DataFrame(data).T\n",
    "\n",
    "#         if ax is None:\n",
    "#             return None, data  # Create a new figure and axes if none is provided\n",
    "\n",
    "#         sns.boxplot(data=df, ax=ax)\n",
    "\n",
    "#         ax.set_title('Baseline Heuristic Replicates')  # Add title\n",
    "#         ax.set_xlabel('Episode Number')  # Add x-axis label\n",
    "\n",
    "#         return ax, data\n",
    "    \n",
    "#     @tf.function\n",
    "#     def train_step(self, inputs, scores_dummy,reward):\n",
    "#         with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:\n",
    "#             actor_output = self.actor([inputs, scores_dummy], training=True)\n",
    "#             critic_value = self.critic([inputs, scores_dummy, actor_output], training=True)\n",
    "#             actor_loss = -tf.reduce_mean(critic_value)\n",
    "#             critic_loss = tf.keras.losses.huber(reward, critic_value)\n",
    "        \n",
    "#         actor_grads = actor_tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "#         critic_grads = critic_tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "\n",
    "#         return (actor_grads, critic_grads), actor_loss, critic_loss\n",
    "    \n",
    "#     def policy2offspring_reward(self, policy, replicates=30):\n",
    "#         \"\"\"\n",
    "#         Runs multiple replicates for a given policy to calculate the average reward.\n",
    "#         \"\"\"\n",
    "#         avg_reward = 0\n",
    "#         for _ in range(replicates):\n",
    "#             parent_values, parent_indices = select_parents(policy)\n",
    "#             selected_parents = self.current_population[parent_indices.numpy()]\n",
    "#             # Ensure that the new population is generated for each replicate\n",
    "#             new_pop = self.simulator.random_crosses(selected_parents[0], n_crosses=selected_parents[0].shape[0])\n",
    "#             # Calculate the reward for the new population\n",
    "#             mass_score = self.simulator.GEBV(new_pop).mean()[0]\n",
    "#             reward = mass_score - self.simulator.GEBV(self.initial_population).mean()[0]\n",
    "#             avg_reward += reward\n",
    "#         avg_reward /= replicates\n",
    "#         # Return the last new_pop, though you might want to consider how to handle multiple populations\n",
    "#         new_pop = np.vstack((new_pop, self.simulator.random_crosses(selected_parents[0], n_crosses=selected_parents[0].shape[0])))\n",
    "\n",
    "#         return new_pop, avg_reward\n",
    "\n",
    "\n",
    "        \n",
    "#     def run_cycle(self, replicates=30):\n",
    "#         \"\"\"\n",
    "#         runs a cycle of selection. updates the current population/scores and does training step\n",
    "#         \"\"\"\n",
    "#         inputs = add_batchdim(self.current_population)\n",
    "#         scores_dummy = np.array(self.current_scores.transpose())\n",
    "#         actor_output = self.actor([inputs, scores_dummy])\n",
    "#         # Run policy2offspring_reward with multiple replicates\n",
    "#         new_pop, avg_reward = self.policy2offspring_reward(actor_output, replicates)\n",
    "#         # Use average reward for training\n",
    "#         grads, actor_loss, critic_loss = self.train_step(inputs, scores_dummy, avg_reward)\n",
    "#         # Reshape new_pop if necessary, depending on your simulator's implementation\n",
    "#         # ...\n",
    "#         self.critic_history.append(critic_loss.numpy()[0])\n",
    "#         self.actor_history.append(actor_loss.numpy())\n",
    "#         self.reward_history.append(avg_reward)\n",
    "#         # Assuming mass_score is equivalent to avg_reward in this context\n",
    "#         self.fitness_history.append(avg_reward)\n",
    "#         self.current_population = new_pop\n",
    "#         self.current_scores = self.simulator.GEBV(new_pop)\n",
    "#         return avg_reward, self.current_scores, grads\n",
    "\n",
    "#     def get_baselines(self):\n",
    "#         ax, data = self.get_baseline()\n",
    "#         baselines = split_and_average(data,self.cycles)\n",
    "#         self.baselines = baselines\n",
    "#         return baselines\n",
    "    \n",
    "#     def reset(self):\n",
    "#         self.current_population = self.initial_population\n",
    "#         self.current_scores = self.initial_scores\n",
    "#         self.actor_history = []\n",
    "#         self.critic_history = []\n",
    "#         self.reward_history = []\n",
    "#         self.fitness_history = []\n",
    "\n",
    "#     def get_random_baseline(self):\n",
    "#         peaks = []\n",
    "#         new_pop = self.simulator.random_crosses(self.initial_population, n_crosses=self.initial_population.shape[0])\n",
    "#         averages = []\n",
    "#         prev_avg = None\n",
    "\n",
    "#         for _ in range(30): # 30 replicates of up to 50 episodes each\n",
    "#             new_pop = self.simulator.random_crosses(self.initial_population, n_crosses=self.initial_population.shape[0])\n",
    "#             for _ in range(50):\n",
    "#                 new_pop = self.simulator.random_crosses(new_pop, n_crosses=self.initial_population.shape[0])\n",
    "#                 new_score = self.simulator.GEBV(new_pop)\n",
    "#                 averages.append(new_score)\n",
    "                \n",
    "#                 # If we have at least 5 elements, calculate the average of the last 5\n",
    "#                 if len(averages) >= 5:\n",
    "#                     current_avg = np.average(averages[-5:])\n",
    "                    \n",
    "#                     # If the previous average is not None and is the same as the current average, break the loop\n",
    "#                     if prev_avg is not None and prev_avg == current_avg:\n",
    "#                         peaks.append(current_avg)\n",
    "#                         break\n",
    "\n",
    "#                     # Update the previous average\n",
    "#                     prev_avg = current_avg\n",
    "#         return np.array(peaks).mean()\n",
    "#     def overfit(self, num_replicates=10, num_episodes=50, num_cycles=2):\n",
    "#         # Initialize 3D arrays to store metrics for each replicate, episode, and cycle\n",
    "#         actor_losses = np.zeros((num_replicates, num_episodes, num_cycles))\n",
    "#         critic_losses = np.zeros((num_replicates, num_episodes, num_cycles))\n",
    "#         rewards = np.zeros((num_replicates, num_episodes, num_cycles))\n",
    "\n",
    "#         for replicate in range(num_replicates):\n",
    "#             self.reset()  # Reset the program for each replicate\n",
    "\n",
    "#             for episode in range(num_episodes):\n",
    "\n",
    "#                 for cycle in range(num_cycles):\n",
    "#                     # Run a single cycle of the breeding program\n",
    "#                     reward, _, (actor_loss, critic_loss) = self.run_cycle()\n",
    "\n",
    "#                     # Convert actor_loss and critic_loss to numpy arrays if they are not already\n",
    "\n",
    "#                     actor_loss = [x.numpy() for x in actor_loss]\n",
    "#                     print(actor_loss)\n",
    "#                     print(len(actor_loss[0]))\n",
    "#                     # # actor_loss = [x.numpy()[0] for x in actor_loss]\n",
    "#                     # # critic_loss =[x.numpy()[0] for x in critic_loss]\n",
    "\n",
    "#                     # # Ensure actor_loss and critic_loss are scalars\n",
    "#                     # actor_loss_scalar = actor_loss.item() if actor_loss.shape == () else actor_loss.mean()\n",
    "#                     # critic_loss_scalar = critic_loss.item() if critic_loss.shape == () else critic_loss.mean()\n",
    "\n",
    "#                     # # Store scalar metrics in their respective arrays\n",
    "#                     # actor_losses[replicate, episode, cycle] = actor_loss_scalar\n",
    "#                     # critic_losses[replicate, episode, cycle] = critic_loss_scalar\n",
    "#                     # rewards[replicate, episode, cycle] = reward\n",
    "\n",
    "#         # Return a dictionary of metrics arrays\n",
    "#         return {\n",
    "#             'actor_loss': actor_losses,\n",
    "#             'critic_loss': critic_losses,\n",
    "#             'reward': rewards\n",
    "#         }\n",
    "\n",
    "\n",
    "\n",
    "# metrics = farm.overfit(num_replicates=num_replicates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_two_offspring_per_cross(dataset):\n",
    "    \"\"\"\n",
    "    Samples two offspring per cross from the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (np.ndarray): The input dataset with shape (n_cross, n_offspring, markers, diploid).\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A new array with sampled offspring, shape (n_cross, 2, markers, diploid).\n",
    "    \"\"\"\n",
    "    n_cross = dataset.shape[0]\n",
    "    n_offspring_per_cross = 2\n",
    "    # Create an array to store the indices of the offspring to sample\n",
    "    indices = np.random.choice(dataset.shape[1], size=(n_cross, n_offspring_per_cross), replace=False)\n",
    "    # Use advanced indexing to select two offspring per cross\n",
    "    sampled_offspring = dataset[np.arange(n_cross)[:, None], indices, :, :]\n",
    "    return sampled_offspring.reshape(-1, sampled_offspring.shape[2], sampled_offspring.shape[3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def collapse_first_two_axes(dataset):\n",
    "    \"\"\"\n",
    "    Collapses the first two axes of the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (np.ndarray): The input dataset with shape (n_cross, n_offspring, markers, diploid).\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A new array with the first two axes collapsed, shape (n_cross * n_offspring, markers, diploid).\n",
    "    \"\"\"\n",
    "    # Calculate the new shape\n",
    "    new_shape = (dataset.shape[0] * dataset.shape[1],) + dataset.shape[2:]\n",
    "    # Reshape the dataset\n",
    "    collapsed_dataset = dataset.reshape(new_shape)\n",
    "    return collapsed_dataset\n",
    "\n",
    "\n",
    "def select_parents(policy):\n",
    "    \"\"\"\n",
    "        input: Policy from actor ( metric value for each individual )\n",
    "        returns : index of parents to be included in random_crosses for next step of breeding program\n",
    "    \"\"\"\n",
    "    # Calculate the number of top elements to select.\n",
    "    k = policy.shape[1] // 2\n",
    "    # If the tensor has an odd number of elements, add one to 'k' to get the upper half.\n",
    "    if policy.shape[1] % 2 != 0:\n",
    "        k += 1\n",
    "    values, indices = tf.math.top_k(policy, k)\n",
    "    return values,indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape haplo (10, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any, List\n",
    "\n",
    "\n",
    "def add_batchdim(arr):\n",
    "    return np.expand_dims(arr, axis=0)\n",
    "\n",
    "\n",
    "def create_actor(n, m, d, layer_size,learning_rate):\n",
    "    \"\"\"\n",
    "        input:\n",
    "        n : population size\n",
    "        m : total markers\n",
    "        d : ploidy\n",
    "        \n",
    "        output:\n",
    "        selection metrics for breeder agent to act on\n",
    "        \n",
    "    \"\"\"\n",
    "    score_input = tf.keras.layers.Input(shape=(n,))\n",
    "    actor_input = tf.keras.layers.Input(shape=(n, m, d))\n",
    "\n",
    "    x1 = Flatten()(actor_input)\n",
    "    x2 = Dense(layer_size, activation='relu')(score_input)  # Dense layer for the scores\n",
    "    x = tf.keras.layers.Concatenate()([x1, x2])  # Concatenate the flattened actor input and the score input\n",
    "    x = Dense(layer_size, activation='relu')(x)\n",
    "    x = Dense(layer_size, activation='relu')(x)\n",
    "    x = Dense(n, activation='linear')(x)  # Output layer with n linear units\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    actor_model = tf.keras.models.Model([actor_input, score_input], x)\n",
    "    actor_model.compile(optimizer=optimizer, loss='mean_squared_error')  # Use MSE loss for value prediction\n",
    "\n",
    "    return actor_model\n",
    "\n",
    "def create_critic(n, m, d, layer_size,learning_rate):\n",
    "    \"\"\"\n",
    "        input:\n",
    "        n : population size\n",
    "        m : total markers\n",
    "        d : ploidy\n",
    "\n",
    "        output:\n",
    "        scalar predicted reward (given)\n",
    "        \n",
    "    \"\"\"\n",
    "    score_input = tf.keras.layers.Input(shape=(n,))\n",
    "    actor_output_input = tf.keras.layers.Input(shape=(n,))\n",
    "    critic_input = tf.keras.layers.Input(shape=(n, m, d))\n",
    "\n",
    "    x1 = Flatten()(critic_input)\n",
    "    x2 = Dense(layer_size, activation='relu')(score_input)  # Dense layer for the scores\n",
    "    x3 = Flatten()(actor_output_input)  # Flatten the actor's output\n",
    "    x = tf.keras.layers.Concatenate()([x1, x2, x3])  # Concatenate the flattened critic input, score input, and actor's output\n",
    "    x = Dense(layer_size, activation='relu')(x)\n",
    "    x = Dense(layer_size, activation='relu')(x)\n",
    "    x = Dense(1, activation='linear')(x)  # Output layer with single linear unit\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    critic_model = tf.keras.models.Model([critic_input, score_input, actor_output_input], x)\n",
    "    critic_model.compile(optimizer=optimizer, loss='mean_squared_error')  # Use MSE loss for value prediction\n",
    "\n",
    "    return critic_model\n",
    "\n",
    "\n",
    "\n",
    "class BreederAgent:\n",
    "    \"\"\"\n",
    "    A PPO agent that learns to select parents for breeding programs using Actor-Critic method.\n",
    "    \n",
    "    Attributes:\n",
    "        compute_baselines (bool): Whether to compute baselines for the agent.\n",
    "        population_size (int): The size of the population.\n",
    "        marker_count (int): The number of genetic markers.\n",
    "        initial_population (np.ndarray): The initial population array.\n",
    "        genetic_map (Dict[str, List[float]]): The genetic map of the population.\n",
    "        marker_strength (np.ndarray): The strength of each genetic marker.\n",
    "        replicates (int): The number of replicates to run per training loop.\n",
    "        episodes (int): The number of episodes per replicate.\n",
    "        cycles (int): The number of cycles per episode.\n",
    "        learning_rate (float): The learning rate for both actor and critic.\n",
    "        layer_size (int): The size of hidden layers in both actor and critic.\n",
    "        simulator (Simulator): The breeding simulator.\n",
    "        initial_scores (np.ndarray): The initial scores from the simulator.\n",
    "        actor: The actor model.\n",
    "        critic: The critic model.\n",
    "        ploidy (int): The level of ploidy in the population.\n",
    "    \"\"\"\n",
    "    compute_baselines: bool\n",
    "    population_size: int\n",
    "    marker_count: int\n",
    "    initial_population: np.ndarray\n",
    "    genetic_map: Dict[str, List[float]]\n",
    "    marker_strength: np.ndarray\n",
    "    replicates: int\n",
    "    episodes: int\n",
    "    cycles: int\n",
    "    learning_rate: float\n",
    "    layer_size: int\n",
    "    simulator: Simulator\n",
    "    initial_scores: np.ndarray\n",
    "    actor: Any  # Replace 'Any' with the actual type of the actor model\n",
    "    critic: Any  # Replace 'Any' with the actual type of the critic model\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initializes the breeding program with a configuration dictionary.\n",
    "        \n",
    "        Parameters:\n",
    "        config (dict): Configuration dictionary containing all necessary parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the basic attributes from the config\n",
    "        self.compute_baselines = config.get('compute_baselines', False)\n",
    "        self.population_size = config['population_size']\n",
    "        self.marker_count = config['marker_count']\n",
    "        self.initial_population = config['initial_population']\n",
    "        self.genetic_map = config['genetic_map']\n",
    "        self.marker_strength = np.array(self.genetic_map['Yield'])\n",
    "        self.ploidy = config['ploidy']\n",
    "\n",
    "        # Training Loop Parameters\n",
    "        self.replicates = config.get('replicates', 1) # number of replicates to run per train loop\n",
    "        self.episodes = config['episodes'] # number of episodes per replicate \n",
    "        self.cycles = config['cycles'] # number of cycles per episode\n",
    "\n",
    "        # Actor Critic Parameters\n",
    "        learning_rate = config.get('learning_rate', 0.001)  # Default to 0.001 if not provided\n",
    "        if not 0 < learning_rate <= 0.01:\n",
    "            print(\"Warning: The learning_rate is outside the range (0, 0.01]. Setting to default of 0.001.\")\n",
    "            learning_rate = 0.001\n",
    "        self.learning_rate = learning_rate        \n",
    "        self.layer_size = config['layer_size'] #size of hidden layers in both actor/critic\n",
    "\n",
    "        # Initialize the breeding simulator ; only takes care of actions related to generating cross + scoring individuals\n",
    "        self.breeding_simulator = config.get('simulator') or Simulator(genetic_map=self.genetic_map, h2=config['heritability'])\n",
    "        self.breeding_simulator.load_population('mypop.npy')\n",
    "        self.initial_scores = self.breeding_simulator.GEBV(self.initial_population)\n",
    "\n",
    "        # Initialize the Actor and Critic models\n",
    "        self.actor = create_actor(n=self.population_size, m=self.marker_count, d=2, layer_size=self.layer_size, learning_rate=self.learning_rate)\n",
    "        self.critic = create_critic(n=self.population_size, m=self.marker_count, d=2, layer_size=self.layer_size, learning_rate=self.learning_rate)\n",
    "\n",
    "        # Init the starting population\n",
    "        self.current_population = self.initial_population\n",
    "        self.current_scores = self.initial_scores\n",
    "\n",
    "    def reset_environment(self):\n",
    "        self.current_population = self.initial_population\n",
    "        self.current_scores = self.initial_scores\n",
    "\n",
    "    def use_actor(self):\n",
    "        \"\"\"\n",
    "            given population,scores returns selected_parents\n",
    "        \"\"\"\n",
    "        # Assuming `self.current_population` is already a NumPy array with shape (n, m, d)\n",
    "        # and `self.current_scores` is a pandas DataFrame or Series with shape (n,).\n",
    "        \n",
    "        # Convert the current population to a tensor and add a batch dimension\n",
    "        format_pop = np.expand_dims(self.current_population, axis=0)\n",
    "        format_pop = tf.convert_to_tensor(format_pop, dtype=tf.float32)\n",
    "        \n",
    "        # Convert the current scores to a tensor and add a batch dimension\n",
    "        format_scores = np.expand_dims(self.current_scores.values, axis=0)\n",
    "        format_scores = tf.convert_to_tensor(format_scores, dtype=tf.float32)\n",
    "        \n",
    "        # Predict the policy using the actor model\n",
    "        # Predict the policy using the actor model\n",
    "        policy_logits = self.actor([format_pop, format_scores])\n",
    "        policy_probs = tf.nn.softmax(policy_logits, axis=-1)\n",
    "        \n",
    "        # Sample actions based on the policy probabilities\n",
    "        # This is a placeholder; you'll need to implement the actual sampling based on your use case\n",
    "        parent_index = select_parents(policy_logits)[1].numpy()[0]\n",
    "        selected_parents = self.current_population[parent_index]\n",
    "        \n",
    "        return policy_logits, policy_probs, selected_parents, parent_index\n",
    "    \n",
    "    def use_critic(self):\n",
    "        # Format the inputs as required by the critic model\n",
    "        format_pop = np.expand_dims(self.current_population, axis=0)\n",
    "        format_pop = tf.convert_to_tensor(format_pop, dtype=tf.float32)\n",
    "        \n",
    "        format_scores = np.expand_dims(self.current_scores, axis=0)\n",
    "        format_scores = tf.convert_to_tensor(format_scores, dtype=tf.float32)\n",
    "        \n",
    "        # Obtain the policy using the actor model\n",
    "        policy_logits, policy_probs, sampled_actions, parent_index = self.use_actor()\n",
    "        \n",
    "        # Predict the value using the critic model\n",
    "        value = self.critic([format_pop, format_scores, policy_logits])\n",
    "        return policy_logits, policy_probs, sampled_actions, parent_index,  value\n",
    "    \n",
    "\n",
    "\n",
    "    def train_ppo(self):\n",
    "        \"\"\" PPO ALGORITHM TRAINING LOOP \"\"\"\n",
    "        #Collect Data with the Current Policy (Old Policy):\n",
    "\n",
    "        #Compute the loss\n",
    "\n",
    "        #Update the Actor Model Weights:\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    def step_mass_selection(self):\n",
    "        \"\"\"\n",
    "        Performs one step/cycle of mass selection\n",
    "\n",
    "        Returns : \n",
    "        policy_logits\n",
    "        policy_probs\n",
    "        mass_pop\n",
    "        new_pop\n",
    "        new_scores\n",
    "        \"\"\"\n",
    "        # use actor\n",
    "        # return next_population, next_scores\n",
    "        policy_logits, policy_probs, selected_parents, parent_index, value = self.use_critic()\n",
    "\n",
    "        mass_pop = self.breeding_simulator.random_crosses(selected_parents, n_crosses = selected_parents.shape[0], n_offspring=50)\n",
    "        mass_score = self.breeding_simulator.GEBV(collapse_first_two_axes(mass_pop))\n",
    "\n",
    "\n",
    "        new_pop = sample_two_offspring_per_cross(mass_pop)\n",
    "        new_scores = self.breeding_simulator.GEBV(new_pop)\n",
    "\n",
    "\n",
    "        return policy_logits, policy_probs, new_pop, new_scores, parent_index, value\n",
    "    def is_episode_done(self):\n",
    "        # Check if any condition for the end of an episode is met\n",
    "        if self.current_cycle >= self.cycles:\n",
    "            return True\n",
    "        # Add other conditions as necessary\n",
    "        return False\n",
    "\n",
    "    def collect_data(self, collection_episodes):\n",
    "        \"\"\"\n",
    "        Collects data from the environment by running the agent for a specified number of episodes.\n",
    "\n",
    "        This function runs the agent through the breeding simulation environment, collecting states,\n",
    "        actions, rewards, policy probabilities, value estimates, and done signals for each step in\n",
    "        each episode. The data collected is used for training the agent using the PPO algorithm.\n",
    "\n",
    "        Parameters:\n",
    "            collection_episodes (int): The number of episodes to run the agent for data collection.\n",
    "\n",
    "        Returns:\n",
    "            states (list of tuples): The states encountered by the agent. Each state is a tuple containing\n",
    "                                    a representation of the population (np.array) and the corresponding\n",
    "                                    scores (np.array) at a given cycle.\n",
    "            actions (np.ndarray): The actions taken by the agent. Actions are represented by the indices\n",
    "                                of the selected parents in the population.\n",
    "            rewards (np.ndarray): The rewards received by the agent for taking actions. Rewards are\n",
    "                                calculated as the difference in scores between the new population and\n",
    "                                the current population.\n",
    "            old_policy_probs (np.ndarray): The probabilities of the actions taken, according to the\n",
    "                                        policy before the update. These are used to calculate the\n",
    "                                        importance sampling weights for the PPO algorithm.\n",
    "            values (np.ndarray): The value estimates given by the critic for the states encountered.\n",
    "                                These are used to calculate the advantage estimates.\n",
    "            dones (np.ndarray): Signals indicating whether an episode has ended. A value of True\n",
    "                                indicates the end of an episode, while False indicates the episode\n",
    "                                is still ongoing.\n",
    "        \"\"\"\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        old_policy_probs = []\n",
    "        values = []  # To store the value estimates\n",
    "        dones = []   # To store the done signals\n",
    "\n",
    "        for episode in range(collection_episodes):\n",
    "            self.reset_environment()  # Make sure this resets self.current_population and self.current_scores\n",
    "            done = False\n",
    "            for cycle in range(self.cycles):\n",
    "                self.current_cycle = cycle\n",
    "                # Collect data for the current cycle\n",
    "                policy_logits, policy_probs, new_pop, new_scores, parent_index, value = self.step_mass_selection()\n",
    "                reward = new_scores - self.current_scores\n",
    "                action_prob = policy_probs.numpy()[0, parent_index]  # Assuming single batch\n",
    "\n",
    "                # Check if the episode is done (you'll need to define the condition)\n",
    "                done = self.is_episode_done()\n",
    "\n",
    "                # Append data to lists\n",
    "                states.append((self.current_population.copy(), self.current_scores.copy()))\n",
    "                actions.append(parent_index)\n",
    "                rewards.append(reward.values.ravel())\n",
    "                old_policy_probs.append(action_prob)\n",
    "                values.append(value.numpy().ravel())  # Ensure value is a 1D array\n",
    "                dones.append(done)\n",
    "\n",
    "                # Update the current population and scores for the next cycle\n",
    "                self.current_population = new_pop\n",
    "                self.current_scores = new_scores  # Ensure this is the correct way to update the current scores\n",
    "\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "        # Convert lists to numpy arrays for actions, rewards, old_policy_probs, values, and dones\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        old_policy_probs = np.array(old_policy_probs)\n",
    "        values = np.array(values)\n",
    "        dones = np.array(dones)\n",
    "\n",
    "        return states, actions, rewards, old_policy_probs, values, dones\n",
    "\n",
    "\n",
    "\n",
    "    def compute_advantages(self, rewards, values_from_critic, dones, gamma=0.99, lam=0.95):\n",
    "        \"\"\"\n",
    "        Computes the advantage estimates using the collected rewards and value estimates from the critic.\n",
    "\n",
    "        Parameters:\n",
    "            rewards (np.ndarray): Observed rewards for each action taken, with shape (num_steps,).\n",
    "            values_from_critic (np.ndarray): Value estimates from the critic for each state, with shape (num_steps,).\n",
    "            dones (np.ndarray): Boolean array indicating if the episode has ended, with shape (num_steps,).\n",
    "            gamma (float): Discount factor for future rewards.\n",
    "            lam (float): Lambda parameter for GAE.\n",
    "\n",
    "        Returns:\n",
    "            advantages (np.ndarray): The calculated advantage estimates for each action, with shape (num_steps,).\n",
    "        \"\"\"\n",
    "        num_steps = len(rewards)\n",
    "        advantages = np.zeros(num_steps)\n",
    "        last_advantage = 0\n",
    "\n",
    "        # Ensure inputs are flattened\n",
    "        rewards = np.ravel(rewards)\n",
    "        values_from_critic = np.ravel(values_from_critic)\n",
    "        dones = np.ravel(dones)\n",
    "\n",
    "        # Append an extra zero for the value of the next state after the last timestep\n",
    "        values_from_critic = np.append(values_from_critic, 0)\n",
    "\n",
    "        # We loop from the last step to the first to calculate advantages\n",
    "        for t in reversed(range(num_steps)):\n",
    "            # If the episode is done, the next value is 0, otherwise it's the value of the next state\n",
    "            next_value = 0 if dones[t] else values_from_critic[t + 1]\n",
    "            # Calculate the TD residual\n",
    "            delta = rewards[t] + gamma * next_value - values_from_critic[t]\n",
    "            # Calculate the advantage using the TD residual and decay factor lambda\n",
    "            advantages[t] = last_advantage = delta + gamma * lam * last_advantage * (1 - dones[t])\n",
    "\n",
    "        return advantages\n",
    "\n",
    "    def recalculate_action_probabilities(self, states, actions):\n",
    "        \"\"\"\n",
    "        Recalculate the probabilities of the actions taken during the data collection phase\n",
    "        using the current policy.\n",
    "\n",
    "        Parameters:\n",
    "        states (list of tuples): The states encountered during data collection. Each state is a tuple\n",
    "                                containing a representation of the population (np.array) and the\n",
    "                                corresponding scores (np.array).\n",
    "        actions (np.ndarray): The actions taken during data collection.\n",
    "\n",
    "        Returns:\n",
    "        np.ndarray: The recalculated probabilities of the actions.\n",
    "        \"\"\"\n",
    "        # Unpack the states into separate lists for populations and scores\n",
    "        populations, scores = zip(*states)  # Unzip the list of tuples\n",
    "        populations_tensor = tf.convert_to_tensor(np.array(populations), dtype=tf.float32)\n",
    "        scores_tensor = tf.convert_to_tensor(np.array(scores), dtype=tf.float32)\n",
    "        \n",
    "        # Convert the lists of populations and scores into tensors\n",
    "        # populations_tensor = tf.convert_to_tensor(np.array(populations), dtype=tf.float32)\n",
    "        # scores_tensor = tf.convert_to_tensor(np.array(scores), dtype=tf.float32)\n",
    "        \n",
    "        # Get the policy logits for the collected states using the current actor model\n",
    "        policy_logits = self.actor([populations_tensor, scores_tensor])\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        policy_probs = tf.nn.softmax(policy_logits, axis=-1)\n",
    "        \n",
    "        # Create a tensor of action indices for gathering the probabilities\n",
    "        action_indices = tf.expand_dims(actions, axis=-1)\n",
    "        \n",
    "        # Gather the probabilities of the actions that were actually taken\n",
    "        action_probs = tf.gather_nd(policy_probs, action_indices, batch_dims=1)\n",
    "        \n",
    "        return action_probs.numpy()\n",
    "    \n",
    "\n",
    "    def train_ppo(self, epochs, batch_size, gamma=0.99, lam=0.95, clip_ratio=0.2):\n",
    "        # Collect data with the old policy\n",
    "        states, actions, rewards, old_policy_probs, values, dones = self.collect_data(self.episodes)\n",
    "        \n",
    "        # Calculate advantages\n",
    "        advantages = self.compute_advantages(rewards, values, dones, gamma=gamma, lam=lam)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)\n",
    "        \n",
    "        # Unpack the states into separate lists for populations and scores\n",
    "        populations, scores = zip(*states)\n",
    "        \n",
    "        # Convert the lists of populations and scores into tensors\n",
    "        populations_tensor = tf.convert_to_tensor(np.array(populations), dtype=tf.float32)\n",
    "        scores_tensor = tf.convert_to_tensor(np.array(scores), dtype=tf.float32)\n",
    "        \n",
    "        # Convert other lists to tensors\n",
    "        actions_tensor = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "        old_policy_probs_tensor = tf.convert_to_tensor(old_policy_probs, dtype=tf.float32)\n",
    "        advantages_tensor = tf.convert_to_tensor(advantages, dtype=tf.float32)\n",
    "        \n",
    "        # Prepare dataset for training\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((populations_tensor, scores_tensor, actions_tensor, old_policy_probs_tensor, advantages_tensor))\n",
    "        dataset = dataset.shuffle(buffer_size=len(states)).batch(batch_size)\n",
    "        \n",
    "        # Training loop\n",
    "        for _ in range(epochs):\n",
    "            for population_batch, score_batch, action_batch, old_prob_batch, advantage_batch in dataset:\n",
    "                with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:\n",
    "                    # Get new policy probabilities and values for each batch\n",
    "                    policy_logits = self.actor([population_batch, score_batch])\n",
    "                    new_policy_probs = tf.nn.softmax(policy_logits, axis=-1)\n",
    "                    values = self.critic([population_batch, score_batch, policy_logits])\n",
    "\n",
    "                    # Calculate the ratio of new to old probabilities\n",
    "                    action_indices = tf.expand_dims(action_batch, axis=-1)\n",
    "                    new_probs = tf.gather_nd(new_policy_probs, action_indices, batch_dims=1)\n",
    "                    ratios = new_probs / old_prob_batch\n",
    "                    \n",
    "                    # Calculate surrogate losses\n",
    "                    surr1 = ratios * advantage_batch\n",
    "                    surr2 = tf.clip_by_value(ratios, 1 - clip_ratio, 1 + clip_ratio) * advantage_batch\n",
    "                    \n",
    "                    # Actor loss\n",
    "                    actor_loss = -tf.reduce_mean(tf.minimum(surr1, surr2))\n",
    "                    \n",
    "                    # Critic loss\n",
    "                    values = self.critic([population_batch, score_batch, policy_logits])\n",
    "                    critic_loss = tf.reduce_mean(tf.square(values - tf.expand_dims(rewards, axis=-1)))\n",
    "                \n",
    "                # Compute gradients and update actor and critic\n",
    "                actor_grads = actor_tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "                critic_grads = critic_tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "                \n",
    "                actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "                critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Preprocessing Data for Breeding Simulation\n",
    "initial_population =  reshape_pop(maizeHaplo) \n",
    "genetic_map = return_genetic_map_df(markerEffects, nChr, geneticMap)\n",
    "genetic_map['Yield'] = simplify_geneticmap(list(genetic_map['Yield']),4)\n",
    "reshapeHaplo = reshape_pop(maizeHaplo)\n",
    "np.save('mypop', reshapeHaplo)\n",
    "print(f'reshape haplo {reshapeHaplo.shape}')\n",
    "population_size = int(nInd)\n",
    "marker_count = int((segSites * nChr))\n",
    "chromosome_number = int(nChr)\n",
    "\n",
    "config = {\n",
    "    'compute_baselines': False,\n",
    "    'initial_population': initial_population,\n",
    "    'genetic_map': genetic_map,\n",
    "    'population_size': population_size,\n",
    "    'marker_count': marker_count,\n",
    "    'heritability': .99,\n",
    "    'episodes': 10,\n",
    "    'cycles': 2,\n",
    "    'learning_rate': .001,\n",
    "    'replicates': 1,\n",
    "    'layer_size': 64,\n",
    "    'ploidy':2,\n",
    "}\n",
    "\n",
    "agent = BreederAgent(config)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=.001)\n",
    "critic_optimizer = tf.keras.optimizers.Adam(learning_rate=.001)\n",
    "# advantages = agent.compute_advantages(rewards, values, dones)\n",
    "# agent.compute_advantages(rewards, values, dones )\n",
    "#def compute_advantages(self, rewards, values_from_critic, dones, gamma=0.99, lam=0.95):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train_ppo(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train_ppo(6,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [2,5] vs. [2] [Op:Mul] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[50], line 440\u001b[0m, in \u001b[0;36mBreederAgent.train_ppo\u001b[0;34m(self, epochs, batch_size, gamma, lam, clip_ratio)\u001b[0m\n\u001b[1;32m    437\u001b[0m ratios \u001b[38;5;241m=\u001b[39m new_probs \u001b[38;5;241m/\u001b[39m old_prob_batch\n\u001b[1;32m    439\u001b[0m \u001b[38;5;66;03m# Calculate surrogate losses\u001b[39;00m\n\u001b[0;32m--> 440\u001b[0m surr1 \u001b[38;5;241m=\u001b[39m \u001b[43mratios\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43madvantage_batch\u001b[49m\n\u001b[1;32m    441\u001b[0m surr2 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mclip_by_value(ratios, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m clip_ratio, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m clip_ratio) \u001b[38;5;241m*\u001b[39m advantage_batch\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Actor loss\u001b[39;00m\n",
      "File \u001b[0;32m~/breeder_agent/agent_env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/breeder_agent/agent_env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:6656\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   6655\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 6656\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [2,5] vs. [2] [Op:Mul] name: "
     ]
    }
   ],
   "source": [
    "agent.train_ppo(5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
