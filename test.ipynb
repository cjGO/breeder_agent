{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython\n",
    "\n",
    "from rpy2.robjects.packages import importr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from chromax import Simulator\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_means_and_variances(dataframes):\n",
    "    # Calculate the mean and variance for each dataframe\n",
    "    mean_values = [df.mean() for df in dataframes]\n",
    "    var_values = [df.var() for df in dataframes]\n",
    "    var_values = np.array(var_values).flatten()\n",
    "    mean_values = np.array(mean_values).flatten()\n",
    "\n",
    "    # Create an array for the x-values\n",
    "    x_values = range(len(dataframes))\n",
    "\n",
    "    # Create the scatter plot with error bars\n",
    "    plt.errorbar(x_values, mean_values, yerr=var_values, fmt='o')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def generate_genetic_map(n_markers, n_chromosomes):\n",
    "  df = pd.DataFrame(generate_marker_effects(n_markers=n_markers), columns=['Yield'])\n",
    "  df['cM'] = np.random.uniform(0, 200, len(df))\n",
    "  df['CHR.PHYS'] = '1A'\n",
    "  df = df.sort_values(by='cM')\n",
    "  df = df[['CHR.PHYS', 'cM', 'Yield']]\n",
    "  # save df as csv under filename\n",
    "  return df\n",
    "\n",
    "def generate_population(n_pop=100, n_markers=500):\n",
    "    \"\"\"\n",
    "    Generate a numpy array of randoms of length 500 with randomized 0, 1, or 2 at each position.\n",
    "    It will generate 100 individuals based on n_pop.\n",
    "\n",
    "    Returns: numpy array of size (n_pop, n_markers)\n",
    "    \"\"\"\n",
    "    shape=(n_pop, n_markers, 2)\n",
    "    # Define the elements to choose from and their associated probabilities\n",
    "    elements = [0, 1, 2]\n",
    "    probabilities = [1/3, 1/3, 1/3]  # equal probabilities for 0, 1, and 2\n",
    "\n",
    "    # Generate the population\n",
    "    population = np.random.choice(elements, size=(n_pop, n_markers), p=probabilities)\n",
    "\n",
    "    return np.random.choice([True, False], size=shape)\n",
    "\n",
    "\n",
    "def generate_marker_effects(n_markers=500, mu=0, sigma=0.1):\n",
    "    \"\"\"\n",
    "    Generate a numpy array of marker effects with a normal distribution.\n",
    "\n",
    "    Parameters:\n",
    "    n_markers (int): Number of markers.\n",
    "    mu (float): Mean of the distribution.\n",
    "    sigma (float): Standard deviation of the distribution.\n",
    "\n",
    "    Returns:\n",
    "    numpy array of marker effects\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the marker effects\n",
    "    marker_effects = np.random.normal(mu, sigma, n_markers)\n",
    "\n",
    "    return marker_effects\n",
    "\n",
    "\n",
    "def select_random_individuals(arr, num_individuals):\n",
    "    # Get the shape of the array\n",
    "    shape = arr.shape\n",
    "\n",
    "    # Generate random indices along the first axis\n",
    "    idx = np.random.choice(shape[0], size=num_individuals)\n",
    "\n",
    "    # Select the random individuals\n",
    "\n",
    "    return random_individuals\n",
    "\n",
    "def select_mixed(population, random_split=.99):\n",
    "  n_pop = population.shape[0]\n",
    "\n",
    "  n_random = int(n_pop * random_split)\n",
    "  n_select = int(n_pop * (1-random_split))\n",
    "\n",
    "  random_parents = select_random_individuals(Farm.current_population, n_random)\n",
    "  selected_parents = Farm.Simulator.select(Farm.current_population, k = n_select)\n",
    "  combined_arr = np.concatenate((random_parents, selected_parents), axis=0)\n",
    "  return combined_arr\n",
    "\n",
    "def plot_replicate_means_and_variances(replicate_data, start_index=None, end_index=None):\n",
    "    # Create a new figure and axes\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # If start_index or end_index is not provided, set them to default values\n",
    "    if start_index is None:\n",
    "        start_index = 0\n",
    "    if end_index is None:\n",
    "        end_index = len(replicate_data[0])\n",
    "\n",
    "    # For each list of dataframes in replicate_data\n",
    "    for i, dataframes in enumerate(replicate_data):\n",
    "        # Select the dataframes in the specified range\n",
    "        dataframes = dataframes[start_index:end_index]\n",
    "\n",
    "        # Calculate the mean and variance for each dataframe\n",
    "        mean_values = [df.mean() for df in dataframes]\n",
    "        var_values = [df.var() for df in dataframes]\n",
    "\n",
    "        # Flatten the var_values and mean_values lists to 1D arrays\n",
    "        var_values = np.array(var_values).flatten()\n",
    "        mean_values = np.array(mean_values).flatten()\n",
    "\n",
    "        # Create an array for the x-values\n",
    "        x_values = range(len(dataframes))\n",
    "\n",
    "        # Plot the means with error bars for the variances\n",
    "        ax.errorbar(x_values, mean_values, yerr=var_values, fmt='o', label=f'Replicate {i+1}')\n",
    "\n",
    "    # Add a legend\n",
    "    ax.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_replicate_means(replicate_data):\n",
    "    # Create a new figure and axes\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # For each list of dataframes in replicate_data\n",
    "    for i, dataframes in enumerate(replicate_data):\n",
    "        # Calculate the mean for each dataframe\n",
    "        mean_values = [df.mean() for df in dataframes]\n",
    "        # Flatten the mean_values list to a 1D array\n",
    "        mean_values = np.array(mean_values).flatten()\n",
    "\n",
    "        # Create an array for the x-values\n",
    "        x_values = range(len(dataframes))\n",
    "\n",
    "        # Plot the means as a line plot\n",
    "        ax.plot(x_values, mean_values, label=f'Replicate {i+1}')\n",
    "\n",
    "    # Add a legend\n",
    "    ax.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "def parse_markerEffects(genetic_map, nChr):\n",
    "    # Get the length of the genetic map\n",
    "    length = len(genetic_map)\n",
    "\n",
    "    # Create a new array for storing the chromosome number for each marker\n",
    "    chr = [0] * length\n",
    "\n",
    "    # Calculate the number of markers per chromosome\n",
    "    markers_per_chr = length // nChr\n",
    "\n",
    "    # Iterate over the range of the genetic map length\n",
    "    for i in range(length):\n",
    "        # Calculate the chromosome number and store it in the chr array\n",
    "        chr[i] = i // markers_per_chr + 1\n",
    "\n",
    "    return chr\n",
    "\n",
    "def score_top(scores: pd.DataFrame, column: str, k: int):\n",
    "    # Sort the DataFrame from high to low\n",
    "    sorted_scores = scores.sort_values(by=column, ascending=False)\n",
    "    # Get the top K indexes\n",
    "    top_k_indexes = sorted_scores.head(k).index\n",
    "    return top_k_indexes\n",
    "\n",
    "\n",
    "def score_top_percentile(scores: pd.DataFrame, column: str, percentile_min: float, percentile_max: float, k: int):\n",
    "    # Ensure max percentile is greater than min percentile\n",
    "    assert percentile_max > percentile_min, \"Error: max percentile should be greater than min percentile\"\n",
    "    \n",
    "    # Calculate the percentiles\n",
    "    lower = scores[column].quantile(percentile_min)\n",
    "    upper = scores[column].quantile(percentile_max)\n",
    "    # Filter the DataFrame\n",
    "    filtered_scores = scores[(scores[column] >= lower) & (scores[column] <= upper)]\n",
    "    # Sample k random indexes\n",
    "    sampled_indexes = np.random.choice(filtered_scores.index, k, replace=True)\n",
    "\n",
    "    return sampled_indexes\n",
    "\n",
    "def reshape_pop(maizeHaplo):\n",
    "    reshapeHaplo = maizeHaplo.reshape(int((maizeHaplo.shape[0])/2),2,maizeHaplo.shape[1])\n",
    "    reshapeHaplo = reshapeHaplo.transpose((0,2,1))\n",
    "    return reshapeHaplo\n",
    "\n",
    "def return_genetic_map_df(markerEffects, nChr, geneticMap):\n",
    "    chr = parse_markerEffects(markerEffects, nChr)\n",
    "    chr = [int(x[0]) for x in chr]\n",
    "    trait = markerEffects\n",
    "    pos = geneticMap\n",
    "    # Assuming chr, trait, pos are your arrays\n",
    "    df = pd.DataFrame({'CHR.PHYS': chr, 'Yield': trait, 'cM': pos})\n",
    "    return df\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_histogram(ax, probabilities, label=None, color=None):\n",
    "    ax.hist(probabilities, bins='auto', density=True, alpha=0.5, label=label, color=color)\n",
    "    ax.set_title('Probability Distribution')\n",
    "    ax.set_xlabel('Probability')\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "def plot_bar(ax, probabilities, label=None, color=None):\n",
    "    indices = range(len(probabilities))\n",
    "    ax.bar(indices, probabilities, alpha=0.5, label=label, color=color)\n",
    "    ax.set_title('Probability per Index')\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Probability')\n",
    "\n",
    "def plot_probabilities(probabilities1, probabilities2):\n",
    "    # Create subplots: 1 row, 2 columns\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Call the plot functions for the first set of probabilities\n",
    "    plot_histogram(axs[0], probabilities1, label='Probabilities 1', color='blue')\n",
    "    plot_bar(axs[1], probabilities1, label='Probabilities 1', color='blue')\n",
    "\n",
    "    # Call the plot functions for the second set of probabilities\n",
    "    plot_histogram(axs[0], probabilities2, label='Probabilities 2', color='red')\n",
    "    plot_bar(axs[1], probabilities2, label='Probabilities 2', color='red')\n",
    "\n",
    "    # Add legends\n",
    "    axs[0].legend(loc='upper right')\n",
    "    axs[1].legend(loc='upper right')\n",
    "\n",
    "    # Display the plots\n",
    "    plt.tight_layout()  # Adjusts subplot params so that subplots fit in the figure area\n",
    "    plt.show()\n",
    "\n",
    "def select_parent(probabilities):\n",
    "    try:\n",
    "        # Ensure probabilities sum up to 1\n",
    "        probabilities = np.array(probabilities, dtype=np.float64)\n",
    "        probabilities[-1] = max(0, 1 - np.sum(probabilities[:-1]))\n",
    "        indices = np.arange(len(probabilities))  # Create an array of indices\n",
    "        sampled_index = np.random.choice(indices, p=probabilities)  # Sample an index based on the probabilities\n",
    "    except ValueError:\n",
    "        print(\"Error: probabilities do not sum up to 1\")\n",
    "        print(\"Probabilities:\", probabilities)\n",
    "        raise\n",
    "    return sampled_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "x <- seq(0, 2*pi, length.out=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "%R -o x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.12822827, 0.25645654, 0.38468481, 0.51291309,\n",
       "       0.64114136, 0.76936963, 0.8975979 , 1.02582617, 1.15405444,\n",
       "       1.28228272, 1.41051099, 1.53873926, 1.66696753, 1.7951958 ,\n",
       "       1.92342407, 2.05165235, 2.17988062, 2.30810889, 2.43633716,\n",
       "       2.56456543, 2.6927937 , 2.82102197, 2.94925025, 3.07747852,\n",
       "       3.20570679, 3.33393506, 3.46216333, 3.5903916 , 3.71861988,\n",
       "       3.84684815, 3.97507642, 4.10330469, 4.23153296, 4.35976123,\n",
       "       4.48798951, 4.61621778, 4.74444605, 4.87267432, 5.00090259,\n",
       "       5.12913086, 5.25735913, 5.38558741, 5.51381568, 5.64204395,\n",
       "       5.77027222, 5.89850049, 6.02672876, 6.15495704, 6.28318531])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %R install.packages(\"AlphaSimR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(\"AlphaSimR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "nInd = 50\n",
    "nChr = 3\n",
    "segSites = 10\n",
    "\n",
    "founderGenomes = runMacs(nInd = nInd,\n",
    "                         nChr = nChr,\n",
    "                         segSites = segSites,\n",
    "                         species = \"MAIZE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "SP = SimParam$new(founderGenomes)\n",
    "SP$addTraitA(segSites)\n",
    "# SP$setVarE(h2=.02)\n",
    "pop = newPop(founderGenomes, simParam=SP)\n",
    "ans = fastRRBLUP(pop, simParam=SP, useQtl=TRUE, use='gv')\n",
    "ans@gv[[1]]@addEff\n",
    "markerEffects = slot(slot(ans, \"gv\")[[1]], \"addEff\")\n",
    "maizeHaplo = pullSegSiteHaplo(pop)\n",
    "maizeGeno = pullSegSiteGeno(pop)\n",
    "#cm positions of each marker\n",
    "genMap = SP$genMap\n",
    "geneticMap = unlist(genMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "%R -o maizeHaplo\n",
    "%R -o maizeGeno\n",
    "%R -o markerEffects\n",
    "%R -o geneticMap\n",
    "%R -o nInd\n",
    "%R -o nChr\n",
    "%R -o segSites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Reshape\n",
    "\n",
    "#create init population + genetic map\n",
    "n = int(nInd)\n",
    "m = int((segSites * nChr))\n",
    "d = 2\n",
    "total_parents = 100 # pop size per cycle\n",
    "\n",
    "def calculate_actor_loss(actor_output, selected_array):\n",
    "    #calculate actor loss\n",
    "    actor_log_probs = tf.math.log(actor_output)  #log probabilities of the actions given by the actor\n",
    "    selected_log_probs = tf.gather(actor_log_probs, selected_array, axis=1)  #log probabilities of the selected actions\n",
    "    loss = -tf.reduce_sum(selected_log_probs)  #negative sum of the log probabilities of the selected actions\n",
    "    return loss\n",
    "\n",
    "def calculate_true_reward(simulator, current_population, new_population):\n",
    "    true_reward = simulator.GEBV(new_population).mean() - simulator.GEBV(current_population).mean()\n",
    "    return true_reward\n",
    "\n",
    "\n",
    "def create_actor():\n",
    "    # Define the actor model\n",
    "    actor_input = keras.layers.Input(shape=(n, m, d))\n",
    "    x = Flatten()(actor_input)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(n * total_parents, activation='linear')(x)\n",
    "    x = Reshape((total_parents, n))(x)\n",
    "    actor_output = keras.layers.Softmax(axis=-1)(x)\n",
    "    actor_model = keras.models.Model(actor_input, actor_output)\n",
    "    actor_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    return actor_model\n",
    "\n",
    "def create_critic():\n",
    "    # Define the critic model\n",
    "    critic_input1 = keras.layers.Input(shape=(n, m, d))\n",
    "    critic_input2 = keras.layers.Input(shape=(total_parents, n))\n",
    "\n",
    "    x1 = Flatten()(critic_input1)\n",
    "    x1 = Dense(256, activation='relu')(x1)\n",
    "\n",
    "    x2 = Flatten()(critic_input2)\n",
    "    x2 = Dense(256, activation='relu')(x2)\n",
    "\n",
    "    combined = keras.layers.concatenate([x1, x2])\n",
    "\n",
    "    x3 = Dense(256, activation='relu')(combined)\n",
    "    critic_output = Dense(1, activation='linear')(x3)\n",
    "\n",
    "    critic_model = keras.models.Model([critic_input1, critic_input2], critic_output)\n",
    "    critic_model.compile(optimizer='adam', loss='mse')\n",
    "    return critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n",
      "actor model output : (1, 100, 50)\n",
      "example population shape, single sample : (1, 50, 30, 2)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "critic model output: (1, 1)\n",
      "[0.5095636]\n",
      "\n",
      "      population size  : [50.]\n",
      "      number of markers  : 30\n",
      "      diploidy : 2\n",
      "      \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute '_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_283850/1251965376.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'actor model output : {actor_output.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'example population shape, single sample : {example_population.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mcritic_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mexample_population\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Print the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'critic model output: {critic_output.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/breeder_agent/agent_env/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1059\u001b[0m               output_gradients))\n\u001b[1;32m   1060\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[1;32m   1061\u001b[0m                           for x in output_gradients]\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1064\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/breeder_agent/agent_env/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     raise ValueError(\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/breeder_agent/agent_env/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute '_id'"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Reshape\n",
    "\n",
    "#create init population + genetic map\n",
    "n = int(nInd)\n",
    "m = int((segSites * nChr))\n",
    "d = 2\n",
    "total_parents = 100 # pop size per cycle\n",
    "\n",
    "def calculate_actor_loss(actor_output, selected_array):\n",
    "    #calculate actor loss\n",
    "    actor_log_probs = tf.math.log(actor_output)  #log probabilities of the actions given by the actor\n",
    "    selected_log_probs = tf.gather(actor_log_probs, selected_array, axis=1)  #log probabilities of the selected actions\n",
    "    loss = -tf.reduce_sum(selected_log_probs)  #negative sum of the log probabilities of the selected actions\n",
    "    return loss\n",
    "\n",
    "def calculate_true_reward(simulator, current_population, new_population):\n",
    "    true_reward = simulator.GEBV(new_population).mean() - simulator.GEBV(current_population).mean()\n",
    "    return true_reward\n",
    "\n",
    "\n",
    "def create_actor():\n",
    "    # Define the actor model\n",
    "    actor_input = keras.layers.Input(shape=(n, m, d))\n",
    "    x = Flatten()(actor_input)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(n * total_parents, activation='linear')(x)\n",
    "    x = Reshape((total_parents, n))(x)\n",
    "    actor_output = keras.layers.Softmax(axis=-1)(x)\n",
    "    actor_model = keras.models.Model(actor_input, actor_output)\n",
    "    actor_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    return actor_model\n",
    "\n",
    "def create_critic():\n",
    "    # Define the critic model\n",
    "    critic_input1 = keras.layers.Input(shape=(n, m, d))\n",
    "    critic_input2 = keras.layers.Input(shape=(total_parents, n))\n",
    "\n",
    "    x1 = Flatten()(critic_input1)\n",
    "    x1 = Dense(256, activation='relu')(x1)\n",
    "\n",
    "    x2 = Flatten()(critic_input2)\n",
    "    x2 = Dense(256, activation='relu')(x2)\n",
    "\n",
    "    combined = keras.layers.concatenate([x1, x2])\n",
    "\n",
    "    x3 = Dense(256, activation='relu')(combined)\n",
    "    critic_output = Dense(1, activation='linear')(x3)\n",
    "\n",
    "    critic_model = keras.models.Model([critic_input1, critic_input2], critic_output)\n",
    "    critic_model.compile(optimizer='adam', loss='mse')\n",
    "    return critic_model\n",
    "\n",
    "actor_model = create_actor()\n",
    "critic_model = create_critic()\n",
    "# Create a dummy population\n",
    "example_population = np.random.randint(0, 2, (n, m, d))\n",
    "example_population = np.expand_dims(example_population, axis=0)\n",
    "\n",
    "# Send the example through the actor network\n",
    "actor_output = actor_model.predict(example_population)\n",
    "print(f'actor model output : {actor_output.shape}')\n",
    "print(f'example population shape, single sample : {example_population.shape}')\n",
    "\n",
    "# Predict\n",
    "critic_output = critic_model.predict([ example_population, actor_output])\n",
    "\n",
    "# Print the output\n",
    "print(f'critic model output: {critic_output.shape}')\n",
    "print(critic_output[0][0:5])\n",
    "\n",
    "print(f\"\"\"\n",
    "      population size  : {nInd}\n",
    "      number of markers  : {m }\n",
    "      diploidy : {d}\n",
    "      \"\"\")\n",
    "\n",
    "action_selections = [select_parent(x) for x in actor_output[0,:,:]]\n",
    "\n",
    "# Reshape action selections into pairs of parents\n",
    "# This prepares the selected parents for the breeding simulation\n",
    "selected_array = np.array([action_selections[i:i+2] for i in range(0, len(action_selections), 2)])\n",
    "#cross\n",
    "new_population = farm.simulator.cross(example_population[0][selected_array])\n",
    "#calculate actor loss\n",
    "actor_loss = calculate_actor_loss(actor_output[0], selected_array)\n",
    "true_reward = calculate_critic_loss(farm.simulator,farm.current_population, new_population)[0]\n",
    "critic_loss = (critic_output[0][0] - true_reward)**2\n",
    "\n",
    "\n",
    "# First, create optimizers for your actor and critic models\n",
    "actor_optimizer = tf.keras.optimizers.Adam()\n",
    "critic_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Then, use GradientTape to calculate and apply gradients\n",
    "with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:\n",
    "    # Assume you've calculated actor_loss and critic_loss here\n",
    "    actor_loss = calculate_actor_loss(actor_output[0], selected_array)\n",
    "    true_reward = calculate_true_reward(farm.simulator, farm.current_population, new_population)\n",
    "    critic_loss = (critic_output[0][0] - true_reward)**2\n",
    "\n",
    "    # Get the gradients\n",
    "    actor_grads = actor_tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "    critic_grads = critic_tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "\n",
    "    # Apply the gradients\n",
    "    actor_optimizer.apply_gradients(zip(actor_grads, actor_model.trainable_variables))\n",
    "    critic_optimizer.apply_gradients(zip(critic_grads, critic_model.trainable_variables))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2455176788859923"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.48680174]], dtype=float32)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yield    0.000661\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming critic_output is the output of your critic network\n",
    "# and returns is the actual return calculated from your simulation\n",
    "critic_loss = tf.reduce_mean(tf.square(returns - critic_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actos loss 39179.2890625\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 50, 2), dtype=float32, numpy=\n",
       "array([[[0.01960192, 0.01945123],\n",
       "        [0.02205056, 0.01948449],\n",
       "        [0.01983046, 0.02205056],\n",
       "        ...,\n",
       "        [0.02026705, 0.01948449],\n",
       "        [0.02225737, 0.0252395 ],\n",
       "        [0.02008723, 0.02118193]],\n",
       "\n",
       "       [[0.02167892, 0.02543873],\n",
       "        [0.01709059, 0.01955276],\n",
       "        [0.01811855, 0.01709059],\n",
       "        ...,\n",
       "        [0.02184433, 0.01955276],\n",
       "        [0.01970268, 0.02298442],\n",
       "        [0.01886165, 0.01799012]],\n",
       "\n",
       "       [[0.01681823, 0.02235353],\n",
       "        [0.02044977, 0.01827162],\n",
       "        [0.02158024, 0.02044977],\n",
       "        ...,\n",
       "        [0.01968785, 0.01827162],\n",
       "        [0.02062302, 0.01720525],\n",
       "        [0.01634029, 0.0247447 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.01993493, 0.01933136],\n",
       "        [0.02231281, 0.0240648 ],\n",
       "        [0.0207593 , 0.02231281],\n",
       "        ...,\n",
       "        [0.01774864, 0.0240648 ],\n",
       "        [0.01960059, 0.02296863],\n",
       "        [0.0201275 , 0.02434123]],\n",
       "\n",
       "       [[0.0195491 , 0.02205527],\n",
       "        [0.01910596, 0.01902192],\n",
       "        [0.0219505 , 0.01910596],\n",
       "        ...,\n",
       "        [0.02024639, 0.01902192],\n",
       "        [0.02539648, 0.02029426],\n",
       "        [0.02294547, 0.02079659]],\n",
       "\n",
       "       [[0.02092774, 0.024158  ],\n",
       "        [0.02091785, 0.01689154],\n",
       "        [0.02117653, 0.02091785],\n",
       "        ...,\n",
       "        [0.02118799, 0.01689154],\n",
       "        [0.01861316, 0.01647804],\n",
       "        [0.02377227, 0.01957757]]], dtype=float32)>"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape haplo (50, 30, 2)\n"
     ]
    }
   ],
   "source": [
    "class BreedingProgram:\n",
    "    \"\"\"\n",
    "    Represents a breeding program with a PPO agent.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, initial_population, genetic_map, population_size, marker_count, chromosome_number, max_generation, heritability):\n",
    "        \"\"\"\n",
    "        Initializes the breeding program.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the basic attributes\n",
    "        self.population_size = population_size\n",
    "        self.marker_count = marker_count\n",
    "        self.initial_population = initial_population\n",
    "        self.genetic_map = genetic_map\n",
    "        self.max_generation = max_generation\n",
    "\n",
    "        # Initialize the simulator\n",
    "        self.simulator = Simulator(genetic_map=self.genetic_map, h2=heritability)\n",
    "        self.simulator.load_population('mypop.npy')\n",
    "\n",
    "        # Initialize the current generation and history\n",
    "        self.current_generation = 0\n",
    "        self.history = []\n",
    "\n",
    "        # Initialize the Actor and Critic models\n",
    "        self.actor = create_actor()\n",
    "        self.critic = create_critic()\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "                # Start the breeding program\n",
    "        self._start_breeding_program()\n",
    "        \n",
    "    def _start_breeding_program(self):\n",
    "        \"\"\"\n",
    "        Starts the breeding program.\n",
    "        \"\"\"\n",
    "        self.current_population = self.initial_population\n",
    "        self.current_scores = self.simulator.GEBV(self.initial_population)\n",
    "        self.history.append(self.current_scores)\n",
    "\n",
    "\n",
    "    def compute_critic_loss(self, true_reward, critic_reward):\n",
    "        \"\"\"\n",
    "        Computes the critic loss.\n",
    "        \"\"\"\n",
    "        # The critic loss is the mean squared error between the true reward and the critic's predicted reward\n",
    "        critic_loss = tf.reduce_mean(tf.square(true_reward - critic_reward))\n",
    "        return critic_loss\n",
    "\n",
    "    def compute_actor_loss(self, action_selections, log_probs, critic_loss):\n",
    "        \"\"\"\n",
    "        Computes the actor loss.\n",
    "        \"\"\"\n",
    "        # The actor loss is the negative average of the log probabilities of the selected actions, weighted by the critic's loss\n",
    "        # This encourages actions that lead to higher than expected rewards and discourages actions that lead to lower than expected rewards\n",
    "        actor_loss = -tf.reduce_mean(log_probs * critic_loss)\n",
    "        return actor_loss\n",
    "\n",
    "    \n",
    "    def training_step(self):\n",
    "        #get input data\n",
    "        #add batch dimension\n",
    "        population_data = self.current_population.reshape(1, *self.current_population.shape)\n",
    "        actor_output = self.actor(population_data)\n",
    "        print(actor_output.shape)\n",
    "\n",
    "        critic_output = self.critic([population_data, actor_output])\n",
    "        print(critic_output.shape)\n",
    "\n",
    "        #take action\n",
    "        selected_parents = [select_parent(x) for x in actor_output[0,:,:]]\n",
    "        #format for chromax\n",
    "        selected_array = np.array([selected_parents[i:i+2] for i in range(0, len(selected_parents), 2)])\n",
    "        #cross next generation\n",
    "        next_population = self.simulator.cross(self.current_population[selected_array])\n",
    "        #calculate reward\n",
    "        reward = self.simulator.GEBV(next_population).mean() - self.simulator.GEBV(self.current_population).mean() \n",
    "        reward = reward[0]\n",
    "        #calculate losses for actor and critic models\n",
    "        #actor_loss = self.compute_actor_loss()\n",
    " \n",
    "initial_population =  reshape_pop(maizeHaplo) \n",
    "genetic_map = return_genetic_map_df(markerEffects, nChr, geneticMap)\n",
    "reshapeHaplo = reshape_pop(maizeHaplo)\n",
    "np.save('mypop', reshapeHaplo)\n",
    "print(f'reshape haplo {reshapeHaplo.shape}')\n",
    "population_size = int(nInd)\n",
    "marker_count = int((segSites * nChr))\n",
    "chromosome_number = int(nChr)\n",
    "max_generation = 10\n",
    "heritability = .5\n",
    "\n",
    "\n",
    "farm  = BreedingProgram(initial_population, genetic_map, population_size, marker_count, chromosome_number, max_generation, heritability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 30, 2)\n",
      "(1, 100, 50)\n",
      "(1, 50)\n"
     ]
    }
   ],
   "source": [
    "print(farm.current_population.shape)\n",
    "act = farm.training_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 50)\n",
      "(1, 50)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 100, 50])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 25,\n",
       " 11,\n",
       " 0,\n",
       " 13,\n",
       " 34,\n",
       " 35,\n",
       " 4,\n",
       " 10,\n",
       " 44,\n",
       " 34,\n",
       " 5,\n",
       " 49,\n",
       " 49,\n",
       " 20,\n",
       " 35,\n",
       " 47,\n",
       " 35,\n",
       " 46,\n",
       " 32,\n",
       " 32,\n",
       " 4,\n",
       " 11,\n",
       " 47,\n",
       " 9,\n",
       " 44,\n",
       " 8,\n",
       " 15,\n",
       " 40,\n",
       " 29,\n",
       " 35,\n",
       " 29,\n",
       " 24,\n",
       " 46,\n",
       " 2,\n",
       " 14,\n",
       " 21,\n",
       " 26,\n",
       " 31,\n",
       " 39,\n",
       " 11,\n",
       " 25,\n",
       " 16,\n",
       " 10,\n",
       " 41,\n",
       " 27,\n",
       " 3,\n",
       " 43,\n",
       " 28,\n",
       " 22,\n",
       " 6,\n",
       " 21,\n",
       " 33,\n",
       " 39,\n",
       " 22,\n",
       " 22,\n",
       " 8,\n",
       " 31,\n",
       " 17,\n",
       " 35,\n",
       " 47,\n",
       " 31,\n",
       " 11,\n",
       " 47,\n",
       " 29,\n",
       " 7,\n",
       " 43,\n",
       " 11,\n",
       " 1,\n",
       " 33,\n",
       " 14,\n",
       " 17,\n",
       " 12,\n",
       " 3,\n",
       " 2,\n",
       " 29,\n",
       " 12,\n",
       " 26,\n",
       " 22,\n",
       " 17,\n",
       " 29,\n",
       " 21,\n",
       " 29,\n",
       " 27,\n",
       " 31,\n",
       " 37,\n",
       " 7,\n",
       " 6,\n",
       " 24,\n",
       " 21,\n",
       " 29,\n",
       " 16,\n",
       " 28,\n",
       " 14,\n",
       " 14,\n",
       " 32,\n",
       " 33,\n",
       " 24,\n",
       " 18,\n",
       " 32]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "    def run_episode(self):\n",
    "        \"\"\"\n",
    "        runs episode where agent takes action in given environment, updates models\n",
    "        \"\"\"\n",
    "\n",
    "        # AI AGENT Train loop\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Reshape the current population for input into the actor model\n",
    "            # This makes the population shape compatible with the actor model's expected input shape\n",
    "            agent_state = self.current_population.reshape(1, *self.current_population.shape)\n",
    "\n",
    "            # The actor model predicts the probability of being a selected parent for each individual in the population\n",
    "            actor_policy = self.actor(agent_state)\n",
    "\n",
    "            # Process the actor output to grab the indexes of the parents from the input\n",
    "            # This represents the actions taken by the actor based on its current policy\n",
    "            action_selections = [select_parent(x) for x in actor_policy[0,:,:]]\n",
    "\n",
    "            # Reshape action selections into pairs of parents\n",
    "            # This prepares the selected parents for the breeding simulation\n",
    "            selected_array = np.array([action_selections[i:i+2] for i in range(0, len(action_selections), 2)])\n",
    "\n",
    "            # Use the selected parents and the simulator to generate the next generation\n",
    "            next_generation = self.simulator.cross(self.current_population[selected_array])\n",
    "\n",
    "            # The critic model predicts the fitness (reward) of the new population\n",
    "            critic_output = self.critic([agent_state, actor_policy])\n",
    "\n",
    "            # Calculate the true fitness vs critic predicted fitness\n",
    "            # This is used to calculate the reward and the critic's loss\n",
    "            critic_fitness = critic_output[0]\n",
    "            truth_fitness = np.array(self.simulator.GEBV(next_generation)).flatten()\n",
    "\n",
    "            # Ensure that the critic's output and the true fitness have the same shape\n",
    "            # This is necessary for calculating the reward and the critic's loss\n",
    "            assert len(critic_fitness) == len(truth_fitness), 'Mismatched scores?? How?'\n",
    "\n",
    "            # Calculate the reward as the difference in mean fitness between the current and next generation\n",
    "            # This represents the agent's performance for this episode\n",
    "            reward = self.simulator.GEBV(self.current_population).mean() - self.simulator.GEBV(next_generation).mean()\n",
    "            true_reward = reward[0]\n",
    "            critic_reward = tf.reduce_mean(critic_output)\n",
    "\n",
    "            # Calculate the log probabilities of the actions taken\n",
    "            log_probs = tf.math.log(self.actor(agent_state))\n",
    "\n",
    "            # Compute actor and critic losses\n",
    "            critic_loss = self.compute_critic_loss(true_reward, critic_reward)\n",
    "            actor_loss = self.compute_actor_loss(action_selections, log_probs, critic_loss)\n",
    "\n",
    "        print(f''' \n",
    "            critic_loss : {critic_loss}\n",
    "            actor_loss : {actor_loss}\n",
    "                ''')\n",
    "\n",
    "        # Compute the gradients of the critic loss and apply them to the critic network's parameters\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grad, self.critic.trainable_variables))\n",
    "\n",
    "        # Compute the gradients of the actor loss and apply them to the actor network's parameters\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grad, self.actor.trainable_variables))\n",
    "\n",
    "        # Delete the tape manually once gradients are computed\n",
    "        del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape haplo (50, 30, 2)\n"
     ]
    }
   ],
   "source": [
    "# class BreedingProgram:\n",
    "#     \"\"\"\n",
    "#     Represents a breeding program with a PPO agent.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, initial_population, genetic_map, population_size, marker_count, chromosome_number, max_generation, heritability):\n",
    "#         \"\"\"\n",
    "#         Initializes the breeding program.\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Initialize the basic attributes\n",
    "#         self.population_size = population_size\n",
    "#         self.marker_count = marker_count\n",
    "#         self.initial_population = initial_population\n",
    "#         self.genetic_map = genetic_map\n",
    "#         self.max_generation = max_generation\n",
    "\n",
    "#         # Initialize the simulator\n",
    "#         self.simulator = Simulator(genetic_map=self.genetic_map, h2=heritability)\n",
    "#         self.simulator.load_population('mypop.npy')\n",
    "\n",
    "#         # Initialize the current generation and history\n",
    "#         self.current_generation = 0\n",
    "#         self.history = []\n",
    "\n",
    "#         # Initialize the Actor and Critic models\n",
    "#         self.actor = create_actor()\n",
    "#         self.critic = create_critic()\n",
    "#         self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "#         self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "#                 # Start the breeding program\n",
    "#         self._start_breeding_program()\n",
    "        \n",
    "#     def _start_breeding_program(self):\n",
    "#         \"\"\"\n",
    "#         Starts the breeding program.\n",
    "#         \"\"\"\n",
    "#         self.current_population = self.initial_population\n",
    "#         self.history.append(list(farm.simulator.GEBV(farm.current_population)['Yield']))\n",
    "\n",
    "\n",
    "#     def compute_critic_loss(self, true_reward, critic_reward):\n",
    "#         \"\"\"\n",
    "#         Computes the critic loss.\n",
    "#         \"\"\"\n",
    "#         # The critic loss is the mean squared error between the true reward and the critic's predicted reward\n",
    "#         critic_loss = tf.reduce_mean(tf.square(true_reward - critic_reward))\n",
    "#         return critic_loss\n",
    "\n",
    "#     def compute_actor_loss(self, action_selections, log_probs, critic_loss):\n",
    "#         \"\"\"\n",
    "#         Computes the actor loss.\n",
    "#         \"\"\"\n",
    "#         # The actor loss is the negative average of the log probabilities of the selected actions, weighted by the critic's loss\n",
    "#         # This encourages actions that lead to higher than expected rewards and discourages actions that lead to lower than expected rewards\n",
    "#         actor_loss = -tf.reduce_mean(log_probs * critic_loss)\n",
    "#         return actor_loss\n",
    "#     def run_episode(self):\n",
    "#         \"\"\"\n",
    "#         runs episode where agent takes action in given environment, updates models\n",
    "#         \"\"\"\n",
    "\n",
    "#         # AI AGENT Train loop\n",
    "\n",
    "#         with tf.GradientTape(persistent=True) as tape:\n",
    "#             # Reshape the current population for input into the actor model\n",
    "#             # This makes the population shape compatible with the actor model's expected input shape\n",
    "#             agent_state = self.current_population.reshape(1, *self.current_population.shape)\n",
    "\n",
    "#             # The actor model predicts the probability of being a selected parent for each individual in the population\n",
    "#             actor_policy = self.actor(agent_state)\n",
    "\n",
    "#             # Process the actor output to grab the indexes of the parents from the input\n",
    "#             # This represents the actions taken by the actor based on its current policy\n",
    "#             action_selections = [select_parent(x) for x in actor_policy[0,:,:]]\n",
    "\n",
    "#             # Reshape action selections into pairs of parents\n",
    "#             # This prepares the selected parents for the breeding simulation\n",
    "#             selected_array = np.array([action_selections[i:i+2] for i in range(0, len(action_selections), 2)])\n",
    "\n",
    "#             # Use the selected parents and the simulator to generate the next generation\n",
    "#             next_generation = self.simulator.cross(self.current_population[selected_array])\n",
    "\n",
    "#             # The critic model predicts the fitness (reward) of the new population\n",
    "#             critic_output = self.critic([agent_state, actor_policy])\n",
    "\n",
    "#             # Calculate the true fitness vs critic predicted fitness\n",
    "#             # This is used to calculate the reward and the critic's loss\n",
    "#             critic_fitness = critic_output[0]\n",
    "#             truth_fitness = np.array(self.simulator.GEBV(next_generation)).flatten()\n",
    "#             self.history.append(truth_fitness)\n",
    "\n",
    "#             # Ensure that the critic's output and the true fitness have the same shape\n",
    "#             # This is necessary for calculating the reward and the critic's loss\n",
    "#             assert len(critic_fitness) == len(truth_fitness), 'Mismatched scores?? How?'\n",
    "\n",
    "#             # Calculate the reward as the difference in mean fitness between the current and next generation\n",
    "#             # This represents the agent's performance for this episode\n",
    "#             reward = self.simulator.GEBV(self.current_population).mean() - self.simulator.GEBV(next_generation).mean()\n",
    "#             true_reward = tf.constant(reward[0], dtype=tf.float32)\n",
    "#             critic_reward = tf.reduce_mean(critic_output)\n",
    "\n",
    "#             # Calculate the log probabilities of the actions taken\n",
    "#             log_probs = tf.math.log(self.actor(agent_state))\n",
    "\n",
    "#             # Compute actor and critic losses\n",
    "#             critic_loss = tf.reduce_mean((true_reward - critic_reward)**2)\n",
    "#             actor_loss = -tf.reduce_mean(log_probs * critic_loss)\n",
    "\n",
    "#         # print(f''' \n",
    "#         #     critic_loss : {critic_loss}\n",
    "#         #     actor_loss : {actor_loss}\n",
    "#         #         ''')\n",
    "\n",
    "#         # Compute the gradients of the critic loss and apply them to the critic network's parameters\n",
    "#         critic_grad = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "#         self.critic_optimizer.apply_gradients(zip(critic_grad, self.critic.trainable_variables))\n",
    "\n",
    "#         # Compute the gradients of the actor loss and apply them to the actor network's parameters\n",
    "#         actor_grad = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "#         self.actor_optimizer.apply_gradients(zip(actor_grad, self.actor.trainable_variables))\n",
    "\n",
    "#         #update pop\n",
    "\n",
    "        \n",
    "# initial_population =  reshape_pop(maizeHaplo) \n",
    "# genetic_map = return_genetic_map_df(markerEffects, nChr, geneticMap)\n",
    "# reshapeHaplo = reshape_pop(maizeHaplo)\n",
    "# np.save('mypop', reshapeHaplo)\n",
    "# print(f'reshape haplo {reshapeHaplo.shape}')\n",
    "# population_size = int(nInd)\n",
    "# marker_count = int((segSites * nChr))\n",
    "# chromosome_number = int(nChr)\n",
    "# max_generation = 10\n",
    "# heritability = .5\n",
    "\n",
    "\n",
    "# farm  = BreedingProgram(initial_population, genetic_map, population_size, marker_count, chromosome_number, max_generation, heritability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10000/10000 [25:48<00:00,  6.46it/s]\n"
     ]
    }
   ],
   "source": [
    "num_episodes=10000\n",
    "for _ in tqdm(range(num_episodes)):\n",
    "    farm.run_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(farm.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3fe96c9070>]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGdCAYAAADuR1K7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAf20lEQVR4nO3de3BU9f3/8deGkATFTcotayARbalEpNAGE8J0htbsGJSOpOKIGQSkGSkV0BpKAUUy2nbSilZQUMaZOgxVCoVaWpHi0GCVysoleOEWxnaUq5uAmA2iJDH5/P7wx9qVEMFvTpJ983zMnGE4+zm7n8+ZwD7ncHbxOeecAAAAjEjo6AkAAAC0JeIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAApiR29AQ6QnNzs44eParLLrtMPp+vo6cDAADOg3NOJ0+eVEZGhhISzn195qKMm6NHjyozM7OjpwEAAL6GQ4cOqV+/fud8/KKMm8suu0zS5yfH7/d38GwAAMD5qKurU2ZmZvR9/Fwuyrg5809Rfr+fuAEAIM581S0l3FAMAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADClXeJmyZIl6t+/v1JSUpSXl6dt27a1On716tUaOHCgUlJSNHjwYK1fv/6cY6dOnSqfz6eFCxe28awBAEA88jxuVq1apdLSUpWVlWnnzp0aMmSICgsLVVNT0+L4LVu2qLi4WCUlJXrzzTdVVFSkoqIi7d69+6yxf/3rX/XGG28oIyPD62UAAIA44Xnc/P73v9ddd92lyZMn65prrtHSpUt1ySWX6Nlnn21x/KJFizRq1CjNmjVL2dnZ+tWvfqXvfe97Wrx4ccy4I0eOaMaMGXr++efVtWtXr5cBAADihKdx09DQoMrKSgWDwS9eMCFBwWBQoVCoxWNCoVDMeEkqLCyMGd/c3KwJEyZo1qxZGjRo0FfOo76+XnV1dTEbAACwydO4OX78uJqampSenh6zPz09XeFwuMVjwuHwV47/3e9+p8TERN1zzz3nNY/y8nKlpqZGt8zMzAtcCQAAiBdx92mpyspKLVq0SMuWLZPP5zuvY+bOnatIJBLdDh065PEsAQBAR/E0bnr16qUuXbqouro6Zn91dbUCgUCLxwQCgVbHb968WTU1NcrKylJiYqISExN14MABzZw5U/3792/xOZOTk+X3+2M2AABgk6dxk5SUpJycHFVUVET3NTc3q6KiQvn5+S0ek5+fHzNekjZu3BgdP2HCBL3zzjt66623oltGRoZmzZqll19+2bvFAACAuJDo9QuUlpZq0qRJGjZsmHJzc7Vw4UKdOnVKkydPliRNnDhRffv2VXl5uSTp3nvv1ciRI/XYY49p9OjRWrlypXbs2KFnnnlGktSzZ0/17Nkz5jW6du2qQCCgq6++2uvlAACATs7zuBk3bpyOHTum+fPnKxwOa+jQodqwYUP0puGDBw8qIeGLC0gjRozQihUrNG/ePN1///0aMGCA1q5dq2uvvdbrqQIAAAN8zjnX0ZNob3V1dUpNTVUkEuH+GwAA4sT5vn/H3aelAAAAWkPcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwJR2iZslS5aof//+SklJUV5enrZt29bq+NWrV2vgwIFKSUnR4MGDtX79+uhjjY2Nmj17tgYPHqxLL71UGRkZmjhxoo4ePer1MgAAQBzwPG5WrVql0tJSlZWVaefOnRoyZIgKCwtVU1PT4vgtW7aouLhYJSUlevPNN1VUVKSioiLt3r1bkvTJJ59o586devDBB7Vz50698MIL2r9/v26++WavlwIAAOKAzznnvHyBvLw8XXfddVq8eLEkqbm5WZmZmZoxY4bmzJlz1vhx48bp1KlTWrduXXTf8OHDNXToUC1durTF19i+fbtyc3N14MABZWVlfeWc6urqlJqaqkgkIr/f/zVXBgAA2tP5vn97euWmoaFBlZWVCgaDX7xgQoKCwaBCoVCLx4RCoZjxklRYWHjO8ZIUiUTk8/mUlpbW4uP19fWqq6uL2QAAgE2exs3x48fV1NSk9PT0mP3p6ekKh8MtHhMOhy9o/OnTpzV79mwVFxefs+LKy8uVmpoa3TIzM7/GagAAQDyI609LNTY26rbbbpNzTk8//fQ5x82dO1eRSCS6HTp0qB1nCQAA2lOil0/eq1cvdenSRdXV1TH7q6urFQgEWjwmEAic1/gzYXPgwAFt2rSp1X97S05OVnJy8tdcBQAAiCeeXrlJSkpSTk6OKioqovuam5tVUVGh/Pz8Fo/Jz8+PGS9JGzdujBl/Jmzeffdd/fOf/1TPnj29WQAAAIg7nl65kaTS0lJNmjRJw4YNU25urhYuXKhTp05p8uTJkqSJEyeqb9++Ki8vlyTde++9GjlypB577DGNHj1aK1eu1I4dO/TMM89I+jxsbr31Vu3cuVPr1q1TU1NT9H6cHj16KCkpyeslAQCATszzuBk3bpyOHTum+fPnKxwOa+jQodqwYUP0puGDBw8qIeGLC0gjRozQihUrNG/ePN1///0aMGCA1q5dq2uvvVaSdOTIEf3973+XJA0dOjTmtV555RX94Ac/8HpJAACgE/P8e246I77nBgCA+NMpvucGAACgvRE3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMKVd4mbJkiXq37+/UlJSlJeXp23btrU6fvXq1Ro4cKBSUlI0ePBgrV+/PuZx55zmz5+vyy+/XN26dVMwGNS7777r5RIAAECc8DxuVq1apdLSUpWVlWnnzp0aMmSICgsLVVNT0+L4LVu2qLi4WCUlJXrzzTdVVFSkoqIi7d69OzrmkUce0RNPPKGlS5dq69atuvTSS1VYWKjTp097vRwAANDJ+ZxzzssXyMvL03XXXafFixdLkpqbm5WZmakZM2Zozpw5Z40fN26cTp06pXXr1kX3DR8+XEOHDtXSpUvlnFNGRoZmzpypX/ziF5KkSCSi9PR0LVu2TLfffvtXzqmurk6pqamKRCLy+/1ttFIAAOCl833/9vTKTUNDgyorKxUMBr94wYQEBYNBhUKhFo8JhUIx4yWpsLAwOv69995TOByOGZOamqq8vLxzPmd9fb3q6upiNgAAYJOncXP8+HE1NTUpPT09Zn96errC4XCLx4TD4VbHn/n1Qp6zvLxcqamp0S0zM/NrrQcAAHR+F8WnpebOnatIJBLdDh061NFTAgAAHvE0bnr16qUuXbqouro6Zn91dbUCgUCLxwQCgVbHn/n1Qp4zOTlZfr8/ZgMAADZ5GjdJSUnKyclRRUVFdF9zc7MqKiqUn5/f4jH5+fkx4yVp48aN0fFXXnmlAoFAzJi6ujpt3br1nM8JAAAuHolev0BpaakmTZqkYcOGKTc3VwsXLtSpU6c0efJkSdLEiRPVt29flZeXS5LuvfdejRw5Uo899phGjx6tlStXaseOHXrmmWckST6fTz//+c/161//WgMGDNCVV16pBx98UBkZGSoqKvJ6OQAAoJPzPG7GjRunY8eOaf78+QqHwxo6dKg2bNgQvSH44MGDSkj44gLSiBEjtGLFCs2bN0/333+/BgwYoLVr1+raa6+NjvnlL3+pU6dOacqUKaqtrdX3v/99bdiwQSkpKV4vBwAAdHKef89NZ8T33AAAEH86xffcAAAAtDfiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKZ4FjcnTpzQ+PHj5ff7lZaWppKSEn388cetHnP69GlNmzZNPXv2VPfu3TV27FhVV1dHH3/77bdVXFyszMxMdevWTdnZ2Vq0aJFXSwAAAHHIs7gZP3689uzZo40bN2rdunV67bXXNGXKlFaPue+++/Tiiy9q9erVevXVV3X06FHdcsst0ccrKyvVp08fPffcc9qzZ48eeOABzZ07V4sXL/ZqGQAAIM74nHOurZ903759uuaaa7R9+3YNGzZMkrRhwwbddNNNOnz4sDIyMs46JhKJqHfv3lqxYoVuvfVWSVJVVZWys7MVCoU0fPjwFl9r2rRp2rdvnzZt2nTe86urq1NqaqoikYj8fv/XWCEAAGhv5/v+7cmVm1AopLS0tGjYSFIwGFRCQoK2bt3a4jGVlZVqbGxUMBiM7hs4cKCysrIUCoXO+VqRSEQ9evRou8kDAIC4lujFk4bDYfXp0yf2hRIT1aNHD4XD4XMek5SUpLS0tJj96enp5zxmy5YtWrVqlV566aVW51NfX6/6+vro7+vq6s5jFQAAIB5d0JWbOXPmyOfztbpVVVV5NdcYu3fv1pgxY1RWVqYbbrih1bHl5eVKTU2NbpmZme0yRwAA0P4u6MrNzJkzdeedd7Y65qqrrlIgEFBNTU3M/s8++0wnTpxQIBBo8bhAIKCGhgbV1tbGXL2prq4+65i9e/eqoKBAU6ZM0bx5875y3nPnzlVpaWn093V1dQQOAABGXVDc9O7dW7179/7Kcfn5+aqtrVVlZaVycnIkSZs2bVJzc7Py8vJaPCYnJ0ddu3ZVRUWFxo4dK0nav3+/Dh48qPz8/Oi4PXv26Prrr9ekSZP0m9/85rzmnZycrOTk5PMaCwAA4psnn5aSpBtvvFHV1dVaunSpGhsbNXnyZA0bNkwrVqyQJB05ckQFBQVavny5cnNzJUk/+9nPtH79ei1btkx+v18zZsyQ9Pm9NdLn/xR1/fXXq7CwUAsWLIi+VpcuXc4rus7g01IAAMSf833/9uSGYkl6/vnnNX36dBUUFCghIUFjx47VE088EX28sbFR+/fv1yeffBLd9/jjj0fH1tfXq7CwUE899VT08TVr1ujYsWN67rnn9Nxzz0X3X3HFFXr//fe9WgoAAIgjnl256cy4cgMAQPzp0O+5AQAA6CjEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCmexc2JEyc0fvx4+f1+paWlqaSkRB9//HGrx5w+fVrTpk1Tz5491b17d40dO1bV1dUtjv3www/Vr18/+Xw+1dbWerACAAAQjzyLm/Hjx2vPnj3auHGj1q1bp9dee01Tpkxp9Zj77rtPL774olavXq1XX31VR48e1S233NLi2JKSEn3nO9/xYuoAACCO+Zxzrq2fdN++fbrmmmu0fft2DRs2TJK0YcMG3XTTTTp8+LAyMjLOOiYSiah3795asWKFbr31VklSVVWVsrOzFQqFNHz48OjYp59+WqtWrdL8+fNVUFCgjz76SGlpaec9v7q6OqWmpioSicjv9//fFgsAANrF+b5/e3LlJhQKKS0tLRo2khQMBpWQkKCtW7e2eExlZaUaGxsVDAaj+wYOHKisrCyFQqHovr179+rhhx/W8uXLlZBwftOvr69XXV1dzAYAAGzyJG7C4bD69OkTsy8xMVE9evRQOBw+5zFJSUlnXYFJT0+PHlNfX6/i4mItWLBAWVlZ5z2f8vJypaamRrfMzMwLWxAAAIgbFxQ3c+bMkc/na3Wrqqryaq6aO3eusrOzdccdd1zwcZFIJLodOnTIoxkCAICOlnghg2fOnKk777yz1TFXXXWVAoGAampqYvZ/9tlnOnHihAKBQIvHBQIBNTQ0qLa2NubqTXV1dfSYTZs2adeuXVqzZo0k6cztQr169dIDDzyghx56qMXnTk5OVnJy8vksEQAAxLkLipvevXurd+/eXzkuPz9ftbW1qqysVE5OjqTPw6S5uVl5eXktHpOTk6OuXbuqoqJCY8eOlSTt379fBw8eVH5+viTpL3/5iz799NPoMdu3b9dPfvITbd68Wd/85jcvZCkAAMCoC4qb85Wdna1Ro0bprrvu0tKlS9XY2Kjp06fr9ttvj35S6siRIyooKNDy5cuVm5ur1NRUlZSUqLS0VD169JDf79eMGTOUn58f/aTUlwPm+PHj0de7kE9LAQAAuzyJG0l6/vnnNX36dBUUFCghIUFjx47VE088EX28sbFR+/fv1yeffBLd9/jjj0fH1tfXq7CwUE899ZRXUwQAAAZ58j03nR3fcwMAQPzp0O+5AQAA6CjEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMCWxoyfQEZxzkqS6uroOngkAADhfZ963z7yPn8tFGTcnT56UJGVmZnbwTAAAwIU6efKkUlNTz/m4z31V/hjU3Nyso0eP6rLLLpPP5+vo6XS4uro6ZWZm6tChQ/L7/R09HbM4z+2D89w+OM/tg/McyzmnkydPKiMjQwkJ576z5qK8cpOQkKB+/fp19DQ6Hb/fzx+edsB5bh+c5/bBeW4fnOcvtHbF5gxuKAYAAKYQNwAAwBTiBkpOTlZZWZmSk5M7eiqmcZ7bB+e5fXCe2wfn+eu5KG8oBgAAdnHlBgAAmELcAAAAU4gbAABgCnEDAABMIW4uAidOnND48ePl9/uVlpamkpISffzxx60ec/r0aU2bNk09e/ZU9+7dNXbsWFVXV7c49sMPP1S/fv3k8/lUW1vrwQrigxfn+e2331ZxcbEyMzPVrVs3ZWdna9GiRV4vpdNZsmSJ+vfvr5SUFOXl5Wnbtm2tjl+9erUGDhyolJQUDR48WOvXr4953Dmn+fPn6/LLL1e3bt0UDAb17rvvermEuNCW57mxsVGzZ8/W4MGDdemllyojI0MTJ07U0aNHvV5Gp9fWP8//a+rUqfL5fFq4cGEbzzrOOJg3atQoN2TIEPfGG2+4zZs3u29961uuuLi41WOmTp3qMjMzXUVFhduxY4cbPny4GzFiRItjx4wZ42688UYnyX300UcerCA+eHGe//CHP7h77rnH/etf/3L//e9/3R//+EfXrVs39+STT3q9nE5j5cqVLikpyT377LNuz5497q677nJpaWmuurq6xfGvv/6669Kli3vkkUfc3r173bx581zXrl3drl27omN++9vfutTUVLd27Vr39ttvu5tvvtldeeWV7tNPP22vZXU6bX2ea2trXTAYdKtWrXJVVVUuFAq53Nxcl5OT057L6nS8+Hk+44UXXnBDhgxxGRkZ7vHHH/d4JZ0bcWPc3r17nSS3ffv26L5//OMfzufzuSNHjrR4TG1trevatatbvXp1dN++ffucJBcKhWLGPvXUU27kyJGuoqLioo4br8/z/7r77rvdD3/4w7abfCeXm5vrpk2bFv19U1OTy8jIcOXl5S2Ov+2229zo0aNj9uXl5bmf/vSnzjnnmpubXSAQcAsWLIg+Xltb65KTk92f/vQnD1YQH9r6PLdk27ZtTpI7cOBA20w6Dnl1ng8fPuz69u3rdu/e7a644oqLPm74ZynjQqGQ0tLSNGzYsOi+YDCohIQEbd26tcVjKisr1djYqGAwGN03cOBAZWVlKRQKRfft3btXDz/8sJYvX97qf2B2MfDyPH9ZJBJRjx492m7ynVhDQ4MqKytjzlFCQoKCweA5z1EoFIoZL0mFhYXR8e+9957C4XDMmNTUVOXl5bV63i3z4jy3JBKJyOfzKS0trU3mHW+8Os/Nzc2aMGGCZs2apUGDBnkz+Thzcb8jXQTC4bD69OkTsy8xMVE9evRQOBw+5zFJSUln/QWUnp4ePaa+vl7FxcVasGCBsrKyPJl7PPHqPH/Zli1btGrVKk2ZMqVN5t3ZHT9+XE1NTUpPT4/Z39o5CofDrY4/8+uFPKd1XpznLzt9+rRmz56t4uLii/Y/gPTqPP/ud79TYmKi7rnnnrafdJwibuLUnDlz5PP5Wt2qqqo8e/25c+cqOztbd9xxh2ev0Rl09Hn+X7t379aYMWNUVlamG264oV1eE2gLjY2Nuu222+Sc09NPP93R0zGlsrJSixYt0rJly+Tz+Tp6Op1GYkdPAF/PzJkzdeedd7Y65qqrrlIgEFBNTU3M/s8++0wnTpxQIBBo8bhAIKCGhgbV1tbGXFWorq6OHrNp0ybt2rVLa9askfT5p08kqVevXnrggQf00EMPfc2VdS4dfZ7P2Lt3rwoKCjRlyhTNmzfva60lHvXq1UtdunQ565N6LZ2jMwKBQKvjz/xaXV2tyy+/PGbM0KFD23D28cOL83zGmbA5cOCANm3adNFetZG8Oc+bN29WTU1NzBX0pqYmzZw5UwsXLtT777/ftouIFx190w+8deZG1x07dkT3vfzyy+d1o+uaNWui+6qqqmJudP3Pf/7jdu3aFd2effZZJ8lt2bLlnHf9W+bVeXbOud27d7s+ffq4WbNmebeATiw3N9dNnz49+vumpibXt2/fVm/A/NGPfhSzLz8//6wbih999NHo45FIhBuK2/g8O+dcQ0ODKyoqcoMGDXI1NTXeTDzOtPV5Pn78eMzfxbt27XIZGRlu9uzZrqqqyruFdHLEzUVg1KhR7rvf/a7bunWr+/e//+0GDBgQ8xHlw4cPu6uvvtpt3bo1um/q1KkuKyvLbdq0ye3YscPl5+e7/Pz8c77GK6+8clF/Wso5b87zrl27XO/evd0dd9zhPvjgg+h2Mb1RrFy50iUnJ7tly5a5vXv3uilTpri0tDQXDoedc85NmDDBzZkzJzr+9ddfd4mJie7RRx91+/btc2VlZS1+FDwtLc397W9/c++8844bM2YMHwVv4/Pc0NDgbr75ZtevXz/31ltvxfz81tfXd8gaOwMvfp6/jE9LETcXhQ8//NAVFxe77t27O7/f7yZPnuxOnjwZffy9995zktwrr7wS3ffpp5+6u+++233jG99wl1xyifvxj3/sPvjgg3O+BnHjzXkuKytzks7arrjiinZcWcd78sknXVZWlktKSnK5ubnujTfeiD42cuRIN2nSpJjxf/7zn923v/1tl5SU5AYNGuReeumlmMebm5vdgw8+6NLT011ycrIrKChw+/fvb4+ldGpteZ7P/Ly3tP3vn4GLUVv/PH8ZceOcz7n/f7MEAACAAXxaCgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABM+X9JnGEujayxKgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x.mean() for x in farm.history[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
