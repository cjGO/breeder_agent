{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "\n",
    "from rpy2.robjects.packages import importr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from chromax import Simulator\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "import matplotlib\n",
    "from jax import device_get\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_genetic_map(n_markers, n_chromosomes):\n",
    "  df = pd.DataFrame(generate_marker_effects(n_markers=n_markers), columns=['Yield'])\n",
    "  df['cM'] = np.random.uniform(0, 200, len(df))\n",
    "  df['CHR.PHYS'] = '1A'\n",
    "  df = df.sort_values(by='cM')\n",
    "  df = df[['CHR.PHYS', 'cM', 'Yield']]\n",
    "  # save df as csv under filename\n",
    "  return df\n",
    "\n",
    "def simplify_geneticmap(lst, qtl):\n",
    "    # Select 'qtl' random indexes\n",
    "    indexes_positive = random.sample(range(len(lst)), qtl)\n",
    "    \n",
    "    # Select 'qtl' random indexes not already selected\n",
    "    remaining_indexes = set(range(len(lst))) - set(indexes_positive)\n",
    "    indexes_negative = random.sample(remaining_indexes, qtl)\n",
    "\n",
    "    # Modify the list\n",
    "    for i in range(len(lst)):\n",
    "        if i in indexes_positive:\n",
    "            lst[i] = random.uniform(0.5, 1.0)  # Assign random float between 0.5 and 1.0\n",
    "        elif i in indexes_negative:\n",
    "            lst[i] = random.uniform(-0.5, -1.0)  # Assign random float between -0.5 and -1.0\n",
    "        else:\n",
    "            lst[i] = 0  # Assign 0\n",
    "\n",
    "    return lst\n",
    "\n",
    "def generate_population(n_pop=100, n_markers=500):\n",
    "    \"\"\"\n",
    "    Generate a numpy array of randoms of length 500 with randomized 0, 1, or 2 at each position.\n",
    "    It will generate 100 individuals based on n_pop.\n",
    "\n",
    "    Returns: numpy array of size (n_pop, n_markers)\n",
    "    \"\"\"\n",
    "    shape=(n_pop, n_markers, 2)\n",
    "    # Define the elements to choose from and their associated probabilities\n",
    "    elements = [0, 1, 2]\n",
    "    probabilities = [1/3, 1/3, 1/3]  # equal probabilities for 0, 1, and 2\n",
    "\n",
    "    # Generate the population\n",
    "    population = np.random.choice(elements, size=(n_pop, n_markers), p=probabilities)\n",
    "\n",
    "    return np.random.choice([True, False], size=shape)\n",
    "\n",
    "\n",
    "def generate_marker_effects(n_markers=500, mu=0, sigma=0.1):\n",
    "    \"\"\"\n",
    "    Generate a numpy array of marker effects with a normal distribution.\n",
    "\n",
    "    Parameters:\n",
    "    n_markers (int): Number of markers.\n",
    "    mu (float): Mean of the distribution.\n",
    "    sigma (float): Standard deviation of the distribution.\n",
    "\n",
    "    Returns:\n",
    "    numpy array of marker effects\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the marker effects\n",
    "    marker_effects = np.random.normal(mu, sigma, n_markers)\n",
    "\n",
    "    return marker_effects\n",
    "\n",
    "def parse_markerEffects(genetic_map, nChr):\n",
    "    # Get the length of the genetic map\n",
    "    length = len(genetic_map)\n",
    "\n",
    "    # Create a new array for storing the chromosome number for each marker\n",
    "    chr = [0] * length\n",
    "\n",
    "    # Calculate the number of markers per chromosome\n",
    "    markers_per_chr = length // nChr\n",
    "\n",
    "    # Iterate over the range of the genetic map length\n",
    "    for i in range(length):\n",
    "        # Calculate the chromosome number and store it in the chr array\n",
    "        chr[i] = i // markers_per_chr + 1\n",
    "\n",
    "    return chr\n",
    "\n",
    "\n",
    "def reshape_pop(maizeHaplo):\n",
    "    reshapeHaplo = maizeHaplo.reshape(int((maizeHaplo.shape[0])/2),2,maizeHaplo.shape[1])\n",
    "    reshapeHaplo = reshapeHaplo.transpose((0,2,1))\n",
    "    return reshapeHaplo\n",
    "\n",
    "def return_genetic_map_df(markerEffects, nChr, geneticMap):\n",
    "    chr = parse_markerEffects(markerEffects, nChr)\n",
    "    chr = [int(x[0]) for x in chr]\n",
    "    trait = markerEffects\n",
    "    pos = geneticMap\n",
    "    # Assuming chr, trait, pos are your arrays\n",
    "    df = pd.DataFrame({'CHR.PHYS': chr, 'Yield': trait, 'cM': pos})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "x <- seq(0, 2*pi, length.out=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%R -o x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %R install.packages(\"AlphaSimR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(\"AlphaSimR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "nInd = 10\n",
    "nChr = 2\n",
    "segSites = 5\n",
    "\n",
    "founderGenomes = runMacs(nInd = nInd,\n",
    "                         nChr = nChr,\n",
    "                         segSites = segSites,\n",
    "                         species = \"MAIZE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "SP = SimParam$new(founderGenomes)\n",
    "SP$addTraitA(segSites)\n",
    "# SP$setVarE(h2=.02)\n",
    "pop = newPop(founderGenomes, simParam=SP)\n",
    "ans = fastRRBLUP(pop, simParam=SP, useQtl=TRUE, use='gv')\n",
    "ans@gv[[1]]@addEff\n",
    "markerEffects = slot(slot(ans, \"gv\")[[1]], \"addEff\")\n",
    "maizeHaplo = pullSegSiteHaplo(pop)\n",
    "maizeGeno = pullSegSiteGeno(pop)\n",
    "#cm positions of each marker\n",
    "genMap = SP$genMap\n",
    "geneticMap = unlist(genMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%R -o maizeHaplo\n",
    "%R -o maizeGeno\n",
    "%R -o markerEffects\n",
    "%R -o geneticMap\n",
    "%R -o nInd\n",
    "%R -o nChr\n",
    "%R -o segSites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nMarkers = segSites * nChr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Reshape\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "#dummy functions to  generate fake data to develop the training pipeline\n",
    "def pop_gen(b, n, m, d):\n",
    "    return np.random.randint(2, size=(b, n, m, d))\n",
    "def reward_gen():\n",
    "    return np.random.rand\n",
    "def scores_gen(n):\n",
    "    return np.random.rand(1, n)\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "def get_r(values):\n",
    "\n",
    "    # Create an array of indices, same length as your list\n",
    "    indices = np.array(range(len(values)))\n",
    "\n",
    "    # Perform linear regression\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(indices, values)\n",
    "    return slope\n",
    "\n",
    "# genetic_map = return_genetic_map_df(markerEffects, nChr, geneticMap)\n",
    "# genetic_map['Yield'] = simplify_geneticmap(list(genetic_map['Yield']),4)\n",
    "\n",
    "# n = int(nInd[0])\n",
    "# m = int(nMarkers)\n",
    "# d = 2\n",
    "# total_parents = n*2\n",
    "\n",
    "# population_dummy = pop_gen(1, n, m, d)  # Extra dimension for batch size\n",
    "# scores_dummy =  scores_gen(n) # Extra dimension for batch size\n",
    "\n",
    "# actor_model = create_actor(n,m,d,total_parents, population_dummy, scores_dummy)\n",
    "# actor_output = actor_model([population_dummy, scores_dummy])\n",
    "# critic_model = create_critic(n,m,d, population_dummy, scores_dummy,actor_output)\n",
    "\n",
    "# simulator = Simulator(genetic_map=genetic_map, h2=.5)\n",
    "# simulator.load_population('mypop.npy')\n",
    "# initial_scores = simulator.GEBV(population_dummy[0])\n",
    "\n",
    "# TOTAL_EPISODES=3\n",
    "# before = print_memory_usage()\n",
    "# for episodes in range(TOTAL_EPISODES):\n",
    "#     inputs = tf.random.normal([1, n, m, d]) # replace with actual inputs\n",
    "#     scores_dummy = tf.random.normal([1, n]) # replace with actual scores\n",
    "#     #policy, current_population, simulator\n",
    "#     new_pop,reward = policy2offspring_reward(actor_output, population_dummy, simulator)\n",
    "#     train_step(actor_model, critic_model, inputs, scores_dummy, reward)\n",
    "#     tf.keras.backend.clear_session()\n",
    "# after = print_memory_usage()\n",
    "\n",
    "# print(f'cost per episode: {( after - before ) / TOTAL_EPISODES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def split_and_average(lst, set_size):\n",
    "    # lst is from get_baseline\n",
    "    #average list\n",
    "    lst = [x.mean() for x in lst]\n",
    "    # Split the list into sublists\n",
    "    split_lst = [lst[i:i + set_size] for i in range(0, len(lst), set_size)]\n",
    "\n",
    "    # Calculate the average of the x-th element in each sublist\n",
    "    averages = []\n",
    "    for x in range(set_size):\n",
    "        x_elements = [sublist[x] for sublist in split_lst if len(sublist) > x]\n",
    "        averages.append(mean(x_elements))\n",
    "\n",
    "    return averages\n",
    "\n",
    "def sample_two_offspring_per_cross(dataset):\n",
    "    \"\"\"\n",
    "    Samples two offspring per cross from the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (np.ndarray): The input dataset with shape (n_cross, n_offspring, markers, diploid).\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A new array with sampled offspring, shape (n_cross, 2, markers, diploid).\n",
    "    \"\"\"\n",
    "    n_cross = dataset.shape[0]\n",
    "    n_offspring_per_cross = 2\n",
    "    # Create an array to store the indices of the offspring to sample\n",
    "    indices = np.random.choice(dataset.shape[1], size=(n_cross, n_offspring_per_cross), replace=False)\n",
    "    # Use advanced indexing to select two offspring per cross\n",
    "    sampled_offspring = dataset[np.arange(n_cross)[:, None], indices, :, :]\n",
    "    return sampled_offspring.reshape(-1, sampled_offspring.shape[2], sampled_offspring.shape[3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def collapse_first_two_axes(dataset):\n",
    "    \"\"\"\n",
    "    Collapses the first two axes of the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (np.ndarray): The input dataset with shape (n_cross, n_offspring, markers, diploid).\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A new array with the first two axes collapsed, shape (n_cross * n_offspring, markers, diploid).\n",
    "    \"\"\"\n",
    "    # Calculate the new shape\n",
    "    new_shape = (dataset.shape[0] * dataset.shape[1],) + dataset.shape[2:]\n",
    "    # Reshape the dataset\n",
    "    collapsed_dataset = dataset.reshape(new_shape)\n",
    "    return collapsed_dataset\n",
    "\n",
    "\n",
    "def select_parents(policy, total_parents=5):\n",
    "    \"\"\"\n",
    "        input: Policy from actor ( metric value for each individual )\n",
    "        returns : index of parents to be included in random_crosses for next step of breeding program\n",
    "    \"\"\"\n",
    "    values, indices = tf.math.top_k(policy, total_parents)\n",
    "    return values,indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd  # Import pandas for rolling average\n",
    "\n",
    "def normalize_data(data, mean, std_dev):\n",
    "    return (data - mean) / std_dev\n",
    "\n",
    "def plot_training_progress(actor_losses, critic_losses, average_scores, iteration, agent):\n",
    "    # Calculate mean and standard deviation of average_scores for normalization\n",
    "    average_scores_mean = np.mean(average_scores)\n",
    "    average_scores_std = np.std(average_scores)\n",
    "\n",
    "    # Normalize each array independently using Z-score normalization\n",
    "    actor_losses_normalized = normalize_data(actor_losses, average_scores_mean, average_scores_std)\n",
    "    critic_losses_normalized = normalize_data(critic_losses, average_scores_mean, average_scores_std)\n",
    "    average_scores_normalized = normalize_data(average_scores, average_scores_mean, average_scores_std)\n",
    "\n",
    "    # Calculate the window size as 10% of the length of the average_scores array\n",
    "    window_size = max(int(len(average_scores) * 0.1), 1)  # Ensure at least one element in the window\n",
    "\n",
    "    # Convert lists to pandas Series\n",
    "    actor_losses_series = pd.Series(actor_losses_normalized)\n",
    "    critic_losses_series = pd.Series(critic_losses_normalized)\n",
    "    average_scores_series = pd.Series(average_scores_normalized)\n",
    "\n",
    "    # Calculate rolling averages\n",
    "    actor_losses_rolling = actor_losses_series.rolling(window=window_size).mean().fillna(method='bfill')\n",
    "    critic_losses_rolling = critic_losses_series.rolling(window=window_size).mean().fillna(method='bfill')\n",
    "    average_scores_rolling = average_scores_series.rolling(window=window_size).mean().fillna(method='bfill')\n",
    "\n",
    "    # Normalize baselines using the mean and std dev of average_scores\n",
    "    random_baseline_normalized = normalize_data(np.array([agent.random_baseline]), average_scores_mean, average_scores_std)\n",
    "    truncation_baseline_normalized = normalize_data(np.array([agent.truncation_baseline]), average_scores_mean, average_scores_std)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot all normalized rolling averages on the same plot\n",
    "    plt.plot(actor_losses_rolling, label='Actor Loss (rolling avg)')\n",
    "    plt.plot(critic_losses_rolling, label='Critic Loss (rolling avg)')\n",
    "    plt.plot(average_scores_rolling, label='Average Score (rolling avg)')\n",
    "\n",
    "    # Add horizontal lines for normalized baselines\n",
    "    plt.axhline(y=random_baseline_normalized, color='r', linestyle='--', label='Random Baseline (normalized)')\n",
    "    plt.axhline(y=truncation_baseline_normalized, color='g', linestyle='--', label='Truncation Baseline (normalized)')\n",
    "\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.title('Normalized Rolling Averages of Training Progress Over Time')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "\n",
    "\n",
    "def add_batchdim(arr):\n",
    "    return np.expand_dims(arr, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS FO SIMULATING THE ENVIRONMENT AND ACTIONS\n",
    "\n",
    "def create_fake_geneticmap(number_markers):\n",
    "    # 'chr' will always be '1A' for every marker\n",
    "    chr_array = ['1A'] * number_markers\n",
    "    \n",
    "    # 'yield': Create a marker_strength array with 1 float between -0.5 and +0.5 randomly\n",
    "    yield_array = np.random.poisson(5, size=number_markers)\n",
    "    \n",
    "    # 'cM': create an array for number_markers length evenly sampled between 0 and 100\n",
    "    cM_array = np.linspace(0, 100, num=number_markers)\n",
    "    \n",
    "    # Create the DataFrame with the auto-generated data\n",
    "    df = pd.DataFrame({'CHR.PHYS': chr_array, 'Yield': yield_array, 'cM': cM_array})\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_fake_population(total_pops, number_individuals, number_markers, ploidy):\n",
    "    # Generate a random population array with values 0 or 1\n",
    "    population = np.random.randint(2, size=(total_pops,number_individuals, number_markers, ploidy))\n",
    "    return population\n",
    "\n",
    "\n",
    "def calculate_scores(population, marker_strength):\n",
    "    \"\"\"\n",
    "    Calculates the additive score by matrix multipling the population (n,m) with the marker strengths (m,)\n",
    "    \"\"\"\n",
    "    # Perform a dot product between the dosages and marker strength arrays\n",
    "    dosages = np.sum(population,axis=2)\n",
    "    scores = np.dot(dosages, marker_strength)\n",
    "    return scores\n",
    "\n",
    "def panmixia(selected_parents, total_offspring = selected_parents.shape[0]*2):\n",
    "    \"\"\" Handles the random crossing for us ; heuristic!\n",
    "\n",
    "    selected_parents ( n , m , d )\n",
    "    \n",
    "    \"\"\"\n",
    "    n, m, d = selected_parents.shape\n",
    "    offspring_target = total_offspring\n",
    "    offspring_list = []\n",
    "\n",
    "    while len(offspring_list) < offspring_target:\n",
    "        # Randomly pick two parents without replacement\n",
    "        parents_indices = np.random.choice(n, size=2, replace=False)\n",
    "        parent1, parent2 = selected_parents[parents_indices]\n",
    "\n",
    "        # Simulate random recombination for each marker\n",
    "        offspring = np.zeros((m, d), dtype=parent1.dtype)\n",
    "        for i in range(m):\n",
    "            # Randomly choose one allele from each parent for each marker\n",
    "            for j in range(d):\n",
    "                parent_allele = np.random.choice([parent1[i, j], parent2[i, j]])\n",
    "                offspring[i, j] = parent_allele\n",
    "\n",
    "        # Add the new offspring to the offspring list\n",
    "        offspring_list.append(offspring)\n",
    "\n",
    "    # Convert offspring list to a numpy array with shape (offspring_target, m, d)\n",
    "    offspring_array = np.array(offspring_list)\n",
    "    return offspring_array\n",
    "\n",
    "\n",
    "def scores2parents(scores,K):\n",
    "    \"\"\"\n",
    "    hint: use output from calculate_scores\n",
    "    \"\"\"\n",
    "    # Specify the number of top values you want (K)\n",
    "    K = 5\n",
    "\n",
    "    # Get the indices that would sort the array\n",
    "    sorted_indices = np.argsort(scores)\n",
    "\n",
    "    # Take the last K indices of the sorted indices array\n",
    "    top_k_indices = sorted_indices[-K:]\n",
    "\n",
    "    # Since argsort returns indices in ascending order, reverse to get the top values\n",
    "    top_k_indices = top_k_indices[::-1]\n",
    "\n",
    "    print(\"Indices of top K values:\", top_k_indices)\n",
    "    print(\"Top K values:\", scores[top_k_indices])\n",
    "\n",
    "    return top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'compute_baselines': False,\n",
    "    'initial_population': initial_population,\n",
    "    'ploidy':ploidy,\n",
    "    'genetic_map': genetic_map_df,\n",
    "    'population_size': number_individuals,\n",
    "    'marker_count': number_markers,\n",
    "    'heritability': .99,\n",
    "    'episodes': 10,\n",
    "    'cycles': 2,\n",
    "    'learning_rate': .001,\n",
    "    'replicates': 1,\n",
    "    'actor_lr':.00005,\n",
    "    'critic_lr':.0000005, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "number_individuals = 50  # Replace with your actual number of individuals\n",
    "number_markers = 100     # Replace with your actual number of markers\n",
    "ploidy = 2              # Replace with your actual ploidy level\n",
    "batch_size = 5\n",
    "\n",
    "initial_population = create_fake_population(batch_size, number_individuals, number_markers, ploidy)\n",
    "genetic_map_df = create_fake_geneticmap(number_markers)\n",
    "marker_strength = np.array(genetic_map_df['Yield'])\n",
    "\n",
    "dummy_score = np.random.rand(batch_size, number_individuals)\n",
    "\n",
    "actor_model = create_actor_model(number_individuals, number_markers, ploidy)\n",
    "critic_model = create_critic_model(number_individuals, number_markers, ploidy, actor_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = actor_model([initial_population,dummy_score])\n",
    "value_estimate = critic_model([initial_population, dummy_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the population in half based on the policy metric\n",
    "top_policy_parents = select_parents(policy[batch_id,:].numpy(), total_parents=int(initial_population.shape[1] /2))[1].numpy()\n",
    "\n",
    "bottom_policy_parents = np.arange(initial_population.shape[1])\n",
    "bottom_policy_parents = np.setdiff1d(bottom_policy_parents, top_policy_parents)\n",
    "\n",
    "top_policy_parents = initial_population[batch_id][top_policy_parents]\n",
    "bottom_policy_parents = initial_population[batch_id][bottom_policy_parents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_scores = calculate_scores(initial_population[batch_id], marker_strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "476.58"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_scores(panmixia(bottom_policy_parents, 500), marker_strength).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478.2\n",
      "Indices of top K values: [41 17 24 36 39]\n",
      "Top K values: [578 569 557 551 534]\n"
     ]
    }
   ],
   "source": [
    "new_pop = panmixia(top_policy_parents, 50)\n",
    "new_scores = calculate_scores(new_pop, marker_strength)\n",
    "print(new_scores.mean())\n",
    "new_parents = scores2parents(new_scores,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562.32\n",
      "Indices of top K values: [33 24 41  5 38]\n",
      "Top K values: [654 641 621 607 606]\n"
     ]
    }
   ],
   "source": [
    "new_pop = panmixia(new_pop[new_parents], 50)\n",
    "new_scores = calculate_scores(new_pop, marker_strength)\n",
    "print(new_scores.mean())\n",
    "new_parents = scores2parents(new_scores,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Batch Size : 5\n",
      "      \n",
      "    Population Size : 50\n",
      "    Marker Count : 100\n",
      "\n",
      "    Population Input Shape : (5, 50, 100, 2)\n",
      "    Scores Input Shape : (5, 50)\n",
      "\n",
      "    Policy Output Shape : (5, 50)\n",
      "    Values Output Shape : (5, 1)\n",
      "  \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'''\n",
    "    Batch Size : {batch_size}\n",
    "      \n",
    "    Population Size : {number_individuals}\n",
    "    Marker Count : {number_markers}\n",
    "\n",
    "    Population Input Shape : {initial_population.shape}\n",
    "    Scores Input Shape : {dummy_score.shape}\n",
    "\n",
    "    Policy Output Shape : {policy.shape}\n",
    "    Values Output Shape : {value_estimate.shape}\n",
    "  \n",
    "    '''\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Concatenate, Lambda\n",
    "\n",
    "def create_actor_model(num_individuals, num_markers, ploidy):\n",
    "    # Define the input layers\n",
    "    population_input = Input(shape=(num_individuals, num_markers, ploidy), name='population_input')\n",
    "    scores_input = Input(shape=(num_individuals,), name='scores_input')\n",
    "\n",
    "    # Flatten the population input to process it with Dense layers\n",
    "    flat_population = Flatten()(population_input)\n",
    "\n",
    "    # Combine the flattened population and scores inputs\n",
    "    combined_input = Concatenate()([flat_population, scores_input])\n",
    "\n",
    "    # Define the hidden layers\n",
    "    hidden1 = Dense(128, activation='relu')(combined_input)\n",
    "    hidden2 = Dense(64, activation='relu')(hidden1)\n",
    "\n",
    "    # Output layer with one scalar value per individual\n",
    "    policy_output = Dense(num_individuals, activation='sigmoid', name='policy_output')(hidden2)\n",
    "\n",
    "    # Create the actor model\n",
    "    actor_model = Model(inputs=[population_input, scores_input], outputs=policy_output)\n",
    "\n",
    "    return actor_model\n",
    "\n",
    "def create_critic_model(num_individuals, num_markers, ploidy, actor_model):\n",
    "    # Define the input layers\n",
    "    population_input = Input(shape=(num_individuals, num_markers, ploidy), name='population_input')\n",
    "    scores_input = Input(shape=(num_individuals,), name='scores_input')\n",
    "\n",
    "    # Call the actor model to get the policy output for the given state\n",
    "    policy_output = actor_model([population_input, scores_input])\n",
    "\n",
    "    # Use Lambda layer to apply tf.stop_gradient to prevent backpropagation\n",
    "    policy_output_no_gradient = Lambda(lambda x: tf.stop_gradient(x))(policy_output)\n",
    "\n",
    "    # Flatten the population input to process it with Dense layers\n",
    "    flat_population = Flatten()(population_input)\n",
    "\n",
    "    # Combine the flattened population, scores, and policy outputs without gradients\n",
    "    combined_input = Concatenate()([flat_population, scores_input, Flatten()(policy_output_no_gradient)])\n",
    "\n",
    "    # Define the hidden layers\n",
    "    hidden1 = Dense(128, activation='relu')(combined_input)\n",
    "    hidden2 = Dense(64, activation='relu')(hidden1)\n",
    "\n",
    "    # Output layer with a single value estimate for the state\n",
    "    value_output = Dense(1, activation='linear', name='value_output')(hidden2)\n",
    "\n",
    "    # Create the critic model\n",
    "    critic_model = Model(inputs=[population_input, scores_input], outputs=value_output)\n",
    "\n",
    "    return critic_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BreederAgent:\n",
    "    \"\"\"\n",
    "    A PPO agent that learns to select parents for breeding programs using Actor-Critic method.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initializes the breeding program with a configuration dictionary.\n",
    "        \n",
    "        Parameters:\n",
    "        config (dict): Configuration dictionary containing all necessary parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the basic attributes from the config\n",
    "        self.compute_baselines = config.get('compute_baselines', False)\n",
    "        self.population_size = config['population_size']\n",
    "        self.marker_count = config['marker_count']\n",
    "        self.initial_population = config['initial_population']\n",
    "        self.genetic_map = config['genetic_map']\n",
    "        self.marker_strength = np.array(self.genetic_map['Yield'])\n",
    "\n",
    "        self.ploidy = config['ploidy']\n",
    "\n",
    "\n",
    "        # Initialize the Actor and Critic models\n",
    "        self.actor = create_actor_model(num_individuals=self.population_size, num_markers=self.marker_count, ploidy=self.ploidy)\n",
    "        self.critic = create_critic_model(num_individuals=self.population_size, num_markers=self.marker_count, ploidy=self.ploidy, actor_model=self.actor)\n",
    "\n",
    "\n",
    "        #truncation and random baselines\n",
    "        # self.truncation_baseline = [calculate_scores(x, self.marker_strength) for x in self.initial_population]\n",
    "        # truncation_baseline = self.breeding_simulator.select(self.initial_population, k = int(self.initial_population.shape[0]/2))\n",
    "        # self.truncation_baseline = self.breeding_simulator.GEBV(collapse_first_two_axes(self.breeding_simulator.random_crosses(truncation_baseline, n_crosses = truncation_baseline.shape[0], n_offspring=50))).mean()[0]\n",
    "\n",
    "        # self.random_baseline = self.breeding_simulator.GEBV(collapse_first_two_axes(self.breeding_simulator.random_crosses(self.initial_population, n_crosses = self.initial_population.shape[0], n_offspring=50))).mean()[0]\n",
    "\n",
    "\n",
    "    def reset_environment(self):\n",
    "        self.current_population = self.initial_population\n",
    "        self.current_scores = self.initial_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "number_individuals = 50  # Replace with your actual number of individuals\n",
    "number_markers = 100     # Replace with your actual number of markers\n",
    "ploidy = 2              # Replace with your actual ploidy level\n",
    "batch_size = 5\n",
    "\n",
    "initial_population = create_fake_population(batch_size, number_individuals, number_markers, ploidy)\n",
    "genetic_map_df = create_fake_geneticmap(number_markers)\n",
    "marker_strength = np.array(genetic_map_df['Yield'])\n",
    "\n",
    "\n",
    "config = {\n",
    "    'compute_baselines': False,\n",
    "    'initial_population': initial_population,\n",
    "    'genetic_map': genetic_map_df,\n",
    "    'population_size': number_individuals,\n",
    "    'marker_count': number_markers,\n",
    "    'heritability': .99,\n",
    "    'episodes': 10,\n",
    "    'cycles': 2,\n",
    "    'learning_rate': .001,\n",
    "    'replicates': 1,\n",
    "    'ploidy':2,\n",
    "}\n",
    "\n",
    "agent = BreederAgent(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_populations = [calculate_scores(x, agent.marker_strength) for x in agent.initial_population]\n",
    "selected_parents = [select_parents(x)[1].numpy() for x in scored_populations]\n",
    "subset_parents = [s[p] for s,p in zip(initial_population,selected_parents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 100, 2)"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_parents[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 1],\n",
       "        [1, 0],\n",
       "        [0, 0],\n",
       "        ...,\n",
       "        [0, 0],\n",
       "        [0, 1],\n",
       "        [0, 0]],\n",
       "\n",
       "       [[1, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        ...,\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1]],\n",
       "\n",
       "       [[1, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        ...,\n",
       "        [0, 0],\n",
       "        [1, 1],\n",
       "        [1, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        ...,\n",
       "        [1, 0],\n",
       "        [1, 1],\n",
       "        [1, 0]],\n",
       "\n",
       "       [[1, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        ...,\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0]],\n",
       "\n",
       "       [[0, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        ...,\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 0]]])"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panmixia(subset_parents[0], total_offspring=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.marker_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "agent.marker_strength.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
