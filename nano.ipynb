{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First value in Actor weights: tensor(-0.1295)\n",
      "First value in Critic weights: tensor(-0.2304)\n",
      "First value in Actor weights: tensor(-0.1195)\n",
      "First value in Critic weights: tensor(-0.2204)\n",
      "First value in Actor weights: tensor(-0.1211)\n",
      "First value in Critic weights: tensor(-0.2219)\n",
      "First value in Actor weights: tensor(-0.1276)\n",
      "First value in Critic weights: tensor(-0.2183)\n",
      "First value in Actor weights: tensor(-0.1326)\n",
      "First value in Critic weights: tensor(-0.2137)\n",
      "First value in Actor weights: tensor(-0.1355)\n",
      "First value in Critic weights: tensor(-0.2086)\n",
      "First value in Actor weights: tensor(-0.1405)\n",
      "First value in Critic weights: tensor(-0.2021)\n",
      "First value in Actor weights: tensor(-0.1453)\n",
      "First value in Critic weights: tensor(-0.1993)\n",
      "First value in Actor weights: tensor(-0.1486)\n",
      "First value in Critic weights: tensor(-0.1946)\n",
      "First value in Actor weights: tensor(-0.1509)\n",
      "First value in Critic weights: tensor(-0.1899)\n",
      "First value in Actor weights: tensor(-0.1525)\n",
      "First value in Critic weights: tensor(-0.1847)\n",
      "First value in Actor weights: tensor(-0.1552)\n",
      "First value in Critic weights: tensor(-0.1787)\n",
      "First value in Actor weights: tensor(-0.1570)\n",
      "First value in Critic weights: tensor(-0.1754)\n",
      "First value in Actor weights: tensor(-0.1589)\n",
      "First value in Critic weights: tensor(-0.1745)\n",
      "First value in Actor weights: tensor(-0.1606)\n",
      "First value in Critic weights: tensor(-0.1735)\n",
      "First value in Actor weights: tensor(-0.1623)\n",
      "First value in Critic weights: tensor(-0.1726)\n",
      "First value in Actor weights: tensor(-0.1640)\n",
      "First value in Critic weights: tensor(-0.1716)\n",
      "First value in Actor weights: tensor(-0.1654)\n",
      "First value in Critic weights: tensor(-0.1703)\n",
      "First value in Actor weights: tensor(-0.1667)\n",
      "First value in Critic weights: tensor(-0.1692)\n",
      "First value in Actor weights: tensor(-0.1674)\n",
      "First value in Critic weights: tensor(-0.1679)\n",
      "First value in Actor weights: tensor(-0.1672)\n",
      "First value in Critic weights: tensor(-0.1685)\n",
      "First value in Actor weights: tensor(-0.1663)\n",
      "First value in Critic weights: tensor(-0.1672)\n",
      "First value in Actor weights: tensor(-0.1656)\n",
      "First value in Critic weights: tensor(-0.1660)\n",
      "First value in Actor weights: tensor(-0.1649)\n",
      "First value in Critic weights: tensor(-0.1649)\n",
      "First value in Actor weights: tensor(-0.1640)\n",
      "First value in Critic weights: tensor(-0.1636)\n",
      "First value in Actor weights: tensor(-0.1624)\n",
      "First value in Critic weights: tensor(-0.1607)\n",
      "First value in Actor weights: tensor(-0.1609)\n",
      "First value in Critic weights: tensor(-0.1583)\n",
      "First value in Actor weights: tensor(-0.1590)\n",
      "First value in Critic weights: tensor(-0.1580)\n",
      "First value in Actor weights: tensor(-0.1569)\n",
      "First value in Critic weights: tensor(-0.1596)\n",
      "First value in Actor weights: tensor(-0.1542)\n",
      "First value in Critic weights: tensor(-0.1594)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\"\"\"\n",
    "a toy example of a actor-critic network.\n",
    "\n",
    "the actor has to pick the index with the highest value in a list of 10 random numbers.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, hs):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hs)\n",
    "        self.fc2 = nn.Linear(hs, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=-1)\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim, hs):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hs)\n",
    "        self.fc2 = nn.Linear(hs, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "input_dim = 10 # number of values the agent has to choose from\n",
    "hs = 5 # hidden layer size for both actor/critic\n",
    "lr = .01\n",
    "\n",
    "# Initialize the networks\n",
    "actor = Actor(input_dim=input_dim, hs=hs)\n",
    "critic = Critic(input_dim=input_dim, hs=hs)\n",
    "\n",
    "# Initialize the optimizers\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=lr)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=lr)\n",
    "\n",
    "# Initialize the average_rewards_per_step list\n",
    "average_rewards_per_step = []\n",
    "\n",
    "# Start the training loop\n",
    "for episode in range(1):  # Train for 1000 episodes\n",
    "    average_rewards = 0\n",
    "\n",
    "    inputs = torch.randn(input_dim)  # Generate a random list of scalars\n",
    "\n",
    "    # Perform 30 updates for the given policy\n",
    "    for _ in range(30):\n",
    "        # Pass the list through the Actor network\n",
    "        log_probs = actor(inputs.unsqueeze(0))\n",
    "\n",
    "        action = torch.multinomial(log_probs, 1)\n",
    "\n",
    "        # Calculate the reward\n",
    "        reward = inputs[action]\n",
    "\n",
    "        # Calculate the average reward per step * for monitoring only\n",
    "        average_rewards += reward.item() / 100\n",
    "\n",
    "        # Pass the list through the Critic network\n",
    "        value = critic(inputs.unsqueeze(0))\n",
    "\n",
    "        # Calculate the advantage\n",
    "        advantage = value - reward\n",
    "\n",
    "        # Calculate the losses\n",
    "        actor_loss = -log_probs[0][action] * advantage.detach()  # Detach the advantage tensor to prevent gradients from flowing into the critic network\n",
    "        critic_loss = advantage.pow(2)\n",
    "\n",
    "        # Print the weights before update for debugging ; they are changing after updates\n",
    "        print(\"First value in Actor weights:\", actor.fc1.weight.data[0][0])\n",
    "        print(\"First value in Critic weights:\", critic.fc1.weight.data[0][0])\n",
    "\n",
    "        # Backpropagate the losses and update the networks\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        actor_optimizer.step()\n",
    "\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_optimizer.step()\n",
    "\n",
    "\n",
    "    average_rewards_per_step.append(average_rewards)\n",
    "    if len(average_rewards_per_step) > 200:\n",
    "        average_rewards_per_step.pop(0)\n",
    "\n",
    "    # * Check for early stopping every 100 episodes, starting from the 101st episode\n",
    "    if episode >= 100 and episode % 50 == 0:\n",
    "        print(f\"Episode {episode} Average reward per step:{average_rewards}\")\n",
    "        if episode >= 200 and average_rewards <= average_rewards_per_step[0]:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9331,  0.2693,  0.1480,  0.2423, -0.2720, -0.5691, -1.4266,  1.7765,\n",
       "         0.7039,  0.3265])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
