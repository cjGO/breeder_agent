{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from chromax import Simulator\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from random import choice\n",
    "from typing import Dict, Any, List\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "\n",
    "def init_program():\n",
    "    number_individuals = 50  # Replace with your actual number of individuals\n",
    "    number_markers = 100     # Replace with your actual number of markers\n",
    "    ploidy = 2              # Replace with your actual ploidy level\n",
    "    batch_size = 3\n",
    "    initial_population = create_fake_population(batch_size, number_individuals, number_markers, ploidy)\n",
    "    #printshape('first pop shape' , initial_population)\n",
    "    genetic_map_df = create_fake_geneticmap(number_markers)\n",
    "    marker_strength = np.array(genetic_map_df['Yield'])\n",
    "    fraction_selection = 10\n",
    "\n",
    "    #score init pop\n",
    "    initial_score = np.array([calculate_scores(x,marker_strength) for x in initial_population])\n",
    "    #printshape('first score list shape' , initial_score)\n",
    "\n",
    "    #score2hist(initial_score)\n",
    "\n",
    "    return initial_population, initial_score, genetic_map_df, marker_strength\n",
    "\n",
    "def create_fake_geneticmap(number_markers:int):\n",
    "    \"\"\"The genetic map represents the rules of the game. It assigns the truth value to each marker.\n",
    "\n",
    "    :param number_markers: the total number of markers to include in this genetic map. must correspond to population shape\n",
    "    :type number_markers: int \n",
    "    \"\"\"\n",
    "    # 'chr' will always be '1A' for every marker\n",
    "    chr_array = ['1A'] * number_markers\n",
    "    \n",
    "    # 'yield': Create a marker_strength array with 1 float between -0.5 and +0.5 randomly\n",
    "    # yield_array = np.random.poisson(np.random.randint(1,10), size=number_markers)\n",
    "    poisson_values = np.random.poisson(np.random.randint(1, 10), size=number_markers)\n",
    "\n",
    "    # Scale the Poisson values to the range [0, 1]\n",
    "    scaled_poisson_values = poisson_values / np.max(poisson_values)\n",
    "\n",
    "    # Stretch and shift the values to the range [-1, 1]\n",
    "    yield_array = (scaled_poisson_values * 2) - 1\n",
    "    \n",
    "    # 'cM': create an array for number_markers length evenly sampled between 0 and 100\n",
    "    cM_array = np.linspace(0, 100, num=number_markers)\n",
    "    \n",
    "    # Create the DataFrame with the auto-generated data\n",
    "    df = pd.DataFrame({'CHR.PHYS': chr_array, 'Yield': yield_array, 'cM': cM_array*.01})\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_fake_population(total_pops:int, number_individuals:int, number_markers:int, ploidy:int):\n",
    "    \"\"\"\n",
    "        Creates a batch of populations. A populations is a group of individuals. An individual is composed of markers. Markers are 0 or 1\n",
    "\n",
    "        Returns : binary array shape (total_pops, number_individuals, number_markers, ploidy)\n",
    "    \"\"\"\n",
    "    # List of generation methods\n",
    "    generation_methods = [\n",
    "        lambda: np.random.randint(2, size=(number_individuals, number_markers, ploidy)),\n",
    "        lambda: np.random.choice([0, 1], size=(number_individuals, number_markers, ploidy))\n",
    "    ]\n",
    "    \n",
    "    populations = []\n",
    "    for _ in range(total_pops):\n",
    "        # Randomly select a generation method and generate the population\n",
    "        gen_method = choice(generation_methods)\n",
    "        population = gen_method()\n",
    "        populations.append(population)\n",
    "        \n",
    "    # Combine all populations into a single numpy array\n",
    "    combined_population = np.array(populations)\n",
    "\n",
    "    return combined_population\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_scores(population, marker_strength):\n",
    "    \"\"\"\n",
    "    Calculates the additive score by matrix multipling the population (n,m) with the marker strengths (m,)\n",
    "    \"\"\"\n",
    "    # Perform a dot product between the dosages and marker strength arrays\n",
    "    dosages = np.sum(population,axis=2)\n",
    "    scores = np.dot(dosages, marker_strength)\n",
    "    return scores\n",
    "\n",
    "def panmixia(selected_parents, total_offspring):\n",
    "    \"\"\" Handles the random crossing for us ; heuristic!\n",
    "\n",
    "    selected_parents ( n , m , d )\n",
    "    \n",
    "    \"\"\"\n",
    "    n, m, d = selected_parents.shape\n",
    "    offspring_target = total_offspring\n",
    "    offspring_list = []\n",
    "\n",
    "    while len(offspring_list) < offspring_target:\n",
    "        # Randomly pick two parents without replacement\n",
    "        parents_indices = np.random.choice(n, size=2, replace=False)\n",
    "        parent1, parent2 = selected_parents[parents_indices]\n",
    "\n",
    "        # Simulate random recombination for each marker\n",
    "        offspring = np.zeros((m, d), dtype=parent1.dtype)\n",
    "        for i in range(m):\n",
    "            # Randomly choose one allele from each parent for each marker\n",
    "            for j in range(d):\n",
    "                parent_allele = np.random.choice([parent1[i, j], parent2[i, j]])\n",
    "                offspring[i, j] = parent_allele\n",
    "\n",
    "        # Add the new offspring to the offspring list\n",
    "        offspring_list.append(offspring)\n",
    "\n",
    "    # Convert offspring list to a numpy array with shape (offspring_target, m, d)\n",
    "    offspring_array = np.array(offspring_list)\n",
    "    return offspring_array\n",
    "\n",
    "\n",
    "def scores2parents(scores,K):\n",
    "    \"\"\"\n",
    "    hint: use output from calculate_scores\n",
    "    \"\"\"\n",
    "    # Specify the number of top values you want (K)\n",
    "    K = 5\n",
    "\n",
    "    # Get the indices that would sort the array\n",
    "    sorted_indices = np.argsort(scores)\n",
    "\n",
    "    # Take the last K indices of the sorted indices array\n",
    "    top_k_indices = sorted_indices[-K:]\n",
    "\n",
    "    # Since argsort returns indices in ascending order, reverse to get the top values\n",
    "    top_k_indices = top_k_indices[::-1]\n",
    "\n",
    "    print(\"Indices of top K values:\", top_k_indices)\n",
    "    print(\"Top K values:\", scores[top_k_indices])\n",
    "\n",
    "    return top_k_indices\n",
    "\n",
    "\n",
    "def select_parents(policy, total_parents):\n",
    "    \"\"\"\n",
    "        input: Policy from actor ( score or metric value for each individual )\n",
    "        returns : index of parents to be included in random_crosses for next step of breeding program\n",
    "    \"\"\"\n",
    "    # Use numpy's argsort function to get the indices of the top 'total_parents' values\n",
    "    indices = np.argsort(policy)[-total_parents:]\n",
    "    values = policy[indices]\n",
    "    return values,indices\n",
    "\n",
    "def printshape(message , arr):\n",
    "    print(f'{message} {arr.shape}')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "def score2hist(score, title='population score distribution'):\n",
    "    #e.g. (3,50) array\n",
    "\n",
    "    # Assuming 'data' is your (3,50) array\n",
    "    data = score\n",
    "\n",
    "    # Plot KDE for each row in the data\n",
    "    for i in range(data.shape[0]):\n",
    "        kde = gaussian_kde(data[i])\n",
    "        dist_space = np.linspace(min(data[i]), max(data[i]), 100)\n",
    "        plt.plot(dist_space, kde(dist_space), label=f'Distribution {i+1}')\n",
    "\n",
    "    # Adding labels and legend\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(title)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "number_individuals = 50  # Replace with your actual number of individuals\n",
    "number_markers = 100     # Replace with your actual number of markers\n",
    "ploidy = 2              # Replace with your actual ploidy level\n",
    "batch_size = 3\n",
    "initial_population = create_fake_population(batch_size, number_individuals, number_markers, ploidy)\n",
    "genetic_map_df = create_fake_geneticmap(number_markers)\n",
    "marker_strength = np.array(genetic_map_df['Yield'])\n",
    "fraction_selection = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def init_program(    number_individuals = 50,\n",
    "    number_markers = 100,\n",
    "    ploidy = 2,\n",
    "    batch_size = 3):\n",
    "\n",
    "    initial_population = create_fake_population(batch_size, number_individuals, number_markers, ploidy)\n",
    "    genetic_map_df = create_fake_geneticmap(number_markers)\n",
    "    marker_strength = np.array(genetic_map_df['Yield'])\n",
    "    fraction_selection = 10\n",
    "\n",
    "    # Convert marker_strength to a PyTorch tensor\n",
    "    marker_strength_tensor = torch.tensor(marker_strength, dtype=torch.float32)\n",
    "\n",
    "    # Score the initial population\n",
    "    initial_score = np.array([calculate_scores(torch.tensor(x, dtype=torch.float32), marker_strength_tensor).numpy() for x in initial_population])\n",
    "\n",
    "    return initial_population, initial_score, genetic_map_df, marker_strength\n",
    "\n",
    "# Make sure the calculate_scores function is defined correctly as shown in the previous response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #random selection baseline\n",
    "\n",
    "# initial_population, initial_score, genetic_map_df, marker_strength = init_program()\n",
    "\n",
    "# current_population = initial_population\n",
    "# current_score = initial_score\n",
    "\n",
    "# for _ in range(5):\n",
    "#     #select random half of individuals in each pop\n",
    "#     selected_parents = current_population[:,:25,:,:]\n",
    "#     selected_parents_score = np.array([calculate_scores(x, marker_strength) for x in selected_parents])\n",
    "#     new_pop = np.array([panmixia(x, total_offspring=initial_population.shape[1]) for x in selected_parents])\n",
    "#     new_score = np.array([calculate_scores(x, marker_strength) for x in new_pop])\n",
    "    \n",
    "#     current_population = new_pop\n",
    "#     current_score = new_score\n",
    "#     score2hist(selected_parents_score, title='selected parents scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #truncation selection baseline\n",
    "\n",
    "# initial_population, initial_score, genetic_map_df, marker_strength = init_program()\n",
    "\n",
    "# current_population = initial_population\n",
    "# current_score = initial_score\n",
    "\n",
    "# frames = []\n",
    "# for _ in range(5):\n",
    "#     #select top half of individuals in each pop\n",
    "#     truncation = [select_parents(x,int(initial_population.shape[1]/2)) for x in current_score]\n",
    "#     parent_index = [x[1] for x in truncation]\n",
    "#     parent_metric = [x[0] for x in truncation]\n",
    "#     parent_pop = np.array([x[y] for x,y in zip(current_population, parent_index)])    \n",
    "#     new_pop = np.array([panmixia(x, total_offspring=initial_population.shape[1]) for x in parent_pop])\n",
    "#     new_score = np.array([calculate_scores(x, marker_strength) for x in new_pop])\n",
    "    \n",
    "#     current_population = new_pop\n",
    "#     current_score = new_score\n",
    "#     score2hist(current_score)\n",
    "#     frames.append(current_score)\n",
    "\n",
    "# frames = np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import kaiming_normal_\n",
    "\n",
    "class ActorModel(nn.Module):\n",
    "    def __init__(self, num_individual, num_markers, ploidy, use_skip_connection=False):\n",
    "        super(ActorModel, self).__init__()\n",
    "        \n",
    "        self.use_skip_connection = use_skip_connection\n",
    "        \n",
    "        # Flatten layers for population and phenotype inputs\n",
    "        self.flatten_population = nn.Flatten()\n",
    "        self.flatten_phenotype = nn.Flatten()\n",
    "        \n",
    "        # Combined input size calculation\n",
    "        combined_input_size = num_individual * num_markers * ploidy + num_individual\n",
    "        \n",
    "        # Define the hidden layers\n",
    "        self.hidden1 = nn.Linear(combined_input_size, 64)\n",
    "        self.hidden2 = nn.Linear(64, 64)\n",
    "        \n",
    "        # Define the output layer\n",
    "        self.output = nn.Linear(64, num_individual)\n",
    "        \n",
    "        # Define the transformation layer for skip connection\n",
    "        if use_skip_connection:\n",
    "            self.phenotype_transform = nn.Linear(num_individual, 64)\n",
    "        \n",
    "        # Initialize weights using He initialization\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.01)\n",
    "    \n",
    "    def forward(self, population_input, phenotype_input):\n",
    "        # Flatten inputs\n",
    "        flat_population = self.flatten_population(population_input)\n",
    "        flat_phenotype = self.flatten_phenotype(phenotype_input)\n",
    "        \n",
    "        # Combine inputs\n",
    "        combined = torch.cat([flat_population, flat_phenotype], dim=1)\n",
    "        \n",
    "        # Hidden layers with ReLU activation\n",
    "        hidden1 = F.relu(self.hidden1(combined))\n",
    "        hidden2 = F.relu(self.hidden2(hidden1))\n",
    "        \n",
    "        # Check if skip connection is to be used\n",
    "        if self.use_skip_connection:\n",
    "            phenotype_transformed = F.relu(self.phenotype_transform(flat_phenotype))\n",
    "            skip_connection = hidden2 + phenotype_transformed\n",
    "            output = F.softmax(self.output(skip_connection), dim=1)\n",
    "        else:\n",
    "            output = F.softmax(self.output(hidden2), dim=1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "def calculate_scores(offspring, marker_tensor):\n",
    "    # Sum the contributions from both sets of chromosomes (ploidy)\n",
    "    summed_offspring = offspring.sum(dim=-1)  # This will sum across the ploidy dimension\n",
    "    \n",
    "    # Perform the dot product between the marker_tensor and the summed_offspring\n",
    "    # We need to unsqueeze the marker_tensor to perform batch matrix multiplication\n",
    "    marker_tensor_unsqueezed = marker_tensor.unsqueeze(0).unsqueeze(-1)\n",
    "    \n",
    "    # Perform batch matrix multiplication\n",
    "    scores = torch.matmul(summed_offspring, marker_tensor_unsqueezed)\n",
    "    \n",
    "    # Remove the last dimension and squeeze the result to get the final scores\n",
    "    scores = scores.squeeze(-1)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "def crossover(parents, num_offspring_per_population):\n",
    "    num_populations, num_parents, num_markers, ploidy = parents.shape\n",
    "    offspring = torch.empty((num_populations, num_offspring_per_population, num_markers, ploidy), dtype=torch.float32)\n",
    "\n",
    "    for i in range(num_populations):\n",
    "        for j in range(num_offspring_per_population):\n",
    "            # Randomly select two parents\n",
    "            parent_indices = torch.randperm(num_parents)[:2]\n",
    "            parent1 = parents[i, parent_indices[0]]\n",
    "            parent2 = parents[i, parent_indices[1]]\n",
    "\n",
    "            # Perform single crossover\n",
    "            crossover_point = torch.randint(low=1, high=num_markers, size=(1,)).item()\n",
    "            child1 = torch.cat((parent1[:crossover_point, :], parent2[crossover_point:, :]), dim=0)\n",
    "            child2 = torch.cat((parent2[:crossover_point, :], parent1[crossover_point:, :]), dim=0)\n",
    "\n",
    "            # Randomly select one of the two possible offspring\n",
    "            offspring[i, j] = child1 if torch.rand(1).item() < 0.5 else child2\n",
    "\n",
    "    return offspring\n",
    "\n",
    "# init the RL environment\n",
    "initial_population, initial_score, genetic_map_df, marker_strength = init_program()\n",
    "\n",
    "# init the Agent\n",
    "a = ActorModel(num_individual=initial_population.shape[1], num_markers = initial_population.shape[2], ploidy = initial_population.shape[3])\n",
    "\n",
    "# prep for torch\n",
    "population_tensor = torch.from_numpy(initial_population).float()\n",
    "score_tensor = torch.from_numpy(initial_score).float()\n",
    "score_tensor = score_tensor[:,0,:]\n",
    "marker_tensor = torch.from_numpy(marker_strength).float()\n",
    "\n",
    "# Perform the prediction\n",
    "with torch.no_grad():\n",
    "    prediction = a(population_tensor, score_tensor)\n",
    "\n",
    "# Sample 25 individuals from each population\n",
    "num_samples = 25\n",
    "samples = torch.multinomial(prediction, num_samples, replacement=True)\n",
    "\n",
    "# Create a tensor for the batch dimension which corresponds to each population\n",
    "batch_indices = torch.arange(population_tensor.size(0)).unsqueeze(1).expand_as(samples)\n",
    "\n",
    "# Use the batch_indices and samples to index into the initial_population tensor\n",
    "selected_parents = population_tensor[batch_indices, samples]\n",
    "\n",
    "# return: 50 offspring per population\n",
    "offspring_geno = crossover(selected_parents, 50)\n",
    "offspring_score = calculate_scores(offspring_geno, marker_tensor)\n",
    "\n",
    "# Calculate the mean of each array along axis 1\n",
    "offspring_score_mean = offspring_score.mean(dim=1, keepdim=True)\n",
    "score_tensor_mean = score_tensor.mean(dim=1, keepdim=True)\n",
    "\n",
    "# Subtract the offspring_score mean from the score_tensor mean\n",
    "loss = offspring_score_mean - score_tensor_mean # WANT TO MAXIMIZE THIS !!! optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import kaiming_normal_\n",
    "\n",
    "class ActorModel(nn.Module):\n",
    "    def __init__(self, num_individual, num_markers, ploidy, use_skip_connection=False):\n",
    "        super(ActorModel, self).__init__()\n",
    "        \n",
    "        self.use_skip_connection = use_skip_connection\n",
    "        \n",
    "        # Flatten layers for population and phenotype inputs\n",
    "        self.flatten_population = nn.Flatten()\n",
    "        self.flatten_phenotype = nn.Flatten()\n",
    "        \n",
    "        # Combined input size calculation\n",
    "        combined_input_size = num_individual * num_markers * ploidy + num_individual\n",
    "        \n",
    "        # Define the hidden layers\n",
    "        self.hidden1 = nn.Linear(combined_input_size, 64)\n",
    "        self.hidden2 = nn.Linear(64, 64)\n",
    "        \n",
    "        # Define the output layer\n",
    "        self.output = nn.Linear(64, num_individual)\n",
    "        \n",
    "        # Define the transformation layer for skip connection\n",
    "        if use_skip_connection:\n",
    "            self.phenotype_transform = nn.Linear(num_individual, 64)\n",
    "        \n",
    "        # Initialize weights using He initialization\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # Create a tensor with the same shape as the weight tensor,\n",
    "                # filled with -1 or 1 randomly\n",
    "                weight_shape = m.weight.size()\n",
    "                random_weights = torch.randint(low=1, high=2, size=weight_shape).float() * 2 - 1\n",
    "                m.weight.data = random_weights\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.01)\n",
    "    \n",
    "    def forward(self, population_input, phenotype_input):\n",
    "        # Flatten inputs\n",
    "        flat_population = self.flatten_population(population_input)\n",
    "        flat_phenotype = self.flatten_phenotype(phenotype_input)\n",
    "        \n",
    "        # Combine inputs\n",
    "        combined = torch.cat([flat_population, flat_phenotype], dim=1)\n",
    "        \n",
    "        # Hidden layers with ReLU activation\n",
    "        hidden1 = F.relu(self.hidden1(combined))\n",
    "        hidden2 = F.relu(self.hidden2(hidden1))\n",
    "        \n",
    "        # Check if skip connection is to be used\n",
    "        if self.use_skip_connection:\n",
    "            phenotype_transformed = F.relu(self.phenotype_transform(flat_phenotype))\n",
    "            skip_connection = hidden2 + phenotype_transformed\n",
    "            output = F.softmax(self.output(skip_connection), dim=1)\n",
    "        else:\n",
    "            output = F.softmax(self.output(hidden2), dim=1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "def calculate_scores(offspring, marker_tensor):\n",
    "    # Sum the contributions from both sets of chromosomes (ploidy)\n",
    "    summed_offspring = offspring.sum(dim=-1)  # This will sum across the ploidy dimension\n",
    "    \n",
    "    # Perform the dot product between the marker_tensor and the summed_offspring\n",
    "    # We need to unsqueeze the marker_tensor to perform batch matrix multiplication\n",
    "    marker_tensor_unsqueezed = marker_tensor.unsqueeze(0).unsqueeze(-1)\n",
    "    \n",
    "    # Perform batch matrix multiplication\n",
    "    scores = torch.matmul(summed_offspring, marker_tensor_unsqueezed)\n",
    "    \n",
    "    # Remove the last dimension and squeeze the result to get the final scores\n",
    "    scores = scores.squeeze(-1)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "def crossover(parents, num_offspring_per_population):\n",
    "    num_populations, num_parents, num_markers, ploidy = parents.shape\n",
    "    offspring = torch.empty((num_populations, num_offspring_per_population, num_markers, ploidy), dtype=torch.float32)\n",
    "\n",
    "    for i in range(num_populations):\n",
    "        for j in range(num_offspring_per_population):\n",
    "            # Randomly select two parents\n",
    "            parent_indices = torch.randperm(num_parents)[:2]\n",
    "            parent1 = parents[i, parent_indices[0]]\n",
    "            parent2 = parents[i, parent_indices[1]]\n",
    "\n",
    "            # Perform single crossover\n",
    "            crossover_point = torch.randint(low=1, high=num_markers, size=(1,)).item()\n",
    "            child1 = torch.cat((parent1[:crossover_point, :], parent2[crossover_point:, :]), dim=0)\n",
    "            child2 = torch.cat((parent2[:crossover_point, :], parent1[crossover_point:, :]), dim=0)\n",
    "\n",
    "            # Randomly select one of the two possible offspring\n",
    "            offspring[i, j] = child1 if torch.rand(1).item() < 0.5 else child2\n",
    "\n",
    "    return offspring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the RL environment\n",
    "initial_population, initial_score, genetic_map_df, marker_strength = init_program(number_markers=10)\n",
    "\n",
    "# init the Agent\n",
    "a = ActorModel(num_individual=initial_population.shape[1], num_markers = initial_population.shape[2], ploidy = initial_population.shape[3], use_skip_connection=False)\n",
    "a.train()\n",
    "\n",
    "# prep for torch\n",
    "population_tensor = torch.from_numpy(initial_population).float().requires_grad_(True)\n",
    "score_tensor = torch.from_numpy(initial_score).float().requires_grad_(True)\n",
    "score_tensor = score_tensor[:, 0, :].requires_grad_(True)\n",
    "marker_tensor = torch.from_numpy(marker_strength).float().requires_grad_(True)\n",
    "\n",
    "optimizer = optim.Adam(a.parameters(), lr=0.0001)\n",
    "\n",
    "losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "\n",
    "def init_program():\n",
    "    number_individuals = 50  # Replace with your actual number of individuals\n",
    "    number_markers = 100     # Replace with your actual number of markers\n",
    "    ploidy = 2              # Replace with your actual ploidy level\n",
    "    batch_size = 3\n",
    "    initial_population = create_fake_population(batch_size, number_individuals, number_markers, ploidy)\n",
    "    #printshape('first pop shape' , initial_population)\n",
    "    genetic_map_df = create_fake_geneticmap(number_markers)\n",
    "    marker_strength = np.array(genetic_map_df['Yield'])\n",
    "    fraction_selection = 10\n",
    "\n",
    "    #score init pop\n",
    "    initial_score = np.array([calculate_scores(x,marker_strength) for x in initial_population])\n",
    "    #printshape('first score list shape' , initial_score)\n",
    "\n",
    "    #score2hist(initial_score)\n",
    "\n",
    "    return initial_population, initial_score, genetic_map_df, marker_strength\n",
    "\n",
    "def create_fake_geneticmap(number_markers:int):\n",
    "    \"\"\"The genetic map represents the rules of the game. It assigns the truth value to each marker.\n",
    "\n",
    "    :param number_markers: the total number of markers to include in this genetic map. must correspond to population shape\n",
    "    :type number_markers: int \n",
    "    \"\"\"\n",
    "    # 'chr' will always be '1A' for every marker\n",
    "    chr_array = ['1A'] * number_markers\n",
    "    \n",
    "    # 'yield': Create a marker_strength array with 1 float between -0.5 and +0.5 randomly\n",
    "    # yield_array = np.random.poisson(np.random.randint(1,10), size=number_markers)\n",
    "    poisson_values = np.random.poisson(np.random.randint(1, 10), size=number_markers)\n",
    "\n",
    "    # Scale the Poisson values to the range [0, 1]\n",
    "    scaled_poisson_values = poisson_values / np.max(poisson_values)\n",
    "\n",
    "    # Stretch and shift the values to the range [-1, 1]\n",
    "    yield_array = (scaled_poisson_values * 2) - 1\n",
    "    \n",
    "    # 'cM': create an array for number_markers length evenly sampled between 0 and 100\n",
    "    cM_array = np.linspace(0, 100, num=number_markers)\n",
    "    \n",
    "    # Create the DataFrame with the auto-generated data\n",
    "    df = pd.DataFrame({'CHR.PHYS': chr_array, 'Yield': yield_array, 'cM': cM_array*.01})\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_fake_population(total_pops:int, number_individuals:int, number_markers:int, ploidy:int):\n",
    "    \"\"\"\n",
    "        Creates a batch of populations. A populations is a group of individuals. An individual is composed of markers. Markers are 0 or 1\n",
    "\n",
    "        Returns : binary array shape (total_pops, number_individuals, number_markers, ploidy)\n",
    "    \"\"\"\n",
    "    # List of generation methods\n",
    "    generation_methods = [\n",
    "        lambda: np.random.randint(2, size=(number_individuals, number_markers, ploidy)),\n",
    "        lambda: np.random.choice([0, 1], size=(number_individuals, number_markers, ploidy))\n",
    "    ]\n",
    "    \n",
    "    populations = []\n",
    "    for _ in range(total_pops):\n",
    "        # Randomly select a generation method and generate the population\n",
    "        gen_method = choice(generation_methods)\n",
    "        population = gen_method()\n",
    "        populations.append(population)\n",
    "        \n",
    "    # Combine all populations into a single numpy array\n",
    "    combined_population = np.array(populations)\n",
    "\n",
    "    return combined_population\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_scores(population, marker_strength):\n",
    "    \"\"\"\n",
    "    Calculates the additive score by matrix multipling the population (n,m) with the marker strengths (m,)\n",
    "    \"\"\"\n",
    "    # Perform a dot product between the dosages and marker strength arrays\n",
    "    dosages = np.sum(population,axis=2)\n",
    "    scores = np.dot(dosages, marker_strength)\n",
    "    return scores\n",
    "\n",
    "def panmixia(selected_parents, total_offspring):\n",
    "    \"\"\" Handles the random crossing for us ; heuristic!\n",
    "\n",
    "    selected_parents ( n , m , d )\n",
    "    \n",
    "    \"\"\"\n",
    "    n, m, d = selected_parents.shape\n",
    "    offspring_target = total_offspring\n",
    "    offspring_list = []\n",
    "\n",
    "    while len(offspring_list) < offspring_target:\n",
    "        # Randomly pick two parents without replacement\n",
    "        parents_indices = np.random.choice(n, size=2, replace=False)\n",
    "        parent1, parent2 = selected_parents[parents_indices]\n",
    "\n",
    "        # Simulate random recombination for each marker\n",
    "        offspring = np.zeros((m, d), dtype=parent1.dtype)\n",
    "        for i in range(m):\n",
    "            # Randomly choose one allele from each parent for each marker\n",
    "            for j in range(d):\n",
    "                parent_allele = np.random.choice([parent1[i, j], parent2[i, j]])\n",
    "                offspring[i, j] = parent_allele\n",
    "\n",
    "        # Add the new offspring to the offspring list\n",
    "        offspring_list.append(offspring)\n",
    "\n",
    "    # Convert offspring list to a numpy array with shape (offspring_target, m, d)\n",
    "    offspring_array = np.array(offspring_list)\n",
    "    return offspring_array\n",
    "\n",
    "\n",
    "def scores2parents(scores,K):\n",
    "    \"\"\"\n",
    "    hint: use output from calculate_scores\n",
    "    \"\"\"\n",
    "    # Specify the number of top values you want (K)\n",
    "    K = 5\n",
    "\n",
    "    # Get the indices that would sort the array\n",
    "    sorted_indices = np.argsort(scores)\n",
    "\n",
    "    # Take the last K indices of the sorted indices array\n",
    "    top_k_indices = sorted_indices[-K:]\n",
    "\n",
    "    # Since argsort returns indices in ascending order, reverse to get the top values\n",
    "    top_k_indices = top_k_indices[::-1]\n",
    "\n",
    "    print(\"Indices of top K values:\", top_k_indices)\n",
    "    print(\"Top K values:\", scores[top_k_indices])\n",
    "\n",
    "    return top_k_indices\n",
    "\n",
    "\n",
    "def select_parents(policy, total_parents):\n",
    "    \"\"\"\n",
    "        input: Policy from actor ( score or metric value for each individual )\n",
    "        returns : index of parents to be included in random_crosses for next step of breeding program\n",
    "    \"\"\"\n",
    "    # Use numpy's argsort function to get the indices of the top 'total_parents' values\n",
    "    indices = np.argsort(policy)[-total_parents:]\n",
    "    values = policy[indices]\n",
    "    return values,indices\n",
    "\n",
    "def printshape(message , arr):\n",
    "    print(f'{message} {arr.shape}')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "def score2hist(score, title='population score distribution'):\n",
    "    #e.g. (3,50) array\n",
    "\n",
    "    # Assuming 'data' is your (3,50) array\n",
    "    data = score\n",
    "\n",
    "    # Plot KDE for each row in the data\n",
    "    for i in range(data.shape[0]):\n",
    "        kde = gaussian_kde(data[i])\n",
    "        dist_space = np.linspace(min(data[i]), max(data[i]), 100)\n",
    "        plt.plot(dist_space, kde(dist_space), label=f'Distribution {i+1}')\n",
    "\n",
    "    # Adding labels and legend\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(title)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "number_individuals = 50  # Replace with your actual number of individuals\n",
    "number_markers = 100     # Replace with your actual number of markers\n",
    "ploidy = 2              # Replace with your actual ploidy level\n",
    "batch_size = 3\n",
    "initial_population = create_fake_population(batch_size, number_individuals, number_markers, ploidy)\n",
    "genetic_map_df = create_fake_geneticmap(number_markers)\n",
    "marker_strength = np.array(genetic_map_df['Yield'])\n",
    "fraction_selection = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def init_program():\n",
    "    number_individuals = 50  # Replace with your actual number of individuals\n",
    "    number_markers = 100     # Replace with your actual number of markers\n",
    "    ploidy = 2               # Replace with your actual ploidy level\n",
    "    batch_size = 3\n",
    "    initial_population = create_fake_population(batch_size, number_individuals, number_markers, ploidy)\n",
    "    genetic_map_df = create_fake_geneticmap(number_markers)\n",
    "    marker_strength = np.array(genetic_map_df['Yield'])\n",
    "    fraction_selection = 10\n",
    "\n",
    "    # Convert marker_strength to a PyTorch tensor\n",
    "    marker_strength_tensor = torch.tensor(marker_strength, dtype=torch.float32)\n",
    "\n",
    "    # Score the initial population\n",
    "    initial_score = np.array([calculate_scores(torch.tensor(x, dtype=torch.float32), marker_strength_tensor).numpy() for x in initial_population])\n",
    "\n",
    "    return initial_population, initial_score, genetic_map_df, marker_strength\n",
    "\n",
    "# Make sure the calculate_scores function is defined correctly as shown in the previous response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #random selection baseline\n",
    "\n",
    "# initial_population, initial_score, genetic_map_df, marker_strength = init_program()\n",
    "\n",
    "# current_population = initial_population\n",
    "# current_score = initial_score\n",
    "\n",
    "# for _ in range(5):\n",
    "#     #select random half of individuals in each pop\n",
    "#     selected_parents = current_population[:,:25,:,:]\n",
    "#     selected_parents_score = np.array([calculate_scores(x, marker_strength) for x in selected_parents])\n",
    "#     new_pop = np.array([panmixia(x, total_offspring=initial_population.shape[1]) for x in selected_parents])\n",
    "#     new_score = np.array([calculate_scores(x, marker_strength) for x in new_pop])\n",
    "    \n",
    "#     current_population = new_pop\n",
    "#     current_score = new_score\n",
    "#     score2hist(selected_parents_score, title='selected parents scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #truncation selection baseline\n",
    "\n",
    "# initial_population, initial_score, genetic_map_df, marker_strength = init_program()\n",
    "\n",
    "# current_population = initial_population\n",
    "# current_score = initial_score\n",
    "\n",
    "# frames = []\n",
    "# for _ in range(5):\n",
    "#     #select top half of individuals in each pop\n",
    "#     truncation = [select_parents(x,int(initial_population.shape[1]/2)) for x in current_score]\n",
    "#     parent_index = [x[1] for x in truncation]\n",
    "#     parent_metric = [x[0] for x in truncation]\n",
    "#     parent_pop = np.array([x[y] for x,y in zip(current_population, parent_index)])    \n",
    "#     new_pop = np.array([panmixia(x, total_offspring=initial_population.shape[1]) for x in parent_pop])\n",
    "#     new_score = np.array([calculate_scores(x, marker_strength) for x in new_pop])\n",
    "    \n",
    "#     current_population = new_pop\n",
    "#     current_score = new_score\n",
    "#     score2hist(current_score)\n",
    "#     frames.append(current_score)\n",
    "\n",
    "# frames = np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 50, 100, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_population.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (out=NoneType, axis=int, ), but expected one of:\n * (*, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: out, axis\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m initial_population, initial_score, genetic_map_df, marker_strength \u001b[38;5;241m=\u001b[39m \u001b[43minit_program\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36minit_program\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m marker_strength_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(marker_strength, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Score the initial population\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m initial_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([calculate_scores(torch\u001b[38;5;241m.\u001b[39mtensor(x, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32), marker_strength_tensor)\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m initial_population])\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m initial_population, initial_score, genetic_map_df, marker_strength\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m marker_strength_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(marker_strength, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Score the initial population\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m initial_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mcalculate_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarker_strength_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m initial_population])\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m initial_population, initial_score, genetic_map_df, marker_strength\n",
      "Cell \u001b[0;32mIn[10], line 81\u001b[0m, in \u001b[0;36mcalculate_scores\u001b[0;34m(population, marker_strength)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03mCalculates the additive score by matrix multipling the population (n,m) with the marker strengths (m,)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Perform a dot product between the dosages and marker strength arrays\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m dosages \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(dosages, marker_strength)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/breeder_agent/.venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2324\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2321\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2325\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/breeder_agent/.venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:84\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (out=NoneType, axis=int, ), but expected one of:\n * (*, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: out, axis\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "initial_population, initial_score, genetic_map_df, marker_strength = init_program()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (out=NoneType, axis=int, ), but expected one of:\n * (*, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: out, axis\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# init the RL environment\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m initial_population, initial_score, genetic_map_df, marker_strength \u001b[38;5;241m=\u001b[39m \u001b[43minit_program\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# init the Agent\u001b[39;00m\n\u001b[1;32m      5\u001b[0m a \u001b[38;5;241m=\u001b[39m ActorModel(num_individual\u001b[38;5;241m=\u001b[39minitial_population\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], num_markers \u001b[38;5;241m=\u001b[39m initial_population\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], ploidy \u001b[38;5;241m=\u001b[39m initial_population\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m])\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36minit_program\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m marker_strength_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(marker_strength, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Score the initial population\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m initial_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([calculate_scores(torch\u001b[38;5;241m.\u001b[39mtensor(x, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32), marker_strength_tensor)\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m initial_population])\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m initial_population, initial_score, genetic_map_df, marker_strength\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m marker_strength_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(marker_strength, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Score the initial population\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m initial_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mcalculate_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarker_strength_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m initial_population])\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m initial_population, initial_score, genetic_map_df, marker_strength\n",
      "Cell \u001b[0;32mIn[10], line 81\u001b[0m, in \u001b[0;36mcalculate_scores\u001b[0;34m(population, marker_strength)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03mCalculates the additive score by matrix multipling the population (n,m) with the marker strengths (m,)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Perform a dot product between the dosages and marker strength arrays\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m dosages \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(dosages, marker_strength)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/breeder_agent/.venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2324\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2321\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2325\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/breeder_agent/.venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:84\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (out=NoneType, axis=int, ), but expected one of:\n * (*, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: out, axis\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "# init the RL environment\n",
    "initial_population, initial_score, genetic_map_df, marker_strength = init_program()\n",
    "\n",
    "# init the Agent\n",
    "a = ActorModel(num_individual=initial_population.shape[1], num_markers = initial_population.shape[2], ploidy = initial_population.shape[3])\n",
    "\n",
    "# prep for torch\n",
    "population_tensor = torch.from_numpy(initial_population).float()\n",
    "score_tensor = torch.from_numpy(initial_score).float()\n",
    "score_tensor = score_tensor[:,0,:]\n",
    "marker_tensor = torch.from_numpy(marker_strength).float()\n",
    "\n",
    "# Perform the prediction\n",
    "with torch.no_grad():\n",
    "    prediction = a(population_tensor, score_tensor)\n",
    "\n",
    "# Sample 25 individuals from each population\n",
    "num_samples = 25\n",
    "samples = torch.multinomial(prediction, num_samples, replacement=True)\n",
    "\n",
    "# Create a tensor for the batch dimension which corresponds to each population\n",
    "batch_indices = torch.arange(population_tensor.size(0)).unsqueeze(1).expand_as(samples)\n",
    "\n",
    "# Use the batch_indices and samples to index into the initial_population tensor\n",
    "selected_parents = population_tensor[batch_indices, samples]\n",
    "\n",
    "# return: 50 offspring per population\n",
    "offspring_geno = crossover(selected_parents, 50)\n",
    "offspring_score = calculate_scores(offspring_geno, marker_tensor)\n",
    "\n",
    "# Calculate the mean of each array along axis 1\n",
    "offspring_score_mean = offspring_score.mean(dim=1, keepdim=True)\n",
    "score_tensor_mean = score_tensor.mean(dim=1, keepdim=True)\n",
    "\n",
    "# Subtract the offspring_score mean from the score_tensor mean\n",
    "loss = offspring_score_mean - score_tensor_mean # WANT TO MAXIMIZE THIS !!! optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'offspring' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moffspring\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'offspring' is not defined"
     ]
    }
   ],
   "source": [
    "offspring.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m sampled_indices \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, population_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Gather the parents using the sampled indices\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# The result will have the same last dimension as the population tensor\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m selected_parents \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampled_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# selected_parents now contains the 25 parents from each population\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "# Assuming samples is the tensor containing the sampled indices with shape [3, 25]\n",
    "# and initial_population is the tensor containing the population data\n",
    "\n",
    "# Convert initial_population to a PyTorch tensor if it's not already\n",
    "population_tensor = torch.tensor(initial_population, dtype=torch.float32)\n",
    "\n",
    "# Use the sampled indices to gather the parents from the population tensor\n",
    "# We need to add an extra dimension for the gather operation to work correctly\n",
    "sampled_indices = samples.unsqueeze(-1).expand(-1, -1, population_tensor.size(-1))\n",
    "\n",
    "# Gather the parents using the sampled indices\n",
    "# The result will have the same last dimension as the population tensor\n",
    "selected_parents = torch.gather(population_tensor, 1, sampled_indices)\n",
    "\n",
    "# selected_parents now contains the 25 parents from each population\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 25, 100, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_parents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-15.0636],\n",
       "        [-17.7000],\n",
       "        [-17.0200]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offspring_score.mean(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2500,  2.0000,  1.7500,  2.2500, -0.7500,  1.0000,  1.5000, -0.2500,\n",
       "          1.7500, -1.0000, -0.5000,  0.2500, -1.2500,  0.7500,  2.2500,  1.5000,\n",
       "         -2.2500,  0.7500,  0.2500,  0.0000, -1.2500,  2.2500, -0.2500,  1.7500,\n",
       "          0.2500, -2.7500, -1.0000,  1.0000, -0.2500, -1.7500, -1.2500, -1.7500,\n",
       "         -1.2500, -2.0000,  1.2500, -0.5000,  0.5000,  1.5000, -1.7500, -0.7500,\n",
       "         -0.5000, -1.5000,  1.2500, -2.7500, -2.2500,  1.2500, -1.7500,  1.0000,\n",
       "         -1.0000,  1.5000],\n",
       "        [ 0.5000,  1.2500, -0.2500, -0.7500,  1.5000, -2.0000,  2.7500, -1.0000,\n",
       "         -1.2500, -2.7500, -1.2500, -1.2500, -0.5000, -0.5000, -1.2500, -0.2500,\n",
       "          0.2500,  0.7500, -1.0000, -1.0000, -0.7500,  1.5000,  0.0000,  1.2500,\n",
       "         -1.7500, -0.5000, -1.5000,  1.2500,  0.0000, -0.7500,  0.7500,  1.7500,\n",
       "          1.5000, -0.2500, -1.2500,  0.5000, -1.2500,  1.7500,  1.2500,  1.2500,\n",
       "         -2.0000, -0.2500,  1.0000, -1.7500, -1.2500,  0.2500,  1.2500, -2.0000,\n",
       "          0.0000, -0.2500],\n",
       "        [ 0.2500,  0.7500, -2.0000,  1.5000, -2.0000,  1.5000, -2.5000, -0.5000,\n",
       "          0.2500, -3.0000,  1.7500,  1.5000, -2.2500, -1.0000,  1.0000,  0.7500,\n",
       "          2.0000, -1.5000, -0.2500, -0.5000, -1.2500,  0.0000,  0.0000, -1.0000,\n",
       "         -0.5000,  1.7500, -0.2500,  0.5000, -3.0000,  1.5000,  1.2500, -0.5000,\n",
       "         -0.5000,  0.5000,  0.5000,  0.7500,  1.7500,  0.5000,  0.0000, -1.2500,\n",
       "         -0.2500, -0.2500,  0.0000,  1.5000,  0.0000, -2.2500, -3.2500,  0.2500,\n",
       "          0.0000, -0.2500]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9b36d373d0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "v cannot be empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Ensure the window size is at least 1\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Calculate the rolling average using NumPy's convolution function and a uniform filter\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m rolling_average \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Plot the original losses\u001b[39;00m\n\u001b[1;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal Losses\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconvolve\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/breeder_agent/.venv/lib/python3.8/site-packages/numpy/core/numeric.py:850\u001b[0m, in \u001b[0;36mconvolve\u001b[0;34m(a, v, mode)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma cannot be empty\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(v) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 850\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv cannot be empty\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m multiarray\u001b[38;5;241m.\u001b[39mcorrelate(a, v[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], mode)\n",
      "\u001b[0;31mValueError\u001b[0m: v cannot be empty"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'losses' is your array of floats\n",
    "losses = np.array(losses)  # Replace with your actual array if not already a NumPy array\n",
    "\n",
    "# Calculate the window size, which is 10% of the length of the losses array\n",
    "window_size = int(len(losses) * 0.1)\n",
    "if window_size < 1:\n",
    "    window_size = 1  # Ensure the window size is at least 1\n",
    "\n",
    "# Calculate the rolling average using NumPy's convolution function and a uniform filter\n",
    "rolling_average = np.convolve(losses, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Plot the original losses\n",
    "plt.plot(losses, label='Original Losses')\n",
    "\n",
    "# Plot the rolling average. Note that the rolling average will be shorter than the original array.\n",
    "# To align it with the original array, you'll need to decide how to handle the edges.\n",
    "# Here, we're starting the rolling average from the midpoint of the window.\n",
    "adjusted_index = (window_size - 1) // 2\n",
    "plt.plot(range(adjusted_index, adjusted_index + len(rolling_average)), rolling_average, label='Rolling Average')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Losses and Rolling Average')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
